{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lvjingzhe/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/lvjingzhe/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/lvjingzhe/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/lvjingzhe/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/lvjingzhe/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/lvjingzhe/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/lvjingzhe/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/lvjingzhe/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/lvjingzhe/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/lvjingzhe/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/lvjingzhe/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/lvjingzhe/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow.compat.v1 as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import sklearn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import scipy.stats as st\n",
    "from sympy.solvers import solve\n",
    "from sympy import Symbol\n",
    "from scipy.optimize import fsolve\n",
    "from sklearn.metrics import fbeta_score,f1_score,balanced_accuracy_score,recall_score,precision_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from cvxopt import matrix\n",
    "from cvxopt import solvers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Flatten, Activation, GlobalAveragePooling2D,Conv2D, MaxPooling2D\n",
    "from sklearn.ensemble import AdaBoostClassifier # AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier # KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine=pd.read_csv('/Users/lvjingzhe/Desktop/璇/modified_althogram/DRM/data/winequality-red.csv',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FixedAcidity</th>\n",
       "      <th>VolatileAcidity</th>\n",
       "      <th>CitricAcid</th>\n",
       "      <th>ResidualSugar</th>\n",
       "      <th>Chlorides</th>\n",
       "      <th>FreeSulfurDioxide</th>\n",
       "      <th>TotalSulfurDioxide</th>\n",
       "      <th>Density</th>\n",
       "      <th>PH</th>\n",
       "      <th>Sulphates</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.47</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.067</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.99549</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.66</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      FixedAcidity   VolatileAcidity   CitricAcid   ResidualSugar   Chlorides  \\\n",
       "0              7.4             0.700         0.00             1.9       0.076   \n",
       "1              7.8             0.880         0.00             2.6       0.098   \n",
       "2              7.8             0.760         0.04             2.3       0.092   \n",
       "3             11.2             0.280         0.56             1.9       0.075   \n",
       "4              7.4             0.700         0.00             1.9       0.076   \n",
       "...            ...               ...          ...             ...         ...   \n",
       "1594           6.2             0.600         0.08             2.0       0.090   \n",
       "1595           5.9             0.550         0.10             2.2       0.062   \n",
       "1596           6.3             0.510         0.13             2.3       0.076   \n",
       "1597           5.9             0.645         0.12             2.0       0.075   \n",
       "1598           6.0             0.310         0.47             3.6       0.067   \n",
       "\n",
       "       FreeSulfurDioxide   TotalSulfurDioxide   Density    PH   Sulphates  \\\n",
       "0                   11.0                 34.0   0.99780  3.51        0.56   \n",
       "1                   25.0                 67.0   0.99680  3.20        0.68   \n",
       "2                   15.0                 54.0   0.99700  3.26        0.65   \n",
       "3                   17.0                 60.0   0.99800  3.16        0.58   \n",
       "4                   11.0                 34.0   0.99780  3.51        0.56   \n",
       "...                  ...                  ...       ...   ...         ...   \n",
       "1594                32.0                 44.0   0.99490  3.45        0.58   \n",
       "1595                39.0                 51.0   0.99512  3.52        0.76   \n",
       "1596                29.0                 40.0   0.99574  3.42        0.75   \n",
       "1597                32.0                 44.0   0.99547  3.57        0.71   \n",
       "1598                18.0                 42.0   0.99549  3.39        0.66   \n",
       "\n",
       "       Alcohol  Class  \n",
       "0          9.4      5  \n",
       "1          9.8      5  \n",
       "2          9.8      5  \n",
       "3          9.8      6  \n",
       "4          9.4      5  \n",
       "...        ...    ...  \n",
       "1594      10.5      5  \n",
       "1595      11.2      6  \n",
       "1596      11.0      6  \n",
       "1597      10.2      5  \n",
       "1598      11.0      6  \n",
       "\n",
       "[1599 rows x 12 columns]"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    681\n",
       "1     10\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_3vs5['Class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_3vs5=wine[wine['Class'].isin([3,5])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "L=[]\n",
    "for k in wine_3vs5['Class'].values:\n",
    "    if k ==3:\n",
    "        L.append(1)\n",
    "    else:L.append(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_3vs5['Class']=L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "E=np.array(wine_3vs5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(691, 11)"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "np.random.shuffle(E)\n",
    "X_sample=E[:,:-1]/np.max(E[:,:-1],axis=0)\n",
    "y_sample=E[:,-1]\n",
    "X_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_sample=np.array(ecoli.iloc[:,:-1]/np.max(ecoli.iloc[:,:-1],axis=0))\n",
    "# y_sample=np.array(ecoli['Class'])\n",
    "# X_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample_XX=np.c_[X_sample,np.ones(X_sample.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample_ord=np.concatenate((X_sample[y_sample==0],X_sample[y_sample==1]))\n",
    "y_sample_ord=np.concatenate((y_sample[y_sample==0],y_sample[y_sample==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(691, 11)"
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sample_ord.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, x_te, y_tr, y_te = train_test_split(X_sample,y_sample,test_size = 0.2,\n",
    "                                                  shuffle = True,\n",
    "                                                  random_state = 4123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.c_[x_tr,np.ones(x_tr.shape[0])]\n",
    "X_test=np.c_[x_te,np.ones(x_te.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((552, 11), (139, 11), 691)"
      ]
     },
     "execution_count": 596,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr.shape,x_te.shape,y_sample.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.1"
      ]
     },
     "execution_count": 594,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sample.shape[0]/sum(y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(x,y):\n",
    "    k=int(y.shape[0]/sum(y))-1\n",
    "    n=y[y==1].shape[0]\n",
    "    m=int((x.shape[0]-n)/k)\n",
    "    indice=[i*m for i in range(1,k)]\n",
    "    indice.append(x.shape[0]-n)\n",
    "    x_c=np.concatenate((x[y==0],x[y==1]))\n",
    "    y_c=np.concatenate((np.array([0]*(x.shape[0]-n)),np.array([1]*n)))\n",
    "    return np.array(np.split(x_c,indice)),np.array(np.split(y_c,indice))\n",
    "\n",
    "\n",
    "n=y_tr[y_tr==1].shape[0]\n",
    "XX=np.concatenate((X_train[y_tr==0],X_train[y_tr==1]))\n",
    "YY=np.concatenate((np.array([0]*(X_train.shape[0]-n)),np.array([1]*n)))\n",
    "m=int((X_train.shape[0]-n)/68)\n",
    "indice=[k*m for k in range(1,68)]\n",
    "indice.append(X_train.shape[0]-n)\n",
    "\n",
    "pre_XX=np.array(np.split(XX,indice))\n",
    "pre_YY=np.array(np.split(YY,indice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_= RepeatedStratifiedKFold(n_splits=3, n_repeats=1, random_state=0)\n",
    "# cv=RepeatedStratifiedKFold(n_splits=3, n_repeats=1, random_state=42)\n",
    "cv= RepeatedStratifiedKFold(n_splits=3, n_repeats=1, random_state=0)\n",
    "cv_=RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      1.00       138\n",
      "         1.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.99       139\n",
      "   macro avg       0.50      0.50      0.50       139\n",
      "weighted avg       0.99      0.99      0.99       139\n",
      "\n",
      "Running time:0.11 s\n",
      "Balanced-Accuracy on testing set：50.00%\n",
      "Recall on testing set：0.00%\n",
      "F-measure on testing set：0.00%\n",
      "Accuracy on training set：98.37%\n",
      "Accuracy on testing set：99.28%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_i=time.time()\n",
    "model_in=LogisticRegression()\n",
    "model_in.fit(x_tr,y_tr)\n",
    "# model_in.fit(X_resampled_smote,y_resampled_smote)\n",
    "end_i=time.time()\n",
    "y_train_proba=model_in.predict_proba(x_tr)\n",
    "y_train_label=model_in.predict(x_tr)\n",
    "y_test_proba=model_in.predict_proba(x_te)\n",
    "\n",
    "y_test_label=model_in.predict(x_te)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_te,y_test_label))\n",
    "print(\"Running time:%.2f s\"%(end_i-start_i))\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_test_label)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_test_label)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_test_label,beta=2)))\n",
    "print('Accuracy on training set：{:.2%}'.format(accuracy_score(y_tr,y_train_label)))\n",
    "print('Accuracy on testing set：{:.2%}'.format(accuracy_score(y_te,y_test_label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Poly_close_solution:\n",
    "    def __init__(self,kernel='self._poly',penalty = None,gamma=1,degree = 2,coef0=0,alpha=1,beta=1):\n",
    "        self.W = None\n",
    "        self.penalty = penalty\n",
    "        self.d = degree\n",
    "        self.b = coef0\n",
    "        self.alpha=alpha\n",
    "        self.beta=beta\n",
    "        self.g=gamma\n",
    "        self.kernel=kernel\n",
    "        \n",
    "    def _poly(self,X_1,X_2):\n",
    "        return (self.g*(X_1.dot(X_2.T))+self.b)**self.d\n",
    "    def _rbf(self,x,y):\n",
    "        return np.exp(-self.g*np.sum((x[...,None,:]-y)**2,axis=2))\n",
    "\n",
    "    def poly_B_matrix(self,X,Y):\n",
    "        n=Y[Y==1].shape[0]\n",
    "        X_sample_split=np.array(np.split(X,np.array([X.shape[0]-n])))\n",
    "        B=np.zeros((X.shape[0],X.shape[0]))\n",
    "        I=0\n",
    "        for  m in X_sample_split:\n",
    "            I+=m.shape[0]\n",
    "            B[I-m.shape[0]:I,I-m.shape[0]:I]=eval(self.kernel+'(m,m)')/m.shape[0]\n",
    "        return B\n",
    "    \n",
    "    def K_x(self,X,x_t):\n",
    "        return eval(self.kernel+'(X,x_t)')\n",
    "    \n",
    "    def QplusBeta(self,X,Y):\n",
    "        s=time.time()\n",
    "        K=eval(self.kernel+'(X,X)')\n",
    "        H_p=np.diag(np.diagonal(K))\n",
    "        B_p=self.poly_B_matrix(X,Y)\n",
    "        Qplus_beta=K+self.alpha*(H_p-B_p)+np.diag([self.beta]*X.shape[0])\n",
    "        e=time.time()\n",
    "        return K,Qplus_beta\n",
    "    def fit(self,invQ,X,x_t):\n",
    "        \n",
    "        \n",
    "        W=invQ.dot(self.K_x(X,x_t))##求逆耗时\n",
    "        \n",
    "        return W.reshape(-1)\n",
    "    def delta_phi(self,K,invQ,X,Y,x_t,j):\n",
    "        \n",
    "        w=self.fit(invQ,X,x_t)\n",
    "        w_noty=w\n",
    "\n",
    "        w_y=np.zeros((X.shape[0]))\n",
    "        indice=np.argwhere(Y==j).reshape(-1)\n",
    "        w_noty[indice]=0\n",
    "        w_y[indice]=w[indice]\n",
    "        delta=np.dot(w_y,K.dot(w_y))+np.dot(w_noty,K.dot(w_noty))-2*np.dot(w_y,self.K_x(X,x_t))\n",
    "        return delta\n",
    "    def predict(self,K,invQ,X,Y,x_t):\n",
    "        delta=[]\n",
    "        for k in np.unique(Y):\n",
    "            delta.append(self.delta_phi(K,invQ,X,Y,x_t,k))\n",
    "        return np.argmin(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv,det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "from numpy.linalg import inv,det\n",
    "metrics_names=['G-mean','Balanced_acc','F1_score','Recall','Precision']\n",
    "def DRM_FiveCV(X,y,model=Poly_close_solution(\n",
    "                                )):\n",
    "    \n",
    "    G_m,bacc,f2,rec,pre=list(),list(),list(),list(),list()\n",
    "    for train_index,test_index in cv_.split(X,y):\n",
    "                \n",
    "                K,Q_beta=model.QplusBeta(X[train_index],y[train_index])\n",
    "                invQ=inv(Q_beta)\n",
    "                y_test_label=list()\n",
    "                for i in range(y[test_index].shape[0]):\n",
    "#                 X_resampled_smote,y_resampled_smote=SMOTE().fit_resample(X[train_index], y[train_index])\n",
    "                    y_test_label.append(model.predict(K,invQ,X[train_index],y[train_index],X[test_index][i]))\n",
    "#                 y_train_proba=model.predict_proba(X[train_index])\n",
    "#                 y_train_label=model.predict(X[train_index])\n",
    "                \n",
    "#                 print(\"running time for each CV:%.2f s\"%(t2-t1))\n",
    "                \n",
    "#     print(classification_report(y,y_pre))\n",
    "                rec.append(recall_score(y[test_index],y_test_label))\n",
    "                pre.append(precision_score(y[test_index],y_test_label))\n",
    "                G_m.append(math.sqrt(recall_score(y[test_index],y_test_label)*recall_score(y[test_index],y_test_label,pos_label=0)))\n",
    "                bacc.append(balanced_accuracy_score(y[test_index],y_test_label))\n",
    "                f2.append(fbeta_score(y[test_index],y_test_label,beta=2))\n",
    " \n",
    "    return G_m,bacc,f2,rec,pre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRM_L_fivefold-cv time:83.95\n"
     ]
    }
   ],
   "source": [
    "degree=np.array([2,3,4,5,8,10])##tr:te=8:2\n",
    "gamma=np.array([0.001,0.01,0.1,1,10])\n",
    "parameter=np.array([0.01,0.1,1,10,100])\n",
    "X_tr_ord=np.concatenate((x_tr[y_tr==0],x_tr[y_tr==1]))\n",
    "y_tr_ord=np.concatenate((y_tr[y_tr==0],y_tr[y_tr==1]))\n",
    "all_results_Ldrm = []\n",
    "import time\n",
    "import math\n",
    "# for k in mean_lr:\n",
    "start=time.time()\n",
    "\n",
    "for k in parameter:\n",
    "    for p in parameter:\n",
    "        for d in degree:\n",
    "            Lo_score=np.mean(DRM_FiveCV(X_tr_ord,y_tr_ord,model=Poly_close_solution(alpha=k,beta=p,degree=d\n",
    "                                )),axis=1)\n",
    "\n",
    "            metric_res = {'alpha':k,'beta':p,'degree':d}\n",
    "\n",
    "            for name, value in zip(metrics_names, Lo_score):\n",
    "        #             print(name, ': ', value)\n",
    "                    metric_res[name] = value\n",
    "\n",
    "\n",
    "            all_results_Ldrm.append(metric_res)\n",
    "end=time.time()\n",
    "print(\"DRM_L_fivefold-cv time:%.2f\"%(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(all_results_Ldrm).to_csv('/Users/lvjingzhe/Desktop/璇/modified_althogram/code/0512CV_select_DRML_keelwine_3vs5.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Model: Poly_DRM--------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99       138\n",
      "         1.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.99       139\n",
      "   macro avg       0.50      0.50      0.50       139\n",
      "weighted avg       0.99      0.99      0.99       139\n",
      "\n",
      "Balanced-Accuracy on testing set：49.64%\n",
      "G-mean on testing set：0.00%\n",
      "Recall on testing set：0.00%\n",
      "F-measure on testing set：0.00%\n",
      "Score :0.131\n",
      "Running time for L_DRM:0.21 s\n"
     ]
    }
   ],
   "source": [
    "P_model=Poly_close_solution(alpha=0.1,beta=0.01,degree=2)\n",
    "start_d=time.time()\n",
    "K,Q_beta=P_model.QplusBeta(X_sample_ord,y_sample_ord)\n",
    "invQ=inv(Q_beta)\n",
    "y_L=[]\n",
    "for i in range(y_te.shape[0]):\n",
    "    y_L.append(P_model.predict(K,invQ,X_sample_ord,y_sample_ord,x_te[i]))\n",
    "end_d=time.time()\n",
    "print('--------------Model: Poly_DRM--------------')\n",
    "print(classification_report(y_te,y_L))\n",
    "\n",
    "\n",
    "# from sklearn.metrics import accuracy_score\n",
    "bacc=balanced_accuracy_score(y_te,y_L)\n",
    "recall=recall_score(y_te,y_L)\n",
    "fbeta=fbeta_score(y_te,y_L,beta=2)\n",
    "G_m=np.sqrt((recall_score(y_te,y_L)*recall_score(y_te,y_L,pos_label=0)))\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_L)))\n",
    "print('G-mean on testing set：{:.2%}'.format(G_m))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_L)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_L,beta=2)))\n",
    "print('Score :{:.3}'.format(np.mean([G_m,recall,bacc,fbeta])*0.99+0.01/((end_d-start_d)/10*9+1)))\n",
    "\n",
    "\n",
    "print(\"Running time for L_DRM:%.2f s\"%(end_d-start_d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree=np.array([2,3,4,5,8,10])\n",
    "gamma=np.array([0.001,0.01,0.1,1,10])\n",
    "parameter=np.array([0.01,0.1,1,10,100])\n",
    "def fivetrials_DRM(X,y):\n",
    "    \n",
    "    \n",
    "    G_m,bacc,fbeta,rec,pre,T=list(),list(),list(),list(),list(),list()\n",
    "    \n",
    "    for train_index,test_index in cv.split(X, y):\n",
    "#         all_results_Ldrm = []\n",
    "#         for k in parameter:\n",
    "#             for p in parameter:\n",
    "#                 for d in degree:\n",
    "#                     Lo_score=np.mean(DRM_FiveCV(X[train_index],y[train_index],model=Poly_close_solution(alpha=k,beta=p,degree=d\n",
    "#                                         )),axis=1)\n",
    "\n",
    "#                     metric_res = {'alpha':k,'beta':p,'degree':d}\n",
    "\n",
    "#                     for name, value in zip(metrics_names, Lo_score):\n",
    "#                 #             print(name, ': ', value)\n",
    "#                             metric_res[name] = value\n",
    "\n",
    "\n",
    "#                     all_results_Ldrm.append(metric_res)\n",
    "#         eva=pd.DataFrame(all_results_Ldrm)\n",
    "#         bias=eva[eva.iloc[:,-5]==(eva.iloc[:,-5]).max()]\n",
    "#         al,be,de=np.array(bias[['alpha','beta','degree']])[0]\n",
    "        start=time.time()\n",
    "        P_model=Poly_close_solution(alpha=0.1,beta=0.01,degree=2)\n",
    "        K,Q_beta=P_model.QplusBeta(X[train_index],y[train_index])\n",
    "        invQ=inv(Q_beta)\n",
    "        prdict_y=[]\n",
    "        for i in range(y[test_index].shape[0]):\n",
    "            prdict_y.append(P_model.predict(K,invQ,X[train_index],y[train_index],X[test_index][i]))\n",
    "        end=time.time()\n",
    "        \n",
    "        print(classification_report(y[test_index],prdict_y))\n",
    "        rec.append(recall_score(y[test_index],prdict_y))\n",
    "        G_m.append(np.sqrt(recall_score(y[test_index],prdict_y)*recall_score(y[test_index],prdict_y,pos_label=0)))\n",
    "        pre.append(precision_score(y[test_index],prdict_y))\n",
    "    #     auc.append(roc_auc_score(y[test_index],prob_y[:,-1]))\n",
    "        bacc.append(balanced_accuracy_score(y[test_index],prdict_y))\n",
    "        fbeta.append(fbeta_score(y[test_index],prdict_y,beta=2))\n",
    "        T.append(end-start)\n",
    "#     pipeline = Pipeline(steps=steps)\n",
    "   \n",
    "#     print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_L)))\n",
    "#     print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_L)))\n",
    "#     print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_L,beta=3.66)))\n",
    "#     print('Score :{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end_d-start_d)))\n",
    "\n",
    "\n",
    "\n",
    "    return G_m,bacc,fbeta,rec,pre,T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99       227\n",
      "         1.0       0.25      0.25      0.25         4\n",
      "\n",
      "    accuracy                           0.97       231\n",
      "   macro avg       0.62      0.62      0.62       231\n",
      "weighted avg       0.97      0.97      0.97       231\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.98      0.99       227\n",
      "         1.0       0.20      0.33      0.25         3\n",
      "\n",
      "    accuracy                           0.97       230\n",
      "   macro avg       0.60      0.66      0.62       230\n",
      "weighted avg       0.98      0.97      0.98       230\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99       227\n",
      "         1.0       0.33      0.33      0.33         3\n",
      "\n",
      "    accuracy                           0.98       230\n",
      "   macro avg       0.66      0.66      0.66       230\n",
      "weighted avg       0.98      0.98      0.98       230\n",
      "\n",
      ">DRM:: Average G-mean:0.548(0.036) \n",
      ">DRM:: Average Balanced_Acc: 0.646(0.020) \n",
      ">DRM:: Average Fbeta: 0.292(0.034)\n",
      ">DRM:: Average Recall: 0.306(0.039)\n",
      ">DRM:: Average Training time: 0.113(0.008)\n",
      ">DRM:: Average accuracy_score: 0.448(0.032)\n",
      ">DRM:: Average Score: 0.450(0.031)\n"
     ]
    }
   ],
   "source": [
    "result_DRM = fivetrials_DRM(X_sample_ord,y_sample_ord)\n",
    "\n",
    "G_mean=result_DRM[0]\n",
    "Bacc=result_DRM[1]\n",
    "# summarize performance\n",
    "recall=result_DRM[3]\n",
    "Fbeta=result_DRM[2]\n",
    "T=result_DRM[-1]\n",
    "acc_s=np.mean(np.array(result_DRM)[:4,:],axis=0)\n",
    "\t# summarize and store\n",
    "print('>%s: Average G-mean:%.3f(%.3f) ' % ('DRM:',np.mean(G_mean),np.std(G_mean)))\n",
    "print('>%s: Average Balanced_Acc: %.3f(%.3f) ' % ('DRM:',np.mean(Bacc),np.std(Bacc)))\n",
    "print('>%s: Average Fbeta: %.3f(%.3f)' % ('DRM:',np.mean(Fbeta),np.std(Fbeta)))\n",
    "print('>%s: Average Recall: %.3f(%.3f)' % ('DRM:',np.mean(recall),np.std(recall)))    \n",
    "print('>%s: Average Training time: %.3f(%.3f)' % ('DRM:',np.mean(T),np.std(T)))\n",
    "print('>%s: Average accuracy_score: %.3f(%.3f)' % ('DRM:',np.mean(acc_s),np.std(acc_s)))\n",
    "print('>%s: Average Score: %.3f(%.3f)' % ('DRM:',np.mean(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1)),np.std(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1))))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 大致跑一版，再对照文章改进，如数据集预处理,调参等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "def get_models():\n",
    "\tmodels, names = list(), list()\n",
    "\t# LR\n",
    "\tmodels.append(LogisticRegression())\n",
    "\tnames.append('LR')\n",
    "\t# SVM\n",
    "\tmodels.append(SVC(probability=True))\n",
    "\tnames.append('SVM')\n",
    "\t# Bagging\n",
    "\tmodels.append(BaggingClassifier(n_estimators=1000))\n",
    "\tnames.append('BAG')\n",
    "\tmodels.append(AdaBoostClassifier(random_state=0))\n",
    "\tnames.append('Adaboost')\n",
    "\t# RF\n",
    "\tmodels.append(\n",
    "\ttree.DecisionTreeClassifier(max_leaf_nodes=6,\n",
    "                               random_state=0))\n",
    "\tnames.append('CART')\n",
    "\tmodels.append(RandomForestClassifier(criterion='entropy',n_estimators=6))\n",
    "\tnames.append('RF')\n",
    "\t# GBM\n",
    "\tmodels.append(MLPClassifier(random_state=0, max_iter=200,hidden_layer_sizes=(50,)))\n",
    "    \n",
    "\tnames.append('MLP')\n",
    "# \tmodels.append(solution_XX())\n",
    "# \tnames.append('max-mean loss')\n",
    "\treturn models, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X,y,model):\n",
    "#     steps = [('p', PowerTransformer()), ('m',model)]\n",
    "    \n",
    "#     pipeline = Pipeline(steps=steps)\n",
    "    pipeline = model\n",
    "    import time\n",
    "    start=time.time()\n",
    "    pipeline.fit(X,y)\n",
    "    end=time.time()\n",
    "    prdict_y=pipeline.predict(x_te)\n",
    "    prob_y=pipeline.predict_proba(x_te)\n",
    "    rec=recall_score(y_te,prdict_y)\n",
    "    G_m=np.sqrt(recall_score(y_te,prdict_y)*recall_score(y_te,prdict_y,pos_label=0))\n",
    "    pre=precision_score(y_te,prdict_y)\n",
    "#     auc.append(roc_auc_score(y[test_index],prob_y[:,-1]))\n",
    "    bacc=balanced_accuracy_score(y_te,prdict_y)\n",
    "    fbeta=fbeta_score(y_te,prdict_y,beta=4)\n",
    "#     print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_L)))\n",
    "#     print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_L)))\n",
    "#     print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_L,beta=3.66)))\n",
    "#     print('Score :{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end_d-start_d)))\n",
    "\n",
    "\n",
    "\n",
    "    return G_m,bacc,fbeta,rec,pre,end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def fivetrials(X,y,model,sample='none'):\n",
    "    pipeline = model\n",
    "    \n",
    "    G_m,bacc,fbeta,rec,pre,T=list(),list(),list(),list(),list(),list()\n",
    "    if sample!='none':\n",
    "#     steps = [('p', PowerTransformer()), ('m',model)]\n",
    "        for train_index,test_index in cv.split(X, y):\n",
    "        \n",
    "            start=time.time()\n",
    "            resample_x,resample_y=sample.fit_resample(X[train_index],y[train_index])\n",
    "            \n",
    "            pipeline.fit(resample_x,resample_y)\n",
    "            end=time.time()\n",
    "            prdict_y=pipeline.predict(X[test_index])\n",
    "            prob_y=pipeline.predict_proba(X[test_index])\n",
    "            rec.append(recall_score(y[test_index],prdict_y))\n",
    "            G_m.append(np.sqrt(recall_score(y[test_index],prdict_y)*recall_score(y[test_index],prdict_y,pos_label=0)))\n",
    "            pre.append(precision_score(y[test_index],prdict_y))\n",
    "        #     auc.append(roc_auc_score(y[test_index],prob_y[:,-1]))\n",
    "            bacc.append(balanced_accuracy_score(y[test_index],prdict_y))\n",
    "            fbeta.append(fbeta_score(y[test_index],prdict_y,beta=2))\n",
    "            T.append(end-start)\n",
    "#     pipeline = Pipeline(steps=steps)\n",
    "    else:\n",
    "        for train_index,test_index in cv.split(X, y):\n",
    "    \n",
    "            start=time.time()\n",
    "            pipeline.fit(X[train_index],y[train_index])\n",
    "            end=time.time()\n",
    "            prdict_y=pipeline.predict(X[test_index])\n",
    "            prob_y=pipeline.predict_proba(X[test_index])\n",
    "            rec.append(recall_score(y[test_index],prdict_y))\n",
    "            G_m.append(np.sqrt(recall_score(y[test_index],prdict_y)*recall_score(y[test_index],prdict_y,pos_label=0)))\n",
    "            pre.append(precision_score(y[test_index],prdict_y))\n",
    "        #     auc.append(roc_auc_score(y[test_index],prob_y[:,-1]))\n",
    "            bacc.append(balanced_accuracy_score(y[test_index],prdict_y))\n",
    "            fbeta.append(fbeta_score(y[test_index],prdict_y,beta=2))\n",
    "            T.append(end-start)\n",
    "#     print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_L)))\n",
    "#     print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_L)))\n",
    "#     print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_L,beta=3.66)))\n",
    "#     print('Score :{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end_d-start_d)))\n",
    "\n",
    "\n",
    "\n",
    "    return G_m,bacc,fbeta,rec,pre,T\n",
    "# print('>%s: Mean Balanced_Acc: %.3f (%.3f)' % (names[-1],mean(Bacc),std(Bacc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "X_resampled,y_resampled=RandomUnderSampler().fit_resample(x_tr,y_tr)\n",
    "X_resampled_smote,y_resampled_smote  = SMOTE().fit_resample(x_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">LR: Average G-mean:0.000(0.000) \n",
      ">LR: Average Balanced_Acc: 0.500(0.000) \n",
      ">LR: Average Fbeta: 0.000(0.000)\n",
      ">LR: Average Recall: 0.000(0.000)\n",
      ">LR: Average Training time: 0.052(0.055)\n",
      ">LR: Average accuracy_score: 0.125(0.000)\n",
      ">LR: Average Score: 0.133(0.000)\n",
      ">SVM: Average G-mean:0.000(0.000) \n",
      ">SVM: Average Balanced_Acc: 0.500(0.000) \n",
      ">SVM: Average Fbeta: 0.000(0.000)\n",
      ">SVM: Average Recall: 0.000(0.000)\n",
      ">SVM: Average Training time: 0.027(0.014)\n",
      ">SVM: Average accuracy_score: 0.125(0.000)\n",
      ">SVM: Average Score: 0.134(0.000)\n",
      ">BAG: Average G-mean:0.000(0.000) \n",
      ">BAG: Average Balanced_Acc: 0.499(0.001) \n",
      ">BAG: Average Fbeta: 0.000(0.000)\n",
      ">BAG: Average Recall: 0.000(0.000)\n",
      ">BAG: Average Training time: 1.922(0.401)\n",
      ">BAG: Average accuracy_score: 0.125(0.000)\n",
      ">BAG: Average Score: 0.127(0.001)\n",
      ">Adaboost: Average G-mean:0.000(0.000) \n",
      ">Adaboost: Average Balanced_Acc: 0.499(0.001) \n",
      ">Adaboost: Average Fbeta: 0.000(0.000)\n",
      ">Adaboost: Average Recall: 0.000(0.000)\n",
      ">Adaboost: Average Training time: 0.068(0.003)\n",
      ">Adaboost: Average accuracy_score: 0.125(0.000)\n",
      ">Adaboost: Average Score: 0.133(0.000)\n",
      ">CART: Average G-mean:0.192(0.272) \n",
      ">CART: Average Balanced_Acc: 0.553(0.081) \n",
      ">CART: Average Fbeta: 0.128(0.181)\n",
      ">CART: Average Recall: 0.111(0.157)\n",
      ">CART: Average Training time: 0.002(0.000)\n",
      ">CART: Average accuracy_score: 0.246(0.173)\n",
      ">CART: Average Score: 0.254(0.171)\n",
      ">RF: Average G-mean:0.000(0.000) \n",
      ">RF: Average Balanced_Acc: 0.500(0.000) \n",
      ">RF: Average Fbeta: 0.000(0.000)\n",
      ">RF: Average Recall: 0.000(0.000)\n",
      ">RF: Average Training time: 0.010(0.002)\n",
      ">RF: Average accuracy_score: 0.125(0.000)\n",
      ">RF: Average Score: 0.134(0.000)\n",
      ">MLP: Average G-mean:0.000(0.000) \n",
      ">MLP: Average Balanced_Acc: 0.500(0.000) \n",
      ">MLP: Average Fbeta: 0.000(0.000)\n",
      ">MLP: Average Recall: 0.000(0.000)\n",
      ">MLP: Average Training time: 0.116(0.012)\n",
      ">MLP: Average accuracy_score: 0.125(0.000)\n",
      ">MLP: Average Score: 0.133(0.000)\n"
     ]
    }
   ],
   "source": [
    "models, names = get_models()\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# evaluate each model\n",
    "for i in range(len(models)):\n",
    "\t# evaluate the model and store results\n",
    "\tresult = fivetrials(X_sample,y_sample,models[i])\n",
    "\n",
    "\tG_mean=result[0]\n",
    "\tBacc=result[1]\n",
    "# summarize performance\n",
    "\trecall=result[3]\n",
    "\tFbeta=result[2]\n",
    "\tT=result[-1]\n",
    "\tacc_s=np.mean(np.array(result)[:4,:],axis=0)\n",
    "\t# summarize and store\n",
    "\tprint('>%s: Average G-mean:%.3f(%.3f) ' % (names[i],np.mean(G_mean),np.std(G_mean)))\n",
    "\tprint('>%s: Average Balanced_Acc: %.3f(%.3f) ' % (names[i],np.mean(Bacc),np.std(Bacc)))\n",
    "\tprint('>%s: Average Fbeta: %.3f(%.3f)' % (names[i],np.mean(Fbeta),np.std(Fbeta)))\n",
    "\tprint('>%s: Average Recall: %.3f(%.3f)' % (names[i],np.mean(recall),np.std(recall)))    \n",
    "\tprint('>%s: Average Training time: %.3f(%.3f)' % (names[i],np.mean(T),np.std(T)))\n",
    "\tprint('>%s: Average accuracy_score: %.3f(%.3f)' % (names[i],np.mean(acc_s),np.std(acc_s)))\n",
    "\tprint('>%s: Average Score: %.3f(%.3f)' % (names[i],np.mean(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1)),np.std(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1))))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">SMOTELR: Average G-mean:0.656(0.084) \n",
      ">SMOTELR: Average Balanced_Acc: 0.691(0.058) \n",
      ">SMOTELR: Average Fbeta: 0.194(0.028)\n",
      ">SMOTELR: Average Recall: 0.500(0.136)\n",
      ">SMOTELR: Average Training time: 0.023(0.007)\n",
      ">SMOTELR: Average accuracy_score: 0.510(0.076)\n",
      ">SMOTELR: Average Score: 0.515(0.075)\n",
      ">SMOTESVM: Average G-mean:0.595(0.055) \n",
      ">SMOTESVM: Average Balanced_Acc: 0.655(0.035) \n",
      ">SMOTESVM: Average Fbeta: 0.199(0.041)\n",
      ">SMOTESVM: Average Recall: 0.389(0.079)\n",
      ">SMOTESVM: Average Training time: 0.099(0.026)\n",
      ">SMOTESVM: Average accuracy_score: 0.459(0.052)\n",
      ">SMOTESVM: Average Score: 0.464(0.052)\n",
      ">SMOTEBAG: Average G-mean:0.000(0.000) \n",
      ">SMOTEBAG: Average Balanced_Acc: 0.488(0.003) \n",
      ">SMOTEBAG: Average Fbeta: 0.000(0.000)\n",
      ">SMOTEBAG: Average Recall: 0.000(0.000)\n",
      ">SMOTEBAG: Average Training time: 3.900(0.361)\n",
      ">SMOTEBAG: Average accuracy_score: 0.122(0.001)\n",
      ">SMOTEBAG: Average Score: 0.123(0.001)\n",
      ">SMOTEAdaboost: Average G-mean:0.189(0.267) \n",
      ">SMOTEAdaboost: Average Balanced_Acc: 0.540(0.075) \n",
      ">SMOTEAdaboost: Average Fbeta: 0.076(0.107)\n",
      ">SMOTEAdaboost: Average Recall: 0.111(0.157)\n",
      ">SMOTEAdaboost: Average Training time: 0.108(0.001)\n",
      ">SMOTEAdaboost: Average accuracy_score: 0.229(0.152)\n",
      ">SMOTEAdaboost: Average Score: 0.236(0.150)\n",
      ">SMOTECART: Average G-mean:0.226(0.319) \n",
      ">SMOTECART: Average Balanced_Acc: 0.537(0.124) \n",
      ">SMOTECART: Average Fbeta: 0.090(0.127)\n",
      ">SMOTECART: Average Recall: 0.167(0.236)\n",
      ">SMOTECART: Average Training time: 0.005(0.001)\n",
      ">SMOTECART: Average accuracy_score: 0.255(0.201)\n",
      ">SMOTECART: Average Score: 0.262(0.199)\n",
      ">SMOTERF: Average G-mean:0.166(0.234) \n",
      ">SMOTERF: Average Balanced_Acc: 0.534(0.059) \n",
      ">SMOTERF: Average Fbeta: 0.083(0.118)\n",
      ">SMOTERF: Average Recall: 0.083(0.118)\n",
      ">SMOTERF: Average Training time: 0.016(0.000)\n",
      ">SMOTERF: Average accuracy_score: 0.217(0.132)\n",
      ">SMOTERF: Average Score: 0.224(0.131)\n",
      ">SMOTEMLP: Average G-mean:0.666(0.084) \n",
      ">SMOTEMLP: Average Balanced_Acc: 0.704(0.056) \n",
      ">SMOTEMLP: Average Fbeta: 0.228(0.021)\n",
      ">SMOTEMLP: Average Recall: 0.500(0.136)\n",
      ">SMOTEMLP: Average Training time: 0.634(0.072)\n",
      ">SMOTEMLP: Average accuracy_score: 0.524(0.074)\n",
      ">SMOTEMLP: Average Score: 0.525(0.074)\n"
     ]
    }
   ],
   "source": [
    "models, names = get_models()\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# evaluate each model\n",
    "for i in range(len(models)):\n",
    "\t# evaluate the model and store results\n",
    "\tresult = fivetrials(X_sample,y_sample,models[i],sample=SMOTE(random_state=0))\n",
    "\n",
    "\tG_mean=result[0]\n",
    "\tBacc=result[1]\n",
    "# summarize performance\n",
    "\trecall=result[3]\n",
    "\tFbeta=result[2]\n",
    "\tT=result[-1]\n",
    "\tacc_s=np.mean(np.array(result)[:4,:],axis=0)\n",
    "\t# summarize and store\n",
    "\tprint('>%s: Average G-mean:%.3f(%.3f) ' % ('SMOTE'+names[i],np.mean(G_mean),np.std(G_mean)))\n",
    "\tprint('>%s: Average Balanced_Acc: %.3f(%.3f) ' % ('SMOTE'+names[i],np.mean(Bacc),np.std(Bacc)))\n",
    "\tprint('>%s: Average Fbeta: %.3f(%.3f)' % ('SMOTE'+names[i],np.mean(Fbeta),np.std(Fbeta)))\n",
    "\tprint('>%s: Average Recall: %.3f(%.3f)' % ('SMOTE'+names[i],np.mean(recall),np.std(recall)))    \n",
    "\tprint('>%s: Average Training time: %.3f(%.3f)' % ('SMOTE'+names[i],np.mean(T),np.std(T)))\n",
    "\tprint('>%s: Average accuracy_score: %.3f(%.3f)' % ('SMOTE'+names[i],np.mean(acc_s),np.std(acc_s)))\n",
    "\tprint('>%s: Average Score: %.3f(%.3f)' % ('SMOTE'+names[i],np.mean(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1)),np.std(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1))))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">RUSLR: Average G-mean:0.691(0.100) \n",
      ">RUSLR: Average Balanced_Acc: 0.725(0.074) \n",
      ">RUSLR: Average Fbeta: 0.168(0.040)\n",
      ">RUSLR: Average Recall: 0.694(0.275)\n",
      ">RUSLR: Average Training time: 0.006(0.001)\n",
      ">RUSLR: Average accuracy_score: 0.569(0.112)\n",
      ">RUSLR: Average Score: 0.574(0.111)\n",
      ">RUSSVM: Average G-mean:0.670(0.074) \n",
      ">RUSSVM: Average Balanced_Acc: 0.720(0.052) \n",
      ">RUSSVM: Average Fbeta: 0.250(0.093)\n",
      ">RUSSVM: Average Recall: 0.611(0.283)\n",
      ">RUSSVM: Average Training time: 0.004(0.001)\n",
      ">RUSSVM: Average accuracy_score: 0.563(0.086)\n",
      ">RUSSVM: Average Score: 0.567(0.085)\n",
      ">RUSBAG: Average G-mean:0.561(0.079) \n",
      ">RUSBAG: Average Balanced_Acc: 0.629(0.057) \n",
      ">RUSBAG: Average Fbeta: 0.138(0.079)\n",
      ">RUSBAG: Average Recall: 0.611(0.283)\n",
      ">RUSBAG: Average Training time: 1.241(0.056)\n",
      ">RUSBAG: Average accuracy_score: 0.485(0.079)\n",
      ">RUSBAG: Average Score: 0.485(0.078)\n",
      ">RUSAdaboost: Average G-mean:0.584(0.138) \n",
      ">RUSAdaboost: Average Balanced_Acc: 0.645(0.097) \n",
      ">RUSAdaboost: Average Fbeta: 0.130(0.076)\n",
      ">RUSAdaboost: Average Recall: 0.694(0.275)\n",
      ">RUSAdaboost: Average Training time: 0.053(0.001)\n",
      ">RUSAdaboost: Average accuracy_score: 0.513(0.112)\n",
      ">RUSAdaboost: Average Score: 0.518(0.111)\n",
      ">RUSCART: Average G-mean:0.590(0.132) \n",
      ">RUSCART: Average Balanced_Acc: 0.646(0.100) \n",
      ">RUSCART: Average Fbeta: 0.129(0.077)\n",
      ">RUSCART: Average Recall: 0.694(0.275)\n",
      ">RUSCART: Average Training time: 0.002(0.000)\n",
      ">RUSCART: Average accuracy_score: 0.515(0.116)\n",
      ">RUSCART: Average Score: 0.520(0.115)\n",
      ">RUSRF: Average G-mean:0.642(0.105) \n",
      ">RUSRF: Average Balanced_Acc: 0.659(0.086) \n",
      ">RUSRF: Average Fbeta: 0.138(0.066)\n",
      ">RUSRF: Average Recall: 0.583(0.180)\n",
      ">RUSRF: Average Training time: 0.008(0.001)\n",
      ">RUSRF: Average accuracy_score: 0.505(0.105)\n",
      ">RUSRF: Average Score: 0.510(0.104)\n",
      ">RUSMLP: Average G-mean:0.652(0.106) \n",
      ">RUSMLP: Average Balanced_Acc: 0.673(0.086) \n",
      ">RUSMLP: Average Fbeta: 0.155(0.076)\n",
      ">RUSMLP: Average Recall: 0.583(0.180)\n",
      ">RUSMLP: Average Training time: 0.089(0.012)\n",
      ">RUSMLP: Average accuracy_score: 0.516(0.105)\n",
      ">RUSMLP: Average Score: 0.520(0.104)\n"
     ]
    }
   ],
   "source": [
    "models, names = get_models()\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# evaluate each model\n",
    "for i in range(len(models)):\n",
    "\t# evaluate the model and store results\n",
    "\tresult = fivetrials(X_sample,y_sample,models[i],sample=RandomUnderSampler(random_state=42))\n",
    "\n",
    "\tG_mean=result[0]\n",
    "\tBacc=result[1]\n",
    "# summarize performance\n",
    "\trecall=result[3]\n",
    "\tFbeta=result[2]\n",
    "\tT=result[-1]\n",
    "\tacc_s=np.mean(np.array(result)[:4,:],axis=0)\n",
    "\t# summarize and store\n",
    "\tprint('>%s: Average G-mean:%.3f(%.3f) ' % ('RUS'+names[i],np.mean(G_mean),np.std(G_mean)))\n",
    "\tprint('>%s: Average Balanced_Acc: %.3f(%.3f) ' % ('RUS'+names[i],np.mean(Bacc),np.std(Bacc)))\n",
    "\tprint('>%s: Average Fbeta: %.3f(%.3f)' % ('RUS'+names[i],np.mean(Fbeta),np.std(Fbeta)))\n",
    "\tprint('>%s: Average Recall: %.3f(%.3f)' % ('RUS'+names[i],np.mean(recall),np.std(recall)))    \n",
    "\tprint('>%s: Average Training time: %.3f(%.3f)' % ('RUS'+names[i],np.mean(T),np.std(T)))\n",
    "\tprint('>%s: Average accuracy_score: %.3f(%.3f)' % ('RUS'+names[i],np.mean(acc_s),np.std(acc_s)))\n",
    "\tprint('>%s: Average Score: %.3f(%.3f)' % ('RUS'+names[i],np.mean(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1)),np.std(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1))))\n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models_1():\n",
    "\tmodels, names = list(), list()\n",
    "\t# LR\n",
    "# \tmodels.append(LogisticRegression(class_weight='balanced'))\n",
    "# \tnames.append('LR')\n",
    "\t# SVM\n",
    "\tmodels.append(SVC(gamma='scale', class_weight='balanced',probability=True))\n",
    "\tnames.append('CS-SVM')\n",
    "\tmodels.append(\n",
    "\ttree.DecisionTreeClassifier(class_weight='balanced',max_leaf_nodes=6,\n",
    "                               random_state=0))\n",
    "\tnames.append('CS-CART')\n",
    "\t# RF\n",
    "# \tmodels.append(RandomForestClassifier(n_estimators=1000))\n",
    "# \tnames.append('RF')\n",
    "\treturn models, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">CS-SVM: Average G-mean:0.855(0.055) \n",
      ">CS-SVM: Average Balanced_Acc: 0.858(0.054) \n",
      ">CS-SVM: Average Fbeta: 0.805(0.086)\n",
      ">CS-SVM: Average Recall: 0.871(0.120)\n",
      ">CS-SVM: Average Training time: 0.011(0.001)\n",
      ">CS-SVM: Average accuracy_score: 0.847(0.077)\n",
      ">CS-SVM: Average Score: 0.849(0.076)\n",
      ">CS-CART: Average G-mean:0.890(0.042) \n",
      ">CS-CART: Average Balanced_Acc: 0.892(0.043) \n",
      ">CS-CART: Average Fbeta: 0.849(0.064)\n",
      ">CS-CART: Average Recall: 0.896(0.088)\n",
      ">CS-CART: Average Training time: 0.001(0.000)\n",
      ">CS-CART: Average accuracy_score: 0.882(0.058)\n",
      ">CS-CART: Average Score: 0.883(0.057)\n"
     ]
    }
   ],
   "source": [
    "models, names = get_models_1()\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# evaluate each model\n",
    "for i in range(len(models)):\n",
    "\t# evaluate the model and store results\n",
    "\tresult = fivetrials(X_sample,y_sample,models[i])\n",
    "\n",
    "\tG_mean=result[0]\n",
    "\tBacc=result[1]\n",
    "# summarize performance\n",
    "\trecall=result[3]\n",
    "\tFbeta=result[2]\n",
    "\tT=result[-1]\n",
    "\tacc_s=np.mean(np.array(result)[:4,:],axis=0)\n",
    "\t# summarize and store\n",
    "\tprint('>%s: Average G-mean:%.3f(%.3f) ' % (names[i],np.mean(G_mean),np.std(G_mean)))\n",
    "\tprint('>%s: Average Balanced_Acc: %.3f(%.3f) ' % (names[i],np.mean(Bacc),np.std(Bacc)))\n",
    "\tprint('>%s: Average Fbeta: %.3f(%.3f)' % (names[i],np.mean(Fbeta),np.std(Fbeta)))\n",
    "\tprint('>%s: Average Recall: %.3f(%.3f)' % (names[i],np.mean(recall),np.std(recall)))    \n",
    "\tprint('>%s: Average Training time: %.3f(%.3f)' % (names[i],np.mean(T),np.std(T)))\n",
    "\tprint('>%s: Average accuracy_score: %.3f(%.3f)' % (names[i],np.mean(acc_s),np.std(acc_s)))\n",
    "\tprint('>%s: Average Score: %.3f(%.3f)' % (names[i],np.mean(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1)),np.std(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1))))\n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75.2"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sample.shape[0]/10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -------------------------分界线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30,)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_YY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class solution_XX:\n",
    "    \n",
    "    \n",
    "    def __init__(self,penalty = None,Lambda = 0.03,a = 0.5,epochs = 200):\n",
    "        self.W = None\n",
    "        self.penalty = penalty\n",
    "        self.Lambda = Lambda\n",
    "        self.a = a\n",
    "        self.epochs =epochs\n",
    "        self.sigmoid = lambda x:1/(1 + np.exp(-x))\n",
    "        \n",
    "#     err=(y_tr-s.sigmoid(np.dot(X_train,w)))**2\n",
    "#     err[index]=2.6*err[index]\n",
    "\n",
    "    def f_XX(self,X,Y):\n",
    "        if self.penalty=='l1':f=np.array([np.mean((self.sigmoid(np.dot(x,self.W))-y)**2 )+self.Lambda*np.sum(np.abs(self.W)) for x,y in zip(X,Y)])\n",
    "        elif self.penalty=='l2':f=np.array([np.mean((self.sigmoid(np.dot(x,self.W))-y)**2 )+self.Lambda*np.sum(self.W**2) for x,y in zip(X,Y)])#pre_Xtrain,pre_Ytrain\n",
    "        else:f=np.array([np.mean((self.sigmoid(np.dot(x,self.W))-y)**2 ) for x,y in zip(X,Y)])\n",
    "\n",
    "        return f         \n",
    "    def Gf_XX(self,X,Y):#To compute the Derivative matrix, the shape of which is N*2\n",
    "        if self.penalty=='l1':d=np.array([x.T.dot(0.02*(self.sigmoid(np.dot(x,self.W))-y)*self.sigmoid(np.dot(x,self.W))*(1-self.sigmoid(np.dot(x,self.W))))+self.Lambda*np.sign(self.W )for x,y in zip(X,Y)])\n",
    "        elif self.penalty=='l2':\n",
    "            d=np.array([x.T.dot(0.02*(self.sigmoid(np.dot(x,self.W))-y)*self.sigmoid(np.dot(x,self.W))*(1-self.sigmoid(np.dot(x,self.W))))+2*self.Lambda*self.W for x,y in zip(X,Y)])\n",
    "#     return d.reshape(20,12289)\n",
    "        else:d=np.array([x.T.dot(0.02*(self.sigmoid(np.dot(x,self.W))-y)*self.sigmoid(np.dot(x,self.W))*(1-self.sigmoid(np.dot(x,self.W)))) for x,y in zip(X,Y)])\n",
    "        return d\n",
    "    def direction_XX(self,X,Y):\n",
    "        gra=self.Gf_XX(X,Y)\n",
    "        p=matrix(gra.dot(gra.T),tc='d')\n",
    "        q=matrix(-self.f_XX(X,Y),tc='d')\n",
    "        G=matrix(np.diag(np.array([-1]*(Y.shape[0]))),tc='d')#N=20\n",
    "        h=matrix(np.array([[0]]*(Y.shape[0])),tc='d')\n",
    "        A=matrix([[1.0]]*(Y.shape[0]))\n",
    "        b=matrix([1.0])\n",
    "        solvers.options['show_progress'] = False\n",
    "        sol = solvers.qp(p,q,G,h,A,b)\n",
    "        t=np.array(sol['x'])\n",
    "        d= -(gra.T.dot(t))\n",
    "        return d.reshape((X_train.shape[-1],))\n",
    "\n",
    "    def fit(self,X,Y):\n",
    "        \n",
    "        call=[]\n",
    "        pre=[]\n",
    "        loss=[]\n",
    "        testloss=[]\n",
    "        np.random.seed(1324)\n",
    "        self.W=np.random.random((X_train.shape[-1],))*2-1\n",
    "#         self.W=w_2\n",
    "        n=y_te[y_te==1.].shape[0]\n",
    "        for k in range(200):\n",
    "    #     while np.linalg.norm(d)//10**(-8) >= 10:\n",
    "            d=self.direction_XX(X,Y)\n",
    "#             print(np.linalg.norm(d))\n",
    "            if np.linalg.norm(d)//10**(-7) < 25:\n",
    "                break\n",
    "            sigma=0.8\n",
    "            f_1=np.max(self.f_XX(X,Y))\n",
    "            w=self.W\n",
    "            self.W=d*sigma+w\n",
    "            while np.max(self.f_XX(X,Y))>np.max(f_1):\n",
    "                sigma=sigma*0.8\n",
    "                self.W=d*sigma+w\n",
    "            self.W=d*sigma+w\n",
    "\n",
    "            \n",
    "    #         output=output.reshape(4000,)\n",
    "    #         pt=max(output[output==1.].shape[0],1)\n",
    "    #         m=0\n",
    "    #         for j in range(4000):\n",
    "    #             if output[j]==test_label[j]==1:\n",
    "    #                 m+=1\n",
    "\n",
    "\n",
    "    # # b1=np.random.random((1,10))*2-1\n",
    "\n",
    "\n",
    "            \n",
    "    #         call.append(m/n)\n",
    "    #         pre.append(m/pt)\n",
    "    #         loss.append(max(f_X(w)))\n",
    "    #         testloss.append(ff_X(w))\n",
    "        return self.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1279, 12)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fivetrials_XX(X,y,model=solution_XX()):\n",
    "    pipeline = model\n",
    "    \n",
    "    G_m,bacc,fbeta,rec,pre,T=list(),list(),list(),list(),list(),list()\n",
    "    \n",
    "#     steps = [('p', PowerTransformer()), ('m',model)]\n",
    "    for train_index,test_index in cv.split(X, y):\n",
    "        \n",
    "        start=time.time()\n",
    "#         n=y[train_index][y[train_index]==1].shape[0]\n",
    "#         XX=np.concatenate((X[train_index][y[train_index]==0],X[train_index][y[train_index]==1]))\n",
    "#         YY=np.concatenate((np.array([0]*(X[train_index].shape[0]-n)),np.array([1]*n)))\n",
    "#         m=int((X[train_index].shape[0]-n)/3)\n",
    "#         indice=[k*m for k in range(1,3)]\n",
    "#         indice.append(X[train_index].shape[0]-n)\n",
    "\n",
    "#         pre_X=np.array(np.split(XX,indice))\n",
    "#         pre_Y=np.array(np.split(YY,indice))\n",
    "        pre_X,pre_Y=split(X[train_index],y[train_index])\n",
    "        print(pre_Y.shape)\n",
    "\n",
    "        w_x=pipeline.fit(pre_X,pre_Y)\n",
    "        end=time.time()\n",
    "        prdict_y=pipeline.sigmoid(X[test_index].dot(w_x))\n",
    "        prdict_y[prdict_y>=0.5]=1\n",
    "        prdict_y[prdict_y<0.5]=0\n",
    "\n",
    "        print(classification_report(y[test_index],prdict_y))\n",
    "        \n",
    "        rec.append(recall_score(y[test_index],prdict_y))\n",
    "        G_m.append(np.sqrt(recall_score(y[test_index],prdict_y)*recall_score(y[test_index],prdict_y,pos_label=0)))\n",
    "        pre.append(precision_score(y[test_index],prdict_y))\n",
    "    #     auc.append(roc_auc_score(y[test_index],prob_y[:,-1]))\n",
    "        bacc.append(balanced_accuracy_score(y[test_index],prdict_y))\n",
    "        fbeta.append(fbeta_score(y[test_index],prdict_y,beta=2))\n",
    "        T.append(end-start)\n",
    "#     pipeline = Pipeline(steps=steps)\n",
    "   \n",
    "#     print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_L)))\n",
    "#     print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_L)))\n",
    "#     print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_L,beta=3.66)))\n",
    "#     print('Score :{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end_d-start_d)))\n",
    "\n",
    "\n",
    "\n",
    "    return G_m,bacc,fbeta,rec,pre,T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1599, 12)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sample_XX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.90      0.94       227\n",
      "         1.0       0.04      0.25      0.07         4\n",
      "\n",
      "    accuracy                           0.89       231\n",
      "   macro avg       0.51      0.58      0.51       231\n",
      "weighted avg       0.97      0.89      0.93       231\n",
      "\n",
      "(65,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.88      0.93       227\n",
      "         1.0       0.04      0.33      0.06         3\n",
      "\n",
      "    accuracy                           0.87       230\n",
      "   macro avg       0.51      0.61      0.50       230\n",
      "weighted avg       0.98      0.87      0.92       230\n",
      "\n",
      "(65,)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.88      0.93       227\n",
      "         1.0       0.07      0.67      0.12         3\n",
      "\n",
      "    accuracy                           0.87       230\n",
      "   macro avg       0.53      0.77      0.53       230\n",
      "weighted avg       0.98      0.87      0.92       230\n",
      "\n",
      ">max_mean loss:: Average G-mean:0.594(0.124) \n",
      ">max_mean loss:: Average Balanced_Acc: 0.652(0.086) \n",
      ">max_mean loss:: Average Fbeta: 0.164(0.053)\n",
      ">max_mean loss:: Average Recall: 0.417(0.180)\n",
      ">max_mean loss:: Average Training time: 4.068(0.404)\n",
      ">max_mean loss:: Average accuracy_score: 0.457(0.110)\n",
      ">max_mean loss:: Average Score: 0.459(0.109)\n"
     ]
    }
   ],
   "source": [
    "result_XX = fivetrials_XX(X_sample_XX,y_sample)\n",
    "\n",
    "G_mean=result_XX[0]\n",
    "Bacc=result_XX[1]\n",
    "# summarize performance\n",
    "recall=result_XX[3]\n",
    "Fbeta=result_XX[2]\n",
    "T=result_XX[-1]\n",
    "acc_s=np.mean(np.array(result_XX)[:4,:],axis=0)\n",
    "\t# summarize and store\n",
    "print('>%s: Average G-mean:%.3f(%.3f) ' % ('max_mean loss:',np.mean(G_mean),np.std(G_mean)))\n",
    "print('>%s: Average Balanced_Acc: %.3f(%.3f) ' % ('max_mean loss:',np.mean(Bacc),np.std(Bacc)))\n",
    "print('>%s: Average Fbeta: %.3f(%.3f)' % ('max_mean loss:',np.mean(Fbeta),np.std(Fbeta)))\n",
    "print('>%s: Average Recall: %.3f(%.3f)' % ('max_mean loss:',np.mean(recall),np.std(recall)))    \n",
    "print('>%s: Average Training time: %.3f(%.3f)' % ('max_mean loss:',np.mean(T),np.std(T)))\n",
    "print('>%s: Average accuracy_score: %.3f(%.3f)' % ('max_mean loss:',np.mean(acc_s),np.std(acc_s)))\n",
    "print('>%s: Average Score: %.3f(%.3f)' % ('max_mean loss:',np.mean(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1)),np.std(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1))))\n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_YY[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier :L2 max-mean loss\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.80      0.89       137\n",
      "         1.0       0.07      1.00      0.13         2\n",
      "\n",
      "    accuracy                           0.81       139\n",
      "   macro avg       0.53      0.90      0.51       139\n",
      "weighted avg       0.99      0.81      0.88       139\n",
      "\n",
      "Running time:3.65 s\n",
      "Balanced-Accuracy on testing set：90.15%\n",
      "Recall on testing set：100.00%\n",
      "F-measure on testing set：27.03%\n",
      "Score:0.63\n"
     ]
    }
   ],
   "source": [
    "s_2=solution_XX()\n",
    "start_x=time.time()\n",
    "w_2 = s_2.fit(pre_XX,pre_YY)\n",
    "'''w_2 is the estimator obtained by max-mean loss method+L2 regulation'''\n",
    "end_x=time.time()\n",
    "y_pred=s_2.sigmoid(X_test.dot(w_2))\n",
    "\n",
    "y_train_pred=s_2.sigmoid(X_train.dot(w_2))\n",
    "y_train_pred[y_train_pred>=0.5]=1\n",
    "y_train_pred[y_train_pred<0.5]=0\n",
    "y_pred[y_pred>=0.5]=1\n",
    "y_pred[y_pred<0.5]=0\n",
    "\n",
    "\n",
    "bacc=balanced_accuracy_score(y_te,y_pred)\n",
    "fbeta=f1_score(y_te,y_test_label)\n",
    "recall=recall_score(y_te,y_pred)\n",
    "print('Classifier :L2 max-mean loss')\n",
    "print(classification_report(y_te,y_pred))\n",
    "\n",
    "\n",
    "print(\"Running time:%.2f s\"%(end_x-start_x))\n",
    "\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_pred)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_pred)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_pred,beta=2)))\n",
    "print('Score:{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/((end_x-start_x)/10*9+1)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  -----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import testjx\n",
    "def sigmoid_(x,u):\n",
    "    return 1.0/(1.0+np.exp(-x-u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1490997])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fsolve(lambda x:equa_m(x,400,1),0.5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 943,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429.49512219429016\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "mean_sm_=np.array([fsolve(lambda x:equa_m(x,n),0.5 ) for n in range(10,600,5)])\n",
    "\n",
    "end=time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def equa_m(x,n,r):\n",
    "    pre_in=(np.dot(x_tr,model_in.coef_.T)+model_in.intercept_).reshape(x_tr.shape[0],)\n",
    "    \n",
    "    ini_err=y_tr-sigmoid_(pre_in,x)##ini_err is the predicted err of training set, based on LR\n",
    "\n",
    "    for k in range(ini_err.shape[0]):\n",
    "        if y_tr[k]==1:\n",
    "            ini_err[k]=r*ini_err[k]\n",
    "\n",
    "#     tm=testjx.testjx.meanuncertainty(ini_err.tolist(),n)\n",
    "    return testjx.testjx.meanuncertainty(ini_err.tolist(),n)[1]\n",
    "def equa_sm(x,n):\n",
    "#     ini_err=y_tr-sigmoid_(pre_in,x)\n",
    "    \n",
    "    ini_err=y_tr-sigmoid_(pre_in,x)##ini_err is the predicted err of training set, based on LR\n",
    "\n",
    "#     for k in ini_err.index:\n",
    "#         if y_tr[k]==1:\n",
    "#             ini_err[k]=1.5*ini_err[k]\n",
    "\n",
    "#     tm=testjx.testjx.meanuncertainty(ini_err.tolist(),n)\n",
    "    return testjx.testjx.meanuncertainty(ini_err.tolist(),n)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def equa_fivem(x,n,r,Mod,x_train,y_train):\n",
    "    pre_in=(np.dot(x_train,Mod.coef_.T)+Mod.intercept_).reshape(x_train.shape[0],)\n",
    "    \n",
    "    ini_err=y_train-sigmoid_(pre_in,x)##ini_err is the predicted err of training set, based on LR\n",
    "\n",
    "    for k in range(ini_err.shape[0]):\n",
    "        if y_train[k]==1:\n",
    "            ini_err[k]=r*ini_err[k]\n",
    "\n",
    "    \n",
    "    return testjx.testjx.meanuncertainty(ini_err.tolist(),n)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equa_xh(x,n,r):\n",
    "    pre_in=np.dot(X_train,w_2).reshape(X_train.shape[0],)\n",
    "    ini_err=y_tr-sigmoid_(pre_in,x)##ini_err is the predicted err of training set, based on max-mean loss.\n",
    "#     err_w=shuffle(np.concatenate((np.random.choice(ini_err[y_tr==0],ini_err[y_tr==1].shape[0]),ini_err[y_tr==1])))\n",
    "    for k in range(ini_err.shape[0]):\n",
    "    \n",
    "        if y_tr[k]==1:\n",
    "            ini_err[k]=r*ini_err[k]\n",
    "\n",
    "    \n",
    "    return testjx.testjx.meanuncertainty(ini_err.tolist(),n)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equa_xhfive(x,n,r,w,x_train,y_train):\n",
    "    pre_in=np.dot(x_train,w).reshape(x_train.shape[0],)\n",
    "    ini_err=y_train-sigmoid_(pre_in,x)##ini_err is the predicted err of training set, based on max-mean loss.\n",
    "#     err_w=shuffle(np.concatenate((np.random.choice(ini_err[y_tr==0],ini_err[y_tr==1].shape[0]),ini_err[y_tr==1])))\n",
    "    for k in range(ini_err.shape[0]):\n",
    "    \n",
    "        if y_train[k]==1:\n",
    "            ini_err[k]=r*ini_err[k]\n",
    "\n",
    "    \n",
    "    return testjx.testjx.meanuncertainty(ini_err.tolist(),n)[1]## the return is the max-mean err\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_names=['Balanced_acc','F2_score','Recall','Precision']\n",
    "def evaluate_model_lr(X,y,m,model=LogisticRegression()):\n",
    "    \n",
    "    bacc,f2,rec,pre=list(),list(),list(),list()\n",
    "    for train_index,test_index in cv_.split(X,y):\n",
    "#                 X_resampled_smote,y_resampled_smote=SMOTE().fit_resample(X[train_index], y[train_index])\n",
    "                model.fit(X[train_index],y[train_index])\n",
    "                prob_y=sigmoid_((np.dot(X[test_index],model.coef_.T)+model.intercept_),m)\n",
    "                \n",
    "                \n",
    "                prdict_y=np.round(prob_y)\n",
    "                rec.append(recall_score(y[test_index],prdict_y))\n",
    "                pre.append(precision_score(y[test_index],prdict_y))\n",
    "                \n",
    "                bacc.append(balanced_accuracy_score(y[test_index],prdict_y))\n",
    "                f2.append(fbeta_score(y[test_index],prdict_y,beta=4))\n",
    " \n",
    "    return bacc,f2,rec,pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.151762203259462"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log((y_sample.shape[0]-sum(y_sample))/sum(y_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([250.66666667, 125.33333333,  68.36363636,  35.80952381])"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sample.shape[0]/(IR+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.485714285714284"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_sample.shape[0])/35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4321)\n",
    "per_=np.random.choice(y_sample[y_sample==1].shape[0],50)\n",
    "y_1=np.concatenate((y_sample[per_],y_sample[y_sample==0]))\n",
    "y_sample_=sklearn.utils.shuffle(y_1,random_state=42)\n",
    "x_1=np.concatenate((diabetes_x_scale[per_],diabetes_x_scale[y_sample==0]))\n",
    "x_sample_=sklearn.utils.shuffle(x_1,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.42857142857143"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr.shape[0]/sum(y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "419.20000000000005"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_tr.shape[0]*0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_lr3vs5=np.array([fsolve(lambda x:equa_m(x,n,0.5*61),0.5 ) for n in range(50,420,5)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_xh=np.array([fsolve(lambda x:equa_xh(x,n,0.5*61),0.5 ) for n in range(50,420,5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=RepeatedStratifiedKFold(n_splits=3, n_repeats=1, random_state=423)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select(arr,n,k):##arr为1-D\n",
    "    per=[]\n",
    "    A=abs(arr-k)\n",
    "\n",
    "    for j in np.sort(A)[:n]:\n",
    "        per.append(list(A).index(j))\n",
    "    return arr[per]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_names=['Balanced_acc','F2_score','Recall','Precision']\n",
    "def Fivetrails_mean(k,X,y,model=LogisticRegression()):\n",
    "    mean=mean_lr3vs5.reshape(-1)\n",
    "    G_m,bacc,f2,rec,pre,T=list(),list(),list(),list(),list(),list()\n",
    "    for train_index,test_index in cv.split(X,y):\n",
    "                s_1=time.time()\n",
    "#                 print(sum(y[train_index]))\n",
    "#                 X_resampled_smote,y_resampled_smote=SMOTE().fit_resample(X[train_index], y[train_index])\n",
    "                model.fit(X[train_index],y[train_index])\n",
    "                t_1=time.time()\n",
    "#                 mean_u=np.array([fsolve(lambda x:equa_fivem(x,n,0.5*(k+1),model,X[train_index],y[train_index]),0.5 ) for n in range(50,int(y[train_index].shape[0]*0.8),5)])\n",
    "                all_results = []\n",
    "                c=0\n",
    "                \n",
    "                \n",
    "                for i in mean_lr3vs5.reshape(-1):\n",
    "                    result=np.mean(evaluate_model_lr(X[train_index],y[train_index],i),axis=1)\n",
    "                    metric_res = {'window':10+5*c,'upper_mean': i}\n",
    "                    c+=1\n",
    "                    for name, value in zip(metrics_names, result):\n",
    "                #             print(name, ': ', value)\n",
    "                            metric_res[name] = value\n",
    "\n",
    "\n",
    "                    all_results.append(metric_res)\n",
    "                eva=pd.DataFrame(all_results)\n",
    "#                 eva.to_csv('{:}: datapre_diabetesCV_which_mu_to_set_inLRBIAS.csv'.format(k))\n",
    "                bias=eva[eva.iloc[:,-4]==eva.iloc[:,-4].max()]\n",
    "#                 mean=select(mean_lr3vs5.reshape(-1),20,bias['upper_mean'].mean())\n",
    "                print(\"optimal upper_mean by CV: %.3f \"%(bias['upper_mean'].mean()))\n",
    "                prob_y=sigmoid_((np.dot(X[test_index],model.coef_.T)+model.intercept_).reshape(X[test_index].shape[0],),bias['upper_mean'].mean()\n",
    "                             )\n",
    "                \n",
    "                \n",
    "                \n",
    "                prdict_y=np.round(prob_y)\n",
    "                print(classification_report(y[test_index],prdict_y))\n",
    "                rec.append(recall_score(y[test_index],prdict_y))\n",
    "                pre.append(precision_score(y[test_index],prdict_y))\n",
    "                G_m.append(np.sqrt(recall_score(y[test_index],prdict_y)*recall_score(y[test_index],prdict_y,pos_label=0)))\n",
    "                bacc.append(balanced_accuracy_score(y[test_index],prdict_y))\n",
    "                f2.append(fbeta_score(y[test_index],prdict_y,beta=max(2,np.log(k))))\n",
    "                T.append(t_1-s_1)\n",
    "    return G_m,bacc,f2,rec,pre,T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal upper_mean by CV: 4.031 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.93      0.96       227\n",
      "         1.0       0.15      0.75      0.25         4\n",
      "\n",
      "    accuracy                           0.92       231\n",
      "   macro avg       0.57      0.84      0.60       231\n",
      "weighted avg       0.98      0.92      0.95       231\n",
      "\n",
      "optimal upper_mean by CV: 3.692 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.93      0.96       227\n",
      "         1.0       0.06      0.33      0.10         3\n",
      "\n",
      "    accuracy                           0.92       230\n",
      "   macro avg       0.52      0.63      0.53       230\n",
      "weighted avg       0.98      0.92      0.95       230\n",
      "\n",
      "optimal upper_mean by CV: 4.024 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.70      0.82       227\n",
      "         1.0       0.04      1.00      0.08         3\n",
      "\n",
      "    accuracy                           0.70       230\n",
      "   macro avg       0.52      0.85      0.45       230\n",
      "weighted avg       0.99      0.70      0.81       230\n",
      "\n",
      ">LR_mean:: Average G-mean:0.741(0.131) \n",
      ">LR_mean:: Average Balanced_Acc: 0.772(0.101) \n",
      ">LR_mean:: Average Fbeta: 0.444(0.145)\n",
      ">LR_mean:: Average Recall: 0.694(0.275)\n",
      ">LR_mean:: Average accuracy_score: 0.663(0.154)\n",
      ">LR_mean:: Average Score: 0.666(0.152)\n"
     ]
    }
   ],
   "source": [
    "result = Fivetrails_mean(y_sample.shape[0]/sum(y_sample)-1,X_sample,y_sample)\n",
    "# \n",
    "G_mean=result[0]\n",
    "Bacc=result[1]\n",
    "# summarize performance\n",
    "recall=result[3]\n",
    "Fbeta=result[2]\n",
    "T=result[-1]\n",
    "acc_s=np.mean(np.array(result)[:4,:],axis=0)\n",
    "\n",
    "print('>%s: Average G-mean:%.3f(%.3f) ' % ('LR_mean:',np.mean(G_mean),np.std(G_mean)))\n",
    "print('>%s: Average Balanced_Acc: %.3f(%.3f) ' % ('LR_mean:',np.mean(Bacc),np.std(Bacc)))\n",
    "print('>%s: Average Fbeta: %.3f(%.3f)' % ('LR_mean:',np.mean(Fbeta),np.std(Fbeta)))\n",
    "print('>%s: Average Recall: %.3f(%.3f)' % ('LR_mean:',np.mean(recall),np.std(recall)))    \n",
    "print('>%s: Average accuracy_score: %.3f(%.3f)' % ('LR_mean:',np.mean(acc_s),np.std(acc_s)))\n",
    "print('>%s: Average Score: %.3f(%.3f)' % ('LR_mean:',np.mean(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1)),np.std(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1))))\n",
    "          \n",
    "# print('>%s: Average accuracy_score: %.3f(%.3f)' % ('IR='+str(k),np.mean(acc_s),np.std(acc_s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0310546901108415\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.93      0.96       227\n",
      "         1.0       0.15      0.75      0.25         4\n",
      "\n",
      "    accuracy                           0.92       231\n",
      "   macro avg       0.57      0.84      0.60       231\n",
      "weighted avg       0.98      0.92      0.95       231\n",
      "\n",
      "3.692042821044385\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.93      0.96       227\n",
      "         1.0       0.06      0.33      0.10         3\n",
      "\n",
      "    accuracy                           0.92       230\n",
      "   macro avg       0.52      0.63      0.53       230\n",
      "weighted avg       0.98      0.92      0.95       230\n",
      "\n",
      "4.024274036111809\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.70      0.82       227\n",
      "         1.0       0.04      1.00      0.08         3\n",
      "\n",
      "    accuracy                           0.70       230\n",
      "   macro avg       0.52      0.85      0.45       230\n",
      "weighted avg       0.99      0.70      0.81       230\n",
      "\n",
      ">LR_mean:: Average G-mean:0.741(0.131) \n",
      ">LR_mean:: Average Balanced_Acc: 0.772(0.101) \n",
      ">LR_mean:: Average Fbeta: 0.444(0.145)\n",
      ">LR_mean:: Average Recall: 0.694(0.275)\n",
      ">LR_mean:: Average accuracy_score: 0.663(0.154)\n",
      ">LR_mean:: Average Score: 0.666(0.152)\n"
     ]
    }
   ],
   "source": [
    "result = Fivetrails_mean(y_sample.shape[0]/sum(y_sample)-1,X_sample,y_sample)\n",
    "# \n",
    "G_mean=result[0]\n",
    "Bacc=result[1]\n",
    "# summarize performance\n",
    "recall=result[3]\n",
    "Fbeta=result[2]\n",
    "T=result[-1]\n",
    "acc_s=np.mean(np.array(result)[:4,:],axis=0)\n",
    "\n",
    "print('>%s: Average G-mean:%.3f(%.3f) ' % ('LR_mean:',np.mean(G_mean),np.std(G_mean)))\n",
    "print('>%s: Average Balanced_Acc: %.3f(%.3f) ' % ('LR_mean:',np.mean(Bacc),np.std(Bacc)))\n",
    "print('>%s: Average Fbeta: %.3f(%.3f)' % ('LR_mean:',np.mean(Fbeta),np.std(Fbeta)))\n",
    "print('>%s: Average Recall: %.3f(%.3f)' % ('LR_mean:',np.mean(recall),np.std(recall)))    \n",
    "print('>%s: Average accuracy_score: %.3f(%.3f)' % ('LR_mean:',np.mean(acc_s),np.std(acc_s)))\n",
    "print('>%s: Average Score: %.3f(%.3f)' % ('LR_mean:',np.mean(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1)),np.std(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1))))\n",
    "          \n",
    "# print('>%s: Average accuracy_score: %.3f(%.3f)' % ('IR='+str(k),np.mean(acc_s),np.std(acc_s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.46257301,  8.91514755],\n",
       "       [ 0.4549512 ,  8.34070517],\n",
       "       [ 0.48023   ,  7.91325868],\n",
       "       [ 0.47674505,  7.48558754],\n",
       "       [ 0.44505391,  7.60111684],\n",
       "       [ 0.45004005,  7.28625618],\n",
       "       [ 0.454138  ,  6.99526146],\n",
       "       [ 0.45868423,  6.74940842],\n",
       "       [ 0.46093924,  6.52412742],\n",
       "       [ 0.46295854,  6.32573866],\n",
       "       [ 0.46532035,  6.14941185],\n",
       "       [ 0.46583578,  5.97784958],\n",
       "       [ 0.4682974 ,  5.83002928],\n",
       "       [ 0.46663639,  5.68677097],\n",
       "       [ 6.21724964, 10.30552211],\n",
       "       [ 5.95245552,  9.80574693],\n",
       "       [ 0.5       ,  1.5       ],\n",
       "       [ 5.52531271,  8.98748594],\n",
       "       [ 5.81409915, 10.61726318],\n",
       "       [ 5.65802735, 10.40103571],\n",
       "       [ 0.5       ,  1.5       ],\n",
       "       [ 5.3624113 ,  9.73020877],\n",
       "       [ 5.22881922,  9.47571931],\n",
       "       [ 8.62871161, 11.88631382],\n",
       "       [ 8.26372293, 11.35882067],\n",
       "       [ 8.97374068, 13.75889213],\n",
       "       [ 8.58145903, 13.12021233],\n",
       "       [ 8.23771874, 12.55489648],\n",
       "       [11.84413433, 15.26247644],\n",
       "       [11.23582583, 14.46257233]])"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fivetrails_var(X,y,model=LogisticRegression()):\n",
    "    \n",
    "    G_m,bacc,f2,rec,pre,T=list(),list(),list(),list(),list(),list()\n",
    "    for train_index,test_index in cv.split(X,y):\n",
    "                s_1=time.time()\n",
    "#                 X_resampled_smote,y_resampled_smote=SMOTE().fit_resample(X[train_index], y[train_index])\n",
    "                model.fit(X[train_index],y[train_index])\n",
    "                t_1=time.time()\n",
    "#                 mean_u=np.array([fsolve(lambda x:equa_fivem(x,n,0.5*(k+1),model,X[train_index],y[train_index]),0.5) for n in range(int(y[train_index].shape[0]*0.2),int(y[train_index].shape[0]*0.8),5)])\n",
    "                all_results = []\n",
    "                c=0\n",
    "                # for k in mean_lr:\n",
    "            #     strat=time.time()\n",
    "                for i in var_lr:\n",
    "                    result=np.mean(var_evaluate_model(X[train_index],y[train_index],i),axis=1)\n",
    "                    metric_res = {'window':10+5*c,'upper_mean': i}\n",
    "                    c+=1\n",
    "                    for name, value in zip(metrics_names, result):\n",
    "                #             print(name, ': ', value)\n",
    "                            metric_res[name] = value\n",
    "\n",
    "\n",
    "                    all_results.append(metric_res)\n",
    "                eva=pd.DataFrame(all_results)\n",
    "#                 eva.to_csv('{:}: datapre_diabetesCV_which_mu_to_set_inLRBIAS.csv'.format(k))\n",
    "                bias=eva[np.mean(eva.iloc[:,-4:-1],axis=1)==np.mean(eva.iloc[:,-4:-1],axis=1).max()]\n",
    "#                 end=time.time()\n",
    "                print(np.array(bias['upper_mean']))  \n",
    "                prob_y=F_u(np.array(bias['upper_mean'])[0],(np.dot(X[test_index],model.coef_.T)+model.intercept_).reshape(X[test_index].shape[0],))\n",
    "                \n",
    "                \n",
    "                prdict_y=np.round(prob_y)\n",
    "                print(classification_report(y[test_index],prdict_y))\n",
    "                rec.append(recall_score(y[test_index],prdict_y))\n",
    "                pre.append(precision_score(y[test_index],prdict_y))\n",
    "                G_m.append(np.sqrt(recall_score(y[test_index],prdict_y)*recall_score(y[test_index],prdict_y,pos_label=0)))\n",
    "                bacc.append(balanced_accuracy_score(y[test_index],prdict_y))\n",
    "                f2.append(fbeta_score(y[test_index],prdict_y,beta=2))\n",
    "                T.append(t_1-s_1)\n",
    "    return G_m,bacc,f2,rec,pre,T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0.45868423, 6.74940842])]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.77      0.87       227\n",
      "         1.0       0.07      1.00      0.13         4\n",
      "\n",
      "    accuracy                           0.77       231\n",
      "   macro avg       0.54      0.89      0.50       231\n",
      "weighted avg       0.98      0.77      0.86       231\n",
      "\n",
      "[array([0.46295854, 6.32573866])]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.79      0.88       227\n",
      "         1.0       0.02      0.33      0.04         3\n",
      "\n",
      "    accuracy                           0.78       230\n",
      "   macro avg       0.50      0.56      0.46       230\n",
      "weighted avg       0.98      0.78      0.87       230\n",
      "\n",
      "[array([0.46093924, 6.52412742])]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.67      0.80       227\n",
      "         1.0       0.04      1.00      0.07         3\n",
      "\n",
      "    accuracy                           0.67       230\n",
      "   macro avg       0.52      0.83      0.44       230\n",
      "weighted avg       0.99      0.67      0.79       230\n",
      "\n",
      ">LR_volatility:: Average G-mean:0.736(0.160) \n",
      ">LR_volatility:: Average Balanced_Acc: 0.760(0.143) \n",
      ">LR_volatility:: Average Fbeta: 0.175(0.080)\n",
      ">LR_volatility:: Average Recall: 0.778(0.314)\n",
      ">LR_volatility:: Average Training time: 0.005(0.000)\n",
      ">LR_volatility:: Average accuracy_score: 0.612(0.171)\n",
      ">LR_volatility:: Average Score: 0.616(0.170)\n"
     ]
    }
   ],
   "source": [
    "result = Fivetrails_var(X_sample,y_sample)\n",
    "# \n",
    "G_mean=result[0]\n",
    "Bacc=result[1]\n",
    "# summarize performance\n",
    "recall=result[3]\n",
    "Fbeta=result[2]\n",
    "# T=result[-1]\n",
    "acc_s=np.mean(np.array(result)[:4,:],axis=0)\n",
    "T=result[-1]\n",
    "print('>%s: Average G-mean:%.3f(%.3f) ' % ('LR_volatility:',np.mean(G_mean),np.std(G_mean)))\n",
    "print('>%s: Average Balanced_Acc: %.3f(%.3f) ' % ('LR_volatility:',np.mean(Bacc),np.std(Bacc)))\n",
    "print('>%s: Average Fbeta: %.3f(%.3f)' % ('LR_volatility:',np.mean(Fbeta),np.std(Fbeta)))\n",
    "print('>%s: Average Recall: %.3f(%.3f)' % ('LR_volatility:',np.mean(recall),np.std(recall)))    \n",
    "print('>%s: Average Training time: %.3f(%.3f)' % ('LR_volatility:',np.mean(T),np.std(T)))\n",
    "print('>%s: Average accuracy_score: %.3f(%.3f)' % ('LR_volatility:',np.mean(acc_s),np.std(acc_s)))\n",
    "print('>%s: Average Score: %.3f(%.3f)' % ('LR_volatility:',np.mean(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1)),np.std(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1))))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4696526 , 2.25753618])"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(bias['upper_mean'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fivetrails_xhvar(X,y,model=solution_XX()):\n",
    "    \n",
    "    G_m,bacc,f2,rec,pre=list(),list(),list(),list(),list()\n",
    "    for train_index,test_index in cv.split(X,y):\n",
    "                pre_X,pre_Y=split(X[train_index],y[train_index])\n",
    "                print(pre_X.shape)\n",
    "#                 X_resampled_smote,y_resampled_smote=SMOTE().fit_resample(X[train_index], y[train_index])\n",
    "                w_x=model.fit(pre_X,pre_Y)\n",
    "#                 mean_u=np.array([fsolve(lambda x:equa_xhfive(x,n,0.5*(k+1),w_x,X[train_index],y[train_index]),0.5 ) for n in range(10,y[train_index].shape[0]-100,5)])\n",
    "                all_results = []\n",
    "                c=0\n",
    "                # for k in mean_lr:\n",
    "            #     strat=time.time()\n",
    "                for i in var_lr:\n",
    "                    result=np.mean(evaluate_varxh(X[train_index],y[train_index],i),axis=1)\n",
    "                    metric_res = {'window':10+5*c,'upper_mean': i}\n",
    "                    c+=1\n",
    "                    for name, value in zip(metrics_names, result):\n",
    "                #             print(name, ': ', value)\n",
    "                            metric_res[name] = value\n",
    "\n",
    "\n",
    "                    all_results.append(metric_res)\n",
    "                eva=pd.DataFrame(all_results)\n",
    "#                 eva.to_csv('{:}: datapre_diabetesCV_which_mu_to_set_inLRBIAS.csv'.format(k))\n",
    "                bias=eva[eva.iloc[:,-4]==(eva.iloc[:,-4]).max()]\n",
    "                end=time.time()\n",
    "                prob_y=F_u(np.array(bias['upper_mean'])[0],X[test_index].dot(w_x))\n",
    "#                 print(abs(np.array(bias['upper_mean'])[0]))\n",
    "                \n",
    "                prdict_y=np.round(prob_y)\n",
    "                rec.append(recall_score(y[test_index],prdict_y))\n",
    "                pre.append(precision_score(y[test_index],prdict_y))\n",
    "                G_m.append(np.sqrt(recall_score(y[test_index],prdict_y)*recall_score(y[test_index],prdict_y,pos_label=0)))\n",
    "                bacc.append(balanced_accuracy_score(y[test_index],prdict_y))\n",
    "                f2.append(fbeta_score(y[test_index],prdict_y,beta=2))\n",
    " \n",
    "    return G_m,bacc,f2,rec,pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      ">MMloss_volatility:: Average G-mean:0.796(0.077) \n",
      ">MMloss_volatility:: Average Balanced_Acc: 0.805(0.079) \n",
      ">MMloss_volatility:: Average Fbeta: 0.607(0.099)\n",
      ">MMloss_volatility:: Average Recall: 0.886(0.167)\n"
     ]
    }
   ],
   "source": [
    "result_xh_var = Fivetrails_xhvar(X_sample_XX,y_sample)\n",
    "# \n",
    "G_mean=result_xh_var[0]\n",
    "Bacc=result_xh_var[1]\n",
    "# summarize performance\n",
    "recall=result_xh_var[3]\n",
    "Fbeta=result_xh_var[2]\n",
    "\n",
    "acc_s=np.mean(np.array(result_xh_var)[:4,:],axis=0)\n",
    "\n",
    "print('>%s: Average G-mean:%.3f(%.3f) ' % ('MMloss_volatility:',np.mean(G_mean),np.std(G_mean)))\n",
    "print('>%s: Average Balanced_Acc: %.3f(%.3f) ' % ('MMloss_volatility:',np.mean(Bacc),np.std(Bacc)))\n",
    "print('>%s: Average Fbeta: %.3f(%.3f)' % ('MMloss_volatility:',np.mean(Fbeta),np.std(Fbeta)))\n",
    "print('>%s: Average Recall: %.3f(%.3f)' % ('MMloss_volatility:',np.mean(recall),np.std(recall)))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.7297232 ,  0.64152076,  0.56076506,  0.48048932,  0.42593041,\n",
       "        0.36247918,  0.30135733,  0.24437623,  0.18629237,  0.13700783,\n",
       "        0.08947369,  0.07971422,  0.03669837, -0.00603846, -0.05082144,\n",
       "       -0.09163005, -0.13060651, -0.16927315,  0.01836012,  0.04121271])"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concmean_xh[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_names=['Balanced_acc','F2_score','Recall','Precision']\n",
    "def Fivetrails_xhmean(k,X,y,model=solution_XX()):\n",
    "    \n",
    "    G_m,bacc,f2,rec,pre,T=list(),list(),list(),list(),list(),list()\n",
    "    for train_index,test_index in cv.split(X,y):\n",
    "                start=time.time()\n",
    "                pre_X,pre_Y=split(X[train_index],y[train_index])\n",
    "#                 X_resampled_smote,y_resampled_smote=SMOTE().fit_resample(X[train_index], y[train_index])\n",
    "                w_x=model.fit(pre_X,pre_Y)\n",
    "                end=time.time()\n",
    "#                 mean_u=np.array([fsolve(lambda x:equa_xhfive(x,n,0.5*(k+1),w_x,X[train_index],y[train_index]),0.5 ) for n in range(int(y[train_index].shape[0]*0.2),int(y[train_index].shape[0]*0.8),5)])\n",
    "                all_results = []\n",
    "                c=0\n",
    "                # for k in mean_lr:\n",
    "                \n",
    "                for i in np.concatenate((mean_xh[5:11].reshape(-1),np.array([0.17,0.16,0.15]))):\n",
    "                    result=np.mean(evaluate_modelxh(X[train_index],y[train_index],i),axis=1)\n",
    "                    metric_res = {'window':10+5*c,'upper_mean': i}\n",
    "                    c+=1\n",
    "                    for name, value in zip(metrics_names, result):\n",
    "                #             print(name, ': ', value)\n",
    "                            metric_res[name] = value\n",
    "\n",
    "\n",
    "                    all_results.append(metric_res)\n",
    "                eva=pd.DataFrame(all_results)\n",
    "#                 eva.to_csv('{:}: datapre_diabetesCV_which_mu_to_set_inLRBIAS.csv'.format(k))\n",
    "                bias=eva[eva.iloc[:,-4]==(eva.iloc[:,-4]).max()]\n",
    "#                 end=time.time()\n",
    "#                 print(end-start)\n",
    "                print(bias['upper_mean'].mean())\n",
    "                prob_y=sigmoid_(np.dot(X[test_index],w_x).reshape(y[test_index].shape[0],),bias['upper_mean'].mean()\n",
    "                             )\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "                prdict_y=np.round(prob_y)\n",
    "                print(classification_report(y[test_index],prdict_y))\n",
    "                rec.append(recall_score(y[test_index],prdict_y))\n",
    "                pre.append(precision_score(y[test_index],prdict_y))\n",
    "                G_m.append(np.sqrt(recall_score(y[test_index],prdict_y)*recall_score(y[test_index],prdict_y,pos_label=0)))\n",
    "                bacc.append(balanced_accuracy_score(y[test_index],prdict_y))\n",
    "                f2.append(fbeta_score(y[test_index],prdict_y,beta=max(2,np.log(k))))\n",
    "                T.append(end-start)\n",
    "    return G_m,bacc,f2,rec,pre,T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.1"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sample.shape[0]/sum(y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08947368853415893\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.80      0.88       227\n",
      "         1.0       0.04      0.50      0.08         4\n",
      "\n",
      "    accuracy                           0.79       231\n",
      "   macro avg       0.52      0.65      0.48       231\n",
      "weighted avg       0.97      0.79      0.87       231\n",
      "\n",
      "0.15\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.66      0.79       227\n",
      "         1.0       0.04      1.00      0.07         3\n",
      "\n",
      "    accuracy                           0.66       230\n",
      "   macro avg       0.52      0.83      0.43       230\n",
      "weighted avg       0.99      0.66      0.78       230\n",
      "\n",
      "0.13700783141279885\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.67      0.80       227\n",
      "         1.0       0.04      1.00      0.07         3\n",
      "\n",
      "    accuracy                           0.67       230\n",
      "   macro avg       0.52      0.83      0.44       230\n",
      "weighted avg       0.99      0.67      0.79       230\n",
      "\n",
      ">max-mean-loss_mean:: Average G-mean:0.753(0.086) \n",
      ">max-mean-loss_mean:: Average Balanced_Acc: 0.771(0.086) \n",
      ">max-mean-loss_mean:: Average Fbeta: 0.388(0.052)\n",
      ">max-mean-loss_mean:: Average Recall: 0.833(0.236)\n",
      ">max-mean-loss_mean:: Average Training time: 3.557(0.439)\n",
      ">max_mean-loss_mean:: Average Score: 0.682(0.114)\n"
     ]
    }
   ],
   "source": [
    "result_XX_mean = Fivetrails_xhmean(68,X_sample_XX,y_sample)\n",
    "# \n",
    "G_mean=result_XX_mean[0]\n",
    "Bacc=result_XX_mean[1]\n",
    "# summarize performance\n",
    "recall=result_XX_mean[3]\n",
    "Fbeta=result_XX_mean[2]\n",
    "T=result_XX_mean[-1]\n",
    "acc_s=np.mean(np.array(result_XX_mean)[:4,:],axis=0)\n",
    "\n",
    "print('>%s: Average G-mean:%.3f(%.3f) ' % ('max-mean-loss_mean:',np.mean(G_mean),np.std(G_mean)))\n",
    "print('>%s: Average Balanced_Acc: %.3f(%.3f) ' % ('max-mean-loss_mean:',np.mean(Bacc),np.std(Bacc)))\n",
    "print('>%s: Average Fbeta: %.3f(%.3f)' % ('max-mean-loss_mean:',np.mean(Fbeta),np.std(Fbeta)))\n",
    "print('>%s: Average Recall: %.3f(%.3f)' % ('max-mean-loss_mean:',np.mean(recall),np.std(recall))) \n",
    "print('>%s: Average Training time: %.3f(%.3f)' % ('max-mean-loss_mean:',np.mean(T),np.std(T)))\n",
    "print('>%s: Average Score: %.3f(%.3f)' % ('max_mean-loss_mean:',np.mean(0.99*acc_s+0.01/(np.array(result_XX_mean)[-1,:]/10*9+1)),np.std(0.99*acc_s+0.01/(np.array(result_XX_mean)[-1,:]/10*9+1))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.80      0.88       227\n",
      "         1.0       0.04      0.50      0.08         4\n",
      "\n",
      "    accuracy                           0.79       231\n",
      "   macro avg       0.52      0.65      0.48       231\n",
      "weighted avg       0.97      0.79      0.87       231\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.61      0.76       227\n",
      "         1.0       0.03      1.00      0.06         3\n",
      "\n",
      "    accuracy                           0.62       230\n",
      "   macro avg       0.52      0.81      0.41       230\n",
      "weighted avg       0.99      0.62      0.75       230\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.67      0.80       227\n",
      "         1.0       0.04      1.00      0.07         3\n",
      "\n",
      "    accuracy                           0.67       230\n",
      "   macro avg       0.52      0.83      0.44       230\n",
      "weighted avg       0.99      0.67      0.79       230\n",
      "\n",
      ">max-mean-loss_mean:: Average G-mean:0.744(0.081) \n",
      ">max-mean-loss_mean:: Average Balanced_Acc: 0.763(0.082) \n",
      ">max-mean-loss_mean:: Average Fbeta: 0.378(0.047)\n",
      ">max-mean-loss_mean:: Average Recall: 0.833(0.236)\n",
      ">max-mean-loss_mean:: Average Training time: 3.582(0.391)\n",
      ">max_mean-loss_mean:: Average Score: 0.675(0.110)\n"
     ]
    }
   ],
   "source": [
    "result_XX_mean = Fivetrails_xhmean(68,X_sample_XX,y_sample)\n",
    "# \n",
    "G_mean=result_XX_mean[0]\n",
    "Bacc=result_XX_mean[1]\n",
    "# summarize performance\n",
    "recall=result_XX_mean[3]\n",
    "Fbeta=result_XX_mean[2]\n",
    "T=result_XX_mean[-1]\n",
    "acc_s=np.mean(np.array(result_XX_mean)[:4,:],axis=0)\n",
    "\n",
    "print('>%s: Average G-mean:%.3f(%.3f) ' % ('max-mean-loss_mean:',np.mean(G_mean),np.std(G_mean)))\n",
    "print('>%s: Average Balanced_Acc: %.3f(%.3f) ' % ('max-mean-loss_mean:',np.mean(Bacc),np.std(Bacc)))\n",
    "print('>%s: Average Fbeta: %.3f(%.3f)' % ('max-mean-loss_mean:',np.mean(Fbeta),np.std(Fbeta)))\n",
    "print('>%s: Average Recall: %.3f(%.3f)' % ('max-mean-loss_mean:',np.mean(recall),np.std(recall))) \n",
    "print('>%s: Average Training time: %.3f(%.3f)' % ('max-mean-loss_mean:',np.mean(T),np.std(T)))\n",
    "print('>%s: Average Score: %.3f(%.3f)' % ('max_mean-loss_mean:',np.mean(0.99*acc_s+0.01/(np.array(result_XX_mean)[-1,:]/10*9+1)),np.std(0.99*acc_s+0.01/(np.array(result_XX_mean)[-1,:]/10*9+1))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.885004997253418"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results = []\n",
    "c=0\n",
    "# for k in mean_lr:\n",
    "strat=time.time()\n",
    "for k in mean_1.reshape(-1):\n",
    "    result=np.mean(evaluate_model(np.array(x_tr),y_tr.values,k),axis=1)\n",
    "    metric_res = {'window':10+5*c,'upper_mean': k}\n",
    "    c+=1\n",
    "    for name, value in zip(metrics_names, result):\n",
    "#             print(name, ': ', value)\n",
    "            metric_res[name] = value\n",
    "       \n",
    "\n",
    "    all_results.append(metric_res)\n",
    "end=time.time()\n",
    "end-strat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_names=['Balanced_acc','F1_score','Recall','Precision']\n",
    "def evaluate_modelxh(X,y,m,model=solution_XX()):\n",
    "    \n",
    "    bacc,f1,rec,pre=list(),list(),list(),list()\n",
    "   \n",
    "\n",
    "    for train_index,test_index in cv_.split(X,y):\n",
    "        \n",
    "        pre_X,pre_Y=split(X[train_index],y[train_index])\n",
    "        start=time.time()\n",
    "        w_x= model.fit(pre_X,pre_Y)\n",
    "        end=time.time()\n",
    "        \n",
    "        \n",
    "        prob_y=sigmoid_(X[test_index].dot(w_x),m)\n",
    "        \n",
    "        prdict_y=np.round(prob_y)\n",
    "        rec.append(recall_score(y[test_index],prdict_y))\n",
    "        pre.append(precision_score(y[test_index],prdict_y))\n",
    "\n",
    "        bacc.append(balanced_accuracy_score(y[test_index],prdict_y))\n",
    "        f1.append(fbeta_score(y[test_index],prdict_y,beta=2))\n",
    " \n",
    "    return bacc,f1,rec,pre\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_resultsxh = []\n",
    "\n",
    "for k in meanxh:\n",
    "    result=np.mean(evaluate_modelxh(X_train,y_tr.values,k),axis=1)\n",
    "    metric_res = {'upper_mean': k}\n",
    "    for name, value in zip(metrics_names, result):\n",
    "#             print(name, ': ', value)\n",
    "            metric_res[name] = value\n",
    "       \n",
    "\n",
    "    all_resultsxh.append(metric_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.62      0.76       138\n",
      "         1.0       0.02      1.00      0.04         1\n",
      "\n",
      "    accuracy                           0.62       139\n",
      "   macro avg       0.51      0.81      0.40       139\n",
      "weighted avg       0.99      0.62      0.76       139\n",
      "\n",
      "Classifier : mean-unceratin max-mean \n",
      "Running time:3.65 s\n",
      "Balanced-Accuracy on testing set：80.80%\n",
      "Recall on testing set：100.00%\n",
      "F-measure on testing set：3.64%\n",
      "Score:0.599\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_preX_x=sigmoid_(np.dot(X_test,w_2).reshape(x_te.shape[0],),0.14)\n",
    "\n",
    "y_preX_x[y_preX_x<0.5]=0\n",
    "y_preX_x[y_preX_x>=0.5]=1\n",
    "print(classification_report(y_te,y_preX_x))\n",
    "bacc=balanced_accuracy_score(y_te,y_preX_x)\n",
    "f1=f1_score(y_te,y_preX_x)\n",
    "recall=recall_score(y_te,y_preX_x)\n",
    "print('Classifier : mean-unceratin max-mean ')\n",
    "print(\"Running time:%.2f s\"%(end_x-start_x))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_preX_x)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_preX_x)))\n",
    "print('F-measure on testing set：{:.2%}'.format(f1_score(y_te,y_preX_x)\n",
    "                                              ))\n",
    "print('Score:{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end_x-start_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_fix=x_tr\n",
    "# y_fix=y_tr##k=89时max-mean优于CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(229,)"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trr[y_trr==1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(445,)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr[y_tr==0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "445/89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_u(x,arr):\n",
    "    p=[]\n",
    "    for c in arr:\n",
    "        \n",
    "        if c <0:\n",
    "            p.append(2*x[1]*st.norm.cdf(c/x[1])/(x[0]+x[1]))\n",
    "        else:\n",
    "            p.append(1-2*x[0]*st.norm.cdf(-c/x[0])/(x[0]+x[1]))\n",
    "    return np.array(p)\n",
    "def F_L(x,arr):\n",
    "    p=[]\n",
    "    for c in arr:\n",
    "        \n",
    "        if c <0:\n",
    "            p.append(2*x[0]*st.norm.cdf(c/x[0])/(x[0]+x[1]))\n",
    "        else:\n",
    "            p.append(1-2*x[1]*st.norm.cdf(-c/x[1])/(x[0]+x[1]))\n",
    "    return np.array(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equa_vxh(x,n):\n",
    "    pre_in=np.dot(X_train,w_2).reshape(x_tr.shape[0],)\n",
    "    err_u=y_tr-F_u(x,pre_in)\n",
    "    err_L=y_tr-F_L(x,pre_in)\n",
    "#     for k in range(y_tr.shape[0]):\n",
    "    \n",
    "#         if y_tr[k]==1:\n",
    "#             err_u[k]=2*err_u[k]\n",
    "#             err_L[k]=19*err_L[k]\n",
    "    \n",
    "    return np.array([testjx.testjx.meanuncertainty(err_u,n)[1],testjx.testjx.meanuncertainty(err_L,n)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0663078, -0.1309536])"
      ]
     },
     "execution_count": 853,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equa_vxh([0.1,0.3],100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_names=['Balanced_acc','F2_score','Recall','Precision']\n",
    "def evaluate_varxh(X,y,m,model=solution_XX()):\n",
    "    \n",
    "    bacc,f2,rec,pre=list(),list(),list(),list()\n",
    "   \n",
    "\n",
    "    for train_index,test_index in cv_.split(X,y):\n",
    "        pre_X,pre_Y=split(X[train_index],y[train_index])\n",
    "        w_x= model.fit(pre_X,pre_Y)\n",
    "        \n",
    "        prob_y=F_u(m,X[test_index].dot(w_x))\n",
    "        \n",
    "        prdict_y=np.round(prob_y)\n",
    "        rec.append(recall_score(y[test_index],prdict_y))\n",
    "        pre.append(precision_score(y[test_index],prdict_y))\n",
    "\n",
    "        bacc.append(balanced_accuracy_score(y[test_index],prdict_y))\n",
    "        f2.append(fbeta_score(y[test_index],prdict_y,beta=2))\n",
    "    return bacc,f2,rec,pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_resultsxhvar = []\n",
    "\n",
    "for k in np.array(var_xhmean):\n",
    "    result=np.mean(evaluate_varxh(X_train,y_tr.values,k),axis=1)\n",
    "    metric_res = {'lower-var': k[0],'upper-var':k[1]}\n",
    "    for name, value in zip(metrics_names, result):\n",
    "#             print(name, ': ', value)\n",
    "            metric_res[name] = value\n",
    "       \n",
    "\n",
    "    all_resultsxhvar.append(metric_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(all_resultsxhvar).to_csv('diabetesCV_which_sigma_to_set_inxhBIAS.csv')\n",
    "pd.DataFrame(all_resultsxhvar).to_csv('pre_diabetesCV_which_sigma_to_set_inxhBIAS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.39308311, 10.75018256])"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fsolve(lambda x:equa_v(x,400),[0.5,1.5] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69.1"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sample.shape[0]/sum(y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equa_v(x,n):\n",
    " \n",
    "    pre_in=(np.dot(x_tr,model_in.coef_.T)+model_in.intercept_).reshape(x_tr.shape[0],)\n",
    "    err_u=y_tr-F_u(x,pre_in)\n",
    "    err_L=y_tr-F_L(x,pre_in)\n",
    "    for k in range(y_tr.shape[0]):\n",
    "    \n",
    "        if y_tr[k]==1:\n",
    "            err_u[k]=30*err_u[k]\n",
    "            err_L[k]=30*err_L[k]\n",
    "    \n",
    "    return np.array([testjx.testjx.meanuncertainty(err_u,n)[1],testjx.testjx.meanuncertainty(err_L,n)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "552"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "##LR的波动率不确定性\n",
    "var_lr=np.array([fsolve(lambda x:equa_v(x,n),[0.5,1.5] ) for n in range(50,200,5)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "##max-mean的波动率不确定性\n",
    "# var_xh=np.array([fsolve(lambda x:equa_vxh(x,n),[0.5,1.5] ) for n in range(50,450,5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(var_lr).to_csv('/Users/lvjingzhe/Desktop/璇/modified_althogram/code/0512_lrvar_mean_wine.csv')\n",
    "# pd.DataFrame(var_xh).to_csv('/Users/lvjingzhe/Desktop/璇/modified_althogram/code/0512_xhvar_mean_wine.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.64644298, 2.0587135 ])"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fsolve(lambda x:equa_v(x,160),[0.5,1.5] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_evaluate_model(X,y,m,model=LogisticRegression()):\n",
    "    \n",
    "    bacc,f2,rec,pre=list(),list(),list(),list()\n",
    "   \n",
    "#   \n",
    "    for train_index,test_index in cv_.split(X,y):\n",
    "#                 X_resampled_smote,y_resampled_smote=SMOTE().fit_resample(X[train_index], y[train_index])\n",
    "                model.fit(X[train_index],y[train_index])\n",
    "                prob_y=F_u(m,(np.dot(X[test_index],model.coef_.T)+model.intercept_).reshape(X[test_index].shape[0],))\n",
    "        \n",
    "#                 auc.append(roc_auc_score(y[test_index],prob_y))\n",
    "                \n",
    "                prdict_y=np.round(prob_y)\n",
    "                rec.append(recall_score(y[test_index],prdict_y))\n",
    "                pre.append(precision_score(y[test_index],prdict_y))\n",
    "                \n",
    "                bacc.append(balanced_accuracy_score(y[test_index],prdict_y))\n",
    "                f2.append(fbeta_score(y[test_index],prdict_y,beta=2))\n",
    " \n",
    "    return bacc,f2,rec,pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.76      0.86       138\n",
      "         1.0       0.03      1.00      0.06         1\n",
      "\n",
      "    accuracy                           0.76       139\n",
      "   macro avg       0.51      0.88      0.46       139\n",
      "weighted avg       0.99      0.76      0.86       139\n",
      "\n",
      "Classifier : volatility-unceratin LR \n",
      "Running time:0.01\n",
      "Balanced-Accuracy on testing set：88.04%\n",
      "Recall on testing set：100.00%\n",
      "F-measure on testing set：13.16%\n",
      "Score:1.95\n"
     ]
    }
   ],
   "source": [
    "y_preX_v=F_u([0.008,5.809],np.dot(x_te,model_in.coef_.T)+model_in.intercept_).reshape(x_te.shape[0],)\n",
    "\n",
    "y_preX_v[y_preX_v<0.5]=0\n",
    "y_preX_v[y_preX_v>=0.5]=1\n",
    "print(classification_report(y_te,y_preX_v))\n",
    "bacc=balanced_accuracy_score(y_te,y_preX_v)\n",
    "f1=f1_score(y_te,y_preX_v)\n",
    "recall=recall_score(y_te,y_preX_v)\n",
    "print('Classifier : volatility-unceratin LR ')\n",
    "print(\"Running time:%.2f\"%(end_i-start_i))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_preX_v)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_preX_v)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_preX_v,beta=2)))\n",
    "print('Score:{:.3}'.format(np.mean([recall,bacc,f1])*0.99+0.01/(end_i-start_i)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.60      0.71        43\n",
      "           1       0.63      0.88      0.73        33\n",
      "\n",
      "    accuracy                           0.72        76\n",
      "   macro avg       0.75      0.74      0.72        76\n",
      "weighted avg       0.76      0.72      0.72        76\n",
      "\n",
      "Classifier : volatility-unceratin max-mean \n",
      "Running time:0.49\n",
      "Balanced-Accuracy on testing set：74.17%\n",
      "Recall on testing set：87.88%\n",
      "F-measure on testing set：73.42%\n",
      "Score:0.798\n"
     ]
    }
   ],
   "source": [
    "y_preX_v=F_u([0.018,0.698],np.dot(X_test,w_2)).reshape(x_te.shape[0],)\n",
    "\n",
    "y_preX_v[y_preX_v<0.5]=0\n",
    "y_preX_v[y_preX_v>=0.5]=1\n",
    "print(classification_report(y_te,y_preX_v))\n",
    "bacc=balanced_accuracy_score(y_te,y_preX_v)\n",
    "fbeta=f1_score(y_te,y_preX_v)\n",
    "recall=recall_score(y_te,y_preX_v)\n",
    "print('Classifier : volatility-unceratin max-mean ')\n",
    "print(\"Running time:%.2f\"%(end_x-start_x))\n",
    "\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_preX_v)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_preX_v)))\n",
    "print('F-measure on testing set：{:.2%}'.format(f1_score(y_te,y_preX_v)))\n",
    "print('Score:{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end_x-start_x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
