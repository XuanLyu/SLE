{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import skimage\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import preprocessing\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from os import listdir\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "from collections import Counter\n",
    "\n",
    "sn.set()\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC # SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "from xgboost import XGBClassifier # XGBClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score,f1_score,recall_score,cohen_kappa_score,precision_score\n",
    "from sklearn.utils import compute_class_weight\n",
    "from sklearn.preprocessing import MinMaxScaler,LabelBinarizer\n",
    "from sklearn.ensemble import AdaBoostClassifier # AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier # KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier # RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.compat.v1 as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.applications.vgg16 import VGG16 # VGG16\n",
    "from tensorflow.keras.applications.vgg19 import VGG19 # VGG19\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50 # ResNet50\n",
    "from tensorflow.keras.applications.xception import Xception # Xception\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet # MobileNet\n",
    "from tensorflow.keras.applications.nasnet import NASNetMobile # NASNetMobile\n",
    "from tensorflow.keras.applications.densenet import DenseNet169 # DenseNet169\n",
    "from tensorflow.keras.applications.densenet import DenseNet121 # DenseNet121\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2 # MobileNetV2\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3 # InceptionV3\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Flatten, Activation, GlobalAveragePooling2D,Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images=[]\n",
    "labels=[]\n",
    "feature_dictionary = {\n",
    "    'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'label_normal': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'image': tf.io.FixedLenFeature([], tf.string)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8941,)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandas数据经典模型比较\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/mammography.csv'\n",
    "# load the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Breast.to_csv('Breast.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Breast['lable']=LabelEncoder().fit_transform(Breast[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.230020</td>\n",
       "      <td>5.072578</td>\n",
       "      <td>-0.276061</td>\n",
       "      <td>0.832444</td>\n",
       "      <td>-0.377866</td>\n",
       "      <td>0.480322</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.155491</td>\n",
       "      <td>-0.169390</td>\n",
       "      <td>0.670652</td>\n",
       "      <td>-0.859553</td>\n",
       "      <td>-0.377866</td>\n",
       "      <td>-0.945723</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.784415</td>\n",
       "      <td>-0.443654</td>\n",
       "      <td>5.674705</td>\n",
       "      <td>-0.859553</td>\n",
       "      <td>-0.377866</td>\n",
       "      <td>-0.945723</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.546088</td>\n",
       "      <td>0.131415</td>\n",
       "      <td>-0.456387</td>\n",
       "      <td>-0.859553</td>\n",
       "      <td>-0.377866</td>\n",
       "      <td>-0.945723</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.102987</td>\n",
       "      <td>-0.394994</td>\n",
       "      <td>-0.140816</td>\n",
       "      <td>0.979703</td>\n",
       "      <td>-0.377866</td>\n",
       "      <td>1.013566</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         0         1         2         3         4         5  6\n",
       "0           0  0.230020  5.072578 -0.276061  0.832444 -0.377866  0.480322  0\n",
       "1           1  0.155491 -0.169390  0.670652 -0.859553 -0.377866 -0.945723  0\n",
       "2           2 -0.784415 -0.443654  5.674705 -0.859553 -0.377866 -0.945723  0\n",
       "3           3  0.546088  0.131415 -0.456387 -0.859553 -0.377866 -0.945723  0\n",
       "4           4 -0.102987 -0.394994 -0.140816  0.979703 -0.377866  1.013566  0"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Breast =pd.read_csv('/Users/lvjingzhe/Desktop/璇/modified_althogram/dataset/Breast.csv',)\n",
    "Breast['6']=LabelEncoder().fit_transform(Breast['6'])\n",
    "X=Breast.values[0:,1:7]\n",
    "y=Breast['6'].values\n",
    "Breast.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample=np.array(Breast[['0','1','2','3','4','5']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_sample=Breast['lable'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11183, 6)"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E=np.array(Breast.iloc[:,1:])\n",
    "np.random.seed(4123)\n",
    "np.random.shuffle(E)\n",
    "X_sample=E[:,:-1]\n",
    "y_sample=E[:,-1]\n",
    "X_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.78441482, -0.47019533, -0.59163147, -0.85955255, -0.37786573,\n",
       "        -0.94572324],\n",
       "       [-0.06894128, -0.36845248, -0.14081588, -0.85955255, -0.37786573,\n",
       "        -0.94572324],\n",
       "       [ 0.03810874, -0.42153571,  0.12967348, -0.85955255, -0.37786573,\n",
       "        -0.94572324],\n",
       "       ...,\n",
       "       [-0.17345055, -0.33748727,  0.62557063,  0.66493223, -0.37786573,\n",
       "         1.1141203 ],\n",
       "       [ 0.48189204,  0.02524812, -0.36622367,  0.23665876, -0.37786573,\n",
       "         1.0379427 ],\n",
       "       [ 1.1296124 ,  1.4938841 , -0.09573432, -0.85955255, -0.37786573,\n",
       "        -0.94572324]])"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sample[y_sample==1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8947, 6)"
      ]
     },
     "execution_count": 557,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_te.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43.011538461538464"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sample.shape[0]/sum(y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, x_te, y_tr, y_te = train_test_split(X_sample,y_sample,test_size = 0.8,\n",
    "                                                  shuffle = True,\n",
    "                                                  random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=0)\n",
    "# for tr_index,te_index in fold.split(X,y):\n",
    "#     print(te_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import scipy.stats as st\n",
    "from sympy.solvers import solve\n",
    "from sympy import Symbol\n",
    "from scipy.optimize import fsolve\n",
    "import testjx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.c_[x_tr,np.ones(x_tr.shape[0])]\n",
    "X_test=np.c_[x_te,np.ones(x_te.shape[0])]\n",
    "X_sample_XX=np.c_[X_sample,np.ones(X_sample.shape[0])]\n",
    "def split(x,y):\n",
    "    k=int(y.shape[0]/sum(y))-1\n",
    "    n=y[y==1].shape[0]\n",
    "    m=int((x.shape[0]-n)/k)\n",
    "    indice=[i*m for i in range(1,k)]\n",
    "    indice.append(x.shape[0]-n)\n",
    "    x_c=np.concatenate((x[y==0],x[y==1]))\n",
    "    y_c=np.concatenate((np.array([0]*(x.shape[0]-n)),np.array([1]*n)))\n",
    "    return np.array(np.split(x_c,indice)),np.array(np.split(y_c,indice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample_ord=np.concatenate((X_sample[y_sample==0],X_sample[y_sample==1]))\n",
    "y_sample_ord=np.concatenate((y_sample[y_sample==0],y_sample[y_sample==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      0.99      8742\n",
      "         1.0       0.77      0.42      0.54       205\n",
      "\n",
      "    accuracy                           0.98      8947\n",
      "   macro avg       0.88      0.71      0.77      8947\n",
      "weighted avg       0.98      0.98      0.98      8947\n",
      "\n",
      "Running time:0.02 s\n",
      "Balanced-Accuracy on testing set：70.83%\n",
      "Recall on testing set：41.95%\n",
      "F-measure on testing set：46.14%\n",
      "Accuracy on training set：98.08%\n",
      "Accuracy on testing set：98.38%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_i=time.time()\n",
    "model_in=LogisticRegression()\n",
    "model_in.fit(x_tr,y_tr)\n",
    "# model_in.fit(X_resampled_smote,y_resampled_smote)\n",
    "end_i=time.time()\n",
    "y_train_proba=model_in.predict_proba(x_tr)\n",
    "y_train_label=model_in.predict(x_tr)\n",
    "y_test_proba=model_in.predict_proba(x_te)\n",
    "\n",
    "y_test_label=model_in.predict(x_te)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_te,y_test_label))\n",
    "print(\"Running time:%.2f s\"%(end_i-start_i))\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_test_label)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_test_label)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_test_label,beta=2)))\n",
    "print('Accuracy on training set：{:.2%}'.format(accuracy_score(y_tr,y_train_label)))\n",
    "print('Accuracy on testing set：{:.2%}'.format(accuracy_score(y_te,y_test_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Poly_close_solution:\n",
    "    def __init__(self,kernel='self._poly',penalty = None,gamma=1,degree = 2,coef0=0,alpha=1,beta=1):\n",
    "        self.W = None\n",
    "        self.penalty = penalty\n",
    "        self.d = degree\n",
    "        self.b = coef0\n",
    "        self.alpha=alpha\n",
    "        self.beta=beta\n",
    "        self.g=gamma\n",
    "        self.kernel=kernel\n",
    "        \n",
    "    def _poly(self,X_1,X_2):\n",
    "        return (self.g*(X_1.dot(X_2.T))+self.b)**self.d\n",
    "    def _rbf(self,x,y):\n",
    "        return np.exp(-self.g*np.sum((x[...,None,:]-y)**2,axis=2))\n",
    "\n",
    "    def poly_B_matrix(self,X,Y):\n",
    "        n=Y[Y==1].shape[0]\n",
    "        X_sample_split=np.array(np.split(X,np.array([X.shape[0]-n])))\n",
    "        B=np.zeros((X.shape[0],X.shape[0]))\n",
    "        I=0\n",
    "        for  m in X_sample_split:\n",
    "            I+=m.shape[0]\n",
    "            B[I-m.shape[0]:I,I-m.shape[0]:I]=eval(self.kernel+'(m,m)')/m.shape[0]\n",
    "        return B\n",
    "    \n",
    "    def K_x(self,X,x_t):\n",
    "        return eval(self.kernel+'(X,x_t)')\n",
    "    \n",
    "    def QplusBeta(self,X,Y):\n",
    "        s=time.time()\n",
    "        K=eval(self.kernel+'(X,X)')\n",
    "        H_p=np.diag(np.diagonal(K))\n",
    "        B_p=self.poly_B_matrix(X,Y)\n",
    "        Qplus_beta=K+self.alpha*(H_p-B_p)+np.diag([self.beta]*X.shape[0])\n",
    "        e=time.time()\n",
    "        return K,Qplus_beta\n",
    "    def fit(self,invQ,X,x_t):\n",
    "        \n",
    "        \n",
    "        W=invQ.dot(self.K_x(X,x_t))##求逆耗时\n",
    "        \n",
    "        return W.reshape(-1)\n",
    "    def delta_phi(self,K,invQ,X,Y,x_t,j):\n",
    "        \n",
    "        w=self.fit(invQ,X,x_t)\n",
    "        w_noty=w\n",
    "\n",
    "        w_y=np.zeros((X.shape[0]))\n",
    "        indice=np.argwhere(Y==j).reshape(-1)\n",
    "        w_noty[indice]=0\n",
    "        w_y[indice]=w[indice]\n",
    "        delta=np.dot(w_y,K.dot(w_y))+np.dot(w_noty,K.dot(w_noty))-2*np.dot(w_y,self.K_x(X,x_t))\n",
    "        return delta\n",
    "    def predict(self,K,invQ,X,Y,x_t):\n",
    "        delta=[]\n",
    "        for k in np.unique(Y):\n",
    "            delta.append(self.delta_phi(K,invQ,X,Y,x_t,k))\n",
    "        return np.argmin(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1788,)\n",
      "(1789,)\n",
      "(1789,)\n",
      "(1789,)\n",
      "(1789,)\n"
     ]
    }
   ],
   "source": [
    "for train_index,test_index in cv.split(X_sample_ord,y_sample_ord):\n",
    "    print(y_sample_ord[train_index].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2236,)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sample_ord.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "from numpy.linalg import inv,det\n",
    "metrics_names=['G-mean','Balanced_acc','F1_score','Recall','Precision']\n",
    "def DRM_FiveCV(X,y,model=Poly_close_solution(\n",
    "                                )):\n",
    "    \n",
    "    G_m,bacc,f2,rec,pre=list(),list(),list(),list(),list()\n",
    "    for train_index,test_index in cv.split(X,y):\n",
    "                \n",
    "                K,Q_beta=model.QplusBeta(X[train_index],y[train_index])\n",
    "                invQ=inv(Q_beta)\n",
    "                y_test_label=list()\n",
    "                for i in range(y[test_index].shape[0]):\n",
    "#                 X_resampled_smote,y_resampled_smote=SMOTE().fit_resample(X[train_index], y[train_index])\n",
    "                    y_test_label.append(model.predict(K,invQ,X[train_index],y[train_index],X[test_index][i]))\n",
    "#                 y_train_proba=model.predict_proba(X[train_index])\n",
    "#                 y_train_label=model.predict(X[train_index])\n",
    "                \n",
    "#                 print(\"running time for each CV:%.2f s\"%(t2-t1))\n",
    "                \n",
    "#     print(classification_report(y,y_pre))\n",
    "                rec.append(recall_score(y[test_index],y_test_label))\n",
    "                pre.append(precision_score(y[test_index],y_test_label))\n",
    "                G_m.append(math.sqrt(recall_score(y[test_index],y_test_label)*recall_score(y[test_index],y_test_label,pos_label=0)))\n",
    "                bacc.append(balanced_accuracy_score(y[test_index],y_test_label))\n",
    "                f2.append(fbeta_score(y[test_index],y_test_label,beta=3.66))\n",
    " \n",
    "    return G_m,bacc,f2,rec,pre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "running time for poly_DRM:0.85 s\n"
     ]
    }
   ],
   "source": [
    "P_model=Poly_close_solution(alpha=0.01,beta=10,degree=1)\n",
    "start_d=time.time()\n",
    "K,Q_beta=P_model.QplusBeta(X_sample_ord,y_sample_ord)\n",
    "invQ=inv(Q_beta)\n",
    "y0=P_model.predict(K,invQ,X_sample_ord,y_sample_ord,x_te[891])\n",
    "end_d=time.time()\n",
    "print(y0)\n",
    "print(\"running time for poly_DRM:%.2f s\"%(end_d-start_d))\n",
    "\n",
    "\n",
    "\n",
    "# y_L1=[]\n",
    "# for i in range(y_te.shape[0]):\n",
    "#     y_L1.append(P_model.predict(K,invQ,faceX_sample_ord,facey_sample_ord,x_te[i]))\n",
    "# end_d=time.time()\n",
    "# print(y0)\n",
    "# print(\"running time for poly_DRM:%.2f s\"%(end_d-start_d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRM_L_fivefold-cv time:2009.10\n"
     ]
    }
   ],
   "source": [
    "degree=np.array([2,3,4,5,8,10])##tr:te=8:2\n",
    "gamma=np.array([0.001,0.01,0.1,1,10])\n",
    "parameter=np.array([0.01,0.1,1,10,100])\n",
    "all_results_Ldrm = []\n",
    "import time\n",
    "import math\n",
    "# for k in mean_lr:\n",
    "start=time.time()\n",
    "\n",
    "for k in parameter:\n",
    "    for p in parameter:\n",
    "        for d in degree:\n",
    "            Lo_score=np.mean(DRM_FiveCV(X_sample_ord,y_sample_ord,model=Poly_close_solution(alpha=k,beta=p,degree=d\n",
    "                                )),axis=1)\n",
    "\n",
    "            metric_res = {'alpha':k,'beta':p,'degree':d}\n",
    "\n",
    "            for name, value in zip(metrics_names, Lo_score):\n",
    "        #             print(name, ': ', value)\n",
    "                    metric_res[name] = value\n",
    "\n",
    "\n",
    "            all_results_Ldrm.append(metric_res)\n",
    "end=time.time()\n",
    "print(\"DRM_L_fivefold-cv time:%.2f\"%(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(all_results_Ldrm).to_csv('/Users/lvjingzhe/Desktop/璇/modified_althogram/code/0510CV_select_DRML_cancer.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree=np.array([2,3,4,5,8,10])\n",
    "gamma=np.array([0.001,0.01,0.1,1,10])\n",
    "parameter=np.array([0.01,0.1,1,10,100])\n",
    "def fivetrials_DRM(X,y):\n",
    "    \n",
    "    \n",
    "    G_m,bacc,fbeta,rec,pre,T=list(),list(),list(),list(),list(),list()\n",
    "    \n",
    "    for train_index,test_index in cv.split(X, y):\n",
    "#         all_results_Ldrm = []\n",
    "#         for k in parameter:\n",
    "#             for p in parameter:\n",
    "#                 for d in degree:\n",
    "#                     Lo_score=np.mean(DRM_FiveCV(X[train_index],y[train_index],model=Poly_close_solution(alpha=k,beta=p,degree=d\n",
    "#                                         )),axis=1)\n",
    "\n",
    "#                     metric_res = {'alpha':k,'beta':p,'degree':d}\n",
    "\n",
    "#                     for name, value in zip(metrics_names, Lo_score):\n",
    "#                 #             print(name, ': ', value)\n",
    "#                             metric_res[name] = value\n",
    "\n",
    "\n",
    "#                     all_results_Ldrm.append(metric_res)\n",
    "#         eva=pd.DataFrame(all_results_Ldrm)\n",
    "#         bias=eva[eva.iloc[:,-5]==(eva.iloc[:,-5]).max()]\n",
    "#         al,be,de=np.array(bias[['alpha','beta','degree']])[0]\n",
    "        start=time.time()\n",
    "        P_model=Poly_close_solution(alpha=0.1,beta=100,degree=4)\n",
    "        K,Q_beta=P_model.QplusBeta(X[test_index],y[test_index])\n",
    "        invQ=inv(Q_beta)\n",
    "        prdict_y=[]\n",
    "        for i in range(y[train_index].shape[0]):\n",
    "            prdict_y.append(P_model.predict(K,invQ,X[test_index],y[test_index],X[train_index][i]))\n",
    "        end=time.time()\n",
    "        \n",
    "        print(classification_report(y[train_index],prdict_y))\n",
    "        rec.append(recall_score(y[train_index],prdict_y))\n",
    "        G_m.append(np.sqrt(recall_score(y[train_index],prdict_y)*recall_score(y[train_index],prdict_y,pos_label=0)))\n",
    "        pre.append(precision_score(y[train_index],prdict_y))\n",
    "    #     auc.append(roc_auc_score(y[test_index],prob_y[:,-1]))\n",
    "        bacc.append(balanced_accuracy_score(y[train_index],prdict_y))\n",
    "        fbeta.append(fbeta_score(y[train_index],prdict_y,beta=max(2,np.log(y_sample.shape[0]/sum(y_sample)-1))))\n",
    "        T.append(end-start)\n",
    "#     pipeline = Pipeline(steps=steps)\n",
    "   \n",
    "#     print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_L)))\n",
    "#     print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_L)))\n",
    "#     print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_L,beta=3.66)))\n",
    "#     print('Score :{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end_d-start_d)))\n",
    "\n",
    "\n",
    "\n",
    "    return G_m,bacc,fbeta,rec,pre,T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99      8738\n",
      "         1.0       0.56      0.45      0.50       208\n",
      "\n",
      "    accuracy                           0.98      8946\n",
      "   macro avg       0.77      0.72      0.75      8946\n",
      "weighted avg       0.98      0.98      0.98      8946\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      0.99      8738\n",
      "         1.0       0.70      0.39      0.50       208\n",
      "\n",
      "    accuracy                           0.98      8946\n",
      "   macro avg       0.84      0.69      0.75      8946\n",
      "weighted avg       0.98      0.98      0.98      8946\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99      8738\n",
      "         1.0       0.55      0.45      0.49       208\n",
      "\n",
      "    accuracy                           0.98      8946\n",
      "   macro avg       0.77      0.72      0.74      8946\n",
      "weighted avg       0.98      0.98      0.98      8946\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.99      0.99      8739\n",
      "         1.0       0.54      0.42      0.48       208\n",
      "\n",
      "    accuracy                           0.98      8947\n",
      "   macro avg       0.76      0.71      0.73      8947\n",
      "weighted avg       0.98      0.98      0.98      8947\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      1.00      0.99      8739\n",
      "         1.0       0.69      0.43      0.53       208\n",
      "\n",
      "    accuracy                           0.98      8947\n",
      "   macro avg       0.84      0.71      0.76      8947\n",
      "weighted avg       0.98      0.98      0.98      8947\n",
      "\n",
      ">DRM:: Average G-mean:0.652(0.017) \n",
      ">DRM:: Average Balanced_Acc: 0.711(0.010) \n",
      ">DRM:: Average Fbeta: 0.437(0.020)\n",
      ">DRM:: Average Recall: 0.429(0.022)\n",
      ">DRM:: Average Training time: 149.388(10.203)\n",
      ">DRM:: Average accuracy_score: 0.557(0.017)\n",
      ">DRM:: Average Score: 0.562(0.017)\n"
     ]
    }
   ],
   "source": [
    "result_DRM = fivetrials_DRM(X_sample_ord,y_sample_ord)\n",
    "\n",
    "G_mean=result_DRM[0]\n",
    "Bacc=result_DRM[1]\n",
    "# summarize performance\n",
    "recall=result_DRM[3]\n",
    "Fbeta=result_DRM[2]\n",
    "T=result_DRM[-1]\n",
    "acc_s=np.mean(np.array(result_DRM)[:4,:],axis=0)\n",
    "\t# summarize and store\n",
    "print('>%s: Average G-mean:%.3f(%.3f) ' % ('DRM:',np.mean(G_mean),np.std(G_mean)))\n",
    "print('>%s: Average Balanced_Acc: %.3f(%.3f) ' % ('DRM:',np.mean(Bacc),np.std(Bacc)))\n",
    "print('>%s: Average Fbeta: %.3f(%.3f)' % ('DRM:',np.mean(Fbeta),np.std(Fbeta)))\n",
    "print('>%s: Average Recall: %.3f(%.3f)' % ('DRM:',np.mean(recall),np.std(recall)))    \n",
    "print('>%s: Average Training time: %.3f(%.3f)' % ('DRM:',np.mean(T),np.std(T)))\n",
    "print('>%s: Average accuracy_score: %.3f(%.3f)' % ('DRM:',np.mean(acc_s),np.std(acc_s)))\n",
    "print('>%s: Average Score: %.3f(%.3f)' % ('DRM:',np.mean(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1)),np.std(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1))))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------Model: Poly_DRM--------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      8743\n",
      "           1       0.45      0.47      0.46       204\n",
      "\n",
      "    accuracy                           0.97      8947\n",
      "   macro avg       0.72      0.73      0.72      8947\n",
      "weighted avg       0.98      0.97      0.98      8947\n",
      "\n",
      "Balanced-Accuracy on testing set：72.87%\n",
      "G-mean on testing set：68.14%\n",
      "Recall on testing set：47.06%\n",
      "F-measure on testing set：46.93%\n",
      "Score :0.582\n",
      "Running time for L_DRM:75.68 s\n"
     ]
    }
   ],
   "source": [
    "P_model=Poly_close_solution(alpha=0.01,beta=100,degree=4)\n",
    "start_d=time.time()\n",
    "K,Q_beta=P_model.QplusBeta(X_sample_ord,y_sample_ord)\n",
    "invQ=inv(Q_beta)\n",
    "y_L=[]\n",
    "for i in range(y_te.shape[0]):\n",
    "    y_L.append(P_model.predict(K,invQ,X_sample_ord,y_sample_ord,x_te[i]))\n",
    "end_d=time.time()\n",
    "print('--------------Model: Poly_DRM--------------')\n",
    "print(classification_report(y_te,y_L))\n",
    "\n",
    "\n",
    "# from sklearn.metrics import accuracy_score\n",
    "bacc=balanced_accuracy_score(y_te,y_L)\n",
    "recall=recall_score(y_te,y_L)\n",
    "fbeta=fbeta_score(y_te,y_L,beta=3.66)\n",
    "G_m=np.sqrt((recall_score(y_te,y_L)*recall_score(y_te,y_L,pos_label=0)))\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_L)))\n",
    "print('G-mean on testing set：{:.2%}'.format(G_m))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_L)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_L,beta=3.66)))\n",
    "print('Score :{:.3}'.format(np.mean([G_m,recall,bacc,fbeta])*0.99+0.01/((end_d-start_d)/10*9+1)))\n",
    "\n",
    "\n",
    "print(\"Running time for L_DRM:%.2f s\"%(end_d-start_d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(50,), random_state=0)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "MLPClassifier(random_state=0, max_iter=200,hidden_layer_sizes=(50,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "def get_models():\n",
    "\tmodels, names = list(), list()\n",
    "\t# LR\n",
    "\tmodels.append(LogisticRegression())\n",
    "\tnames.append('LR')\n",
    "\t# SVM\n",
    "\tmodels.append(SVC(probability=True))\n",
    "\tnames.append('SVM')\n",
    "\t# Bagging\n",
    "\tmodels.append(BaggingClassifier(n_estimators=1000))\n",
    "\tnames.append('BAG')\n",
    "\tmodels.append(AdaBoostClassifier(random_state=0))\n",
    "\tnames.append('Adaboost')\n",
    "\t# RF\n",
    "\tmodels.append(\n",
    "\ttree.DecisionTreeClassifier(max_leaf_nodes=6,\n",
    "                               random_state=0))\n",
    "\tnames.append('CART')\n",
    "\tmodels.append(RandomForestClassifier(criterion='entropy',n_estimators=6))\n",
    "\tnames.append('RF')\n",
    "\t# GBM\n",
    "\tmodels.append(MLPClassifier(random_state=0, max_iter=200,hidden_layer_sizes=(50,)))\n",
    "    \n",
    "\tnames.append('MLP')\n",
    "# \tmodels.append(solution_XX())\n",
    "# \tnames.append('max-mean loss')\n",
    "\treturn models, names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "import time\n",
    "def fivetrials(X,y,model,sample='none'):\n",
    "    pipeline = model\n",
    "    \n",
    "    G_m,bacc,fbeta,rec,pre,T=list(),list(),list(),list(),list(),list()\n",
    "    if sample!='none':\n",
    "#     steps = [('p', PowerTransformer()), ('m',model)]\n",
    "        for train_index,test_index in cv.split(X, y):\n",
    "        \n",
    "            start=time.time()\n",
    "            resample_x,resample_y=sample.fit_resample(X[test_index],y[test_index])\n",
    "            \n",
    "            pipeline.fit(resample_x,resample_y)\n",
    "            end=time.time()\n",
    "            prdict_y=pipeline.predict(X[train_index])\n",
    "            prob_y=pipeline.predict_proba(X[train_index])\n",
    "            rec.append(recall_score(y[train_index],prdict_y))\n",
    "            G_m.append(np.sqrt(recall_score(y[train_index],prdict_y)*recall_score(y[train_index],prdict_y,pos_label=0)))\n",
    "            pre.append(precision_score(y[train_index],prdict_y))\n",
    "        #     auc.append(roc_auc_score(y[test_index],prob_y[:,-1]))\n",
    "            bacc.append(balanced_accuracy_score(y[train_index],prdict_y))\n",
    "            fbeta.append(fbeta_score(y[train_index],prdict_y,beta=max(2,np.log(y_sample.shape[0]/sum(y_sample)-1))))\n",
    "            T.append(end-start)\n",
    "#     pipeline = Pipeline(steps=steps)\n",
    "    else:\n",
    "        for train_index,test_index in cv.split(X, y):\n",
    "    \n",
    "            start=time.time()\n",
    "            pipeline.fit(X[test_index],y[test_index])\n",
    "            end=time.time()\n",
    "            prdict_y=pipeline.predict(X[train_index])\n",
    "            prob_y=pipeline.predict_proba(X[train_index])\n",
    "            rec.append(recall_score(y[train_index],prdict_y))\n",
    "            G_m.append(np.sqrt(recall_score(y[train_index],prdict_y)*recall_score(y[train_index],prdict_y,pos_label=0)))\n",
    "            pre.append(precision_score(y[train_index],prdict_y))\n",
    "        #     auc.append(roc_auc_score(y[test_index],prob_y[:,-1]))\n",
    "            bacc.append(balanced_accuracy_score(y[train_index],prdict_y))\n",
    "            fbeta.append(fbeta_score(y[train_index],prdict_y,beta=max(2,np.log(y_sample.shape[0]/sum(y_sample)-1))))\n",
    "            T.append(end-start)\n",
    "#     print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_L)))\n",
    "#     print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_L)))\n",
    "#     print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_L,beta=3.66)))\n",
    "#     print('Score :{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end_d-start_d)))\n",
    "\n",
    "\n",
    "\n",
    "    return G_m,bacc,fbeta,rec,pre,T\n",
    "# print('>%s: Mean Balanced_Acc: %.3f (%.3f)' % (names[-1],mean(Bacc),std(Bacc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=object)"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">LR: Average G-mean:0.629(0.013) \n",
      ">LR: Average Balanced_Acc: 0.697(0.008) \n",
      ">LR: Average Fbeta: 0.410(0.016)\n",
      ">LR: Average Recall: 0.397(0.016)\n",
      ">LR: Average Training time: 0.019(0.005)\n",
      ">LR: Average accuracy_score: 0.533(0.013)\n",
      ">LR: Average Score: 0.538(0.013)\n",
      ">SVM: Average G-mean:0.578(0.039) \n",
      ">SVM: Average Balanced_Acc: 0.668(0.022) \n",
      ">SVM: Average Fbeta: 0.350(0.045)\n",
      ">SVM: Average Recall: 0.337(0.044)\n",
      ">SVM: Average Training time: 0.180(0.050)\n",
      ">SVM: Average accuracy_score: 0.483(0.037)\n",
      ">SVM: Average Score: 0.487(0.037)\n",
      ">BAG: Average G-mean:0.695(0.026) \n",
      ">BAG: Average Balanced_Acc: 0.741(0.018) \n",
      ">BAG: Average Fbeta: 0.498(0.036)\n",
      ">BAG: Average Recall: 0.486(0.036)\n",
      ">BAG: Average Training time: 6.314(0.194)\n",
      ">BAG: Average accuracy_score: 0.605(0.029)\n",
      ">BAG: Average Score: 0.600(0.029)\n",
      ">Adaboost: Average G-mean:0.663(0.027) \n",
      ">Adaboost: Average Balanced_Acc: 0.719(0.018) \n",
      ">Adaboost: Average Fbeta: 0.453(0.035)\n",
      ">Adaboost: Average Recall: 0.442(0.036)\n",
      ">Adaboost: Average Training time: 0.181(0.055)\n",
      ">Adaboost: Average accuracy_score: 0.569(0.029)\n",
      ">Adaboost: Average Score: 0.572(0.029)\n",
      ">CART: Average G-mean:0.650(0.033) \n",
      ">CART: Average Balanced_Acc: 0.710(0.021) \n",
      ">CART: Average Fbeta: 0.436(0.041)\n",
      ">CART: Average Recall: 0.425(0.043)\n",
      ">CART: Average Training time: 0.005(0.001)\n",
      ">CART: Average accuracy_score: 0.555(0.034)\n",
      ">CART: Average Score: 0.560(0.034)\n",
      ">RF: Average G-mean:0.614(0.012) \n",
      ">RF: Average Balanced_Acc: 0.688(0.007) \n",
      ">RF: Average Fbeta: 0.391(0.014)\n",
      ">RF: Average Recall: 0.378(0.014)\n",
      ">RF: Average Training time: 0.031(0.004)\n",
      ">RF: Average accuracy_score: 0.518(0.012)\n",
      ">RF: Average Score: 0.522(0.012)\n",
      ">MLP: Average G-mean:0.696(0.021) \n",
      ">MLP: Average Balanced_Acc: 0.742(0.015) \n",
      ">MLP: Average Fbeta: 0.499(0.029)\n",
      ">MLP: Average Recall: 0.487(0.030)\n",
      ">MLP: Average Training time: 2.178(0.269)\n",
      ">MLP: Average accuracy_score: 0.606(0.024)\n",
      ">MLP: Average Score: 0.603(0.024)\n"
     ]
    }
   ],
   "source": [
    " \n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import fbeta_score\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "models, names = get_models()\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# evaluate each model\n",
    "for i in range(len(models)):\n",
    "\t# evaluate the model and store results\n",
    "\tresult = fivetrials(X_sample,y_sample,models[i])\n",
    "\n",
    "\tG_mean=result[0]\n",
    "\tBacc=result[1]\n",
    "# summarize performance\n",
    "\trecall=result[3]\n",
    "\tFbeta=result[2]\n",
    "\tT=result[-1]\n",
    "\tacc_s=np.mean(np.array(result)[:4,:],axis=0)\n",
    "\t# summarize and store\n",
    "\tprint('>%s: Average G-mean:%.3f(%.3f) ' % (names[i],np.mean(G_mean),np.std(G_mean)))\n",
    "\tprint('>%s: Average Balanced_Acc: %.3f(%.3f) ' % (names[i],np.mean(Bacc),np.std(Bacc)))\n",
    "\tprint('>%s: Average Fbeta: %.3f(%.3f)' % (names[i],np.mean(Fbeta),np.std(Fbeta)))\n",
    "\tprint('>%s: Average Recall: %.3f(%.3f)' % (names[i],np.mean(recall),np.std(recall)))    \n",
    "\tprint('>%s: Average Training time: %.3f(%.3f)' % (names[i],np.mean(T),np.std(T)))\n",
    "\tprint('>%s: Average accuracy_score: %.3f(%.3f)' % (names[i],np.mean(acc_s),np.std(acc_s)))\n",
    "\tprint('>%s: Average Score: %.3f(%.3f)' % (names[i],np.mean(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1)),np.std(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1))))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">SMOTELR: Average G-mean:0.880(0.007) \n",
      ">SMOTELR: Average Balanced_Acc: 0.880(0.007) \n",
      ">SMOTELR: Average Fbeta: 0.678(0.016)\n",
      ">SMOTELR: Average Recall: 0.859(0.012)\n",
      ">SMOTELR: Average Training time: 0.022(0.012)\n",
      ">SMOTELR: Average accuracy_score: 0.824(0.009)\n",
      ">SMOTELR: Average Score: 0.826(0.009)\n",
      ">SMOTESVM: Average G-mean:0.881(0.008) \n",
      ">SMOTESVM: Average Balanced_Acc: 0.883(0.008) \n",
      ">SMOTESVM: Average Fbeta: 0.718(0.023)\n",
      ">SMOTESVM: Average Recall: 0.822(0.009)\n",
      ">SMOTESVM: Average Training time: 1.166(0.061)\n",
      ">SMOTESVM: Average accuracy_score: 0.826(0.012)\n",
      ">SMOTESVM: Average Score: 0.822(0.012)\n",
      ">SMOTEBAG: Average G-mean:0.812(0.017) \n",
      ">SMOTEBAG: Average Balanced_Acc: 0.826(0.014) \n",
      ">SMOTEBAG: Average Fbeta: 0.649(0.028)\n",
      ">SMOTEBAG: Average Recall: 0.675(0.028)\n",
      ">SMOTEBAG: Average Training time: 10.418(0.687)\n",
      ">SMOTEBAG: Average accuracy_score: 0.741(0.022)\n",
      ">SMOTEBAG: Average Score: 0.734(0.022)\n",
      ">SMOTEAdaboost: Average G-mean:0.842(0.012) \n",
      ">SMOTEAdaboost: Average Balanced_Acc: 0.848(0.010) \n",
      ">SMOTEAdaboost: Average Fbeta: 0.656(0.015)\n",
      ">SMOTEAdaboost: Average Recall: 0.754(0.026)\n",
      ">SMOTEAdaboost: Average Training time: 0.184(0.003)\n",
      ">SMOTEAdaboost: Average accuracy_score: 0.775(0.015)\n",
      ">SMOTEAdaboost: Average Score: 0.776(0.015)\n",
      ">SMOTECART: Average G-mean:0.845(0.026) \n",
      ">SMOTECART: Average Balanced_Acc: 0.850(0.022) \n",
      ">SMOTECART: Average Fbeta: 0.650(0.027)\n",
      ">SMOTECART: Average Recall: 0.772(0.063)\n",
      ">SMOTECART: Average Training time: 0.008(0.000)\n",
      ">SMOTECART: Average accuracy_score: 0.779(0.033)\n",
      ">SMOTECART: Average Score: 0.782(0.033)\n",
      ">SMOTERF: Average G-mean:0.788(0.021) \n",
      ">SMOTERF: Average Balanced_Acc: 0.807(0.017) \n",
      ">SMOTERF: Average Fbeta: 0.618(0.032)\n",
      ">SMOTERF: Average Recall: 0.631(0.034)\n",
      ">SMOTERF: Average Training time: 0.032(0.001)\n",
      ">SMOTERF: Average accuracy_score: 0.711(0.026)\n",
      ">SMOTERF: Average Score: 0.714(0.026)\n",
      ">SMOTEMLP: Average G-mean:0.874(0.013) \n",
      ">SMOTEMLP: Average Balanced_Acc: 0.878(0.012) \n",
      ">SMOTEMLP: Average Fbeta: 0.724(0.022)\n",
      ">SMOTEMLP: Average Recall: 0.796(0.028)\n",
      ">SMOTEMLP: Average Training time: 2.204(0.079)\n",
      ">SMOTEMLP: Average accuracy_score: 0.818(0.018)\n",
      ">SMOTEMLP: Average Score: 0.813(0.018)\n"
     ]
    }
   ],
   "source": [
    "models, names = get_models()\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# evaluate each model\n",
    "for i in range(len(models)):\n",
    "\t# evaluate the model and store results\n",
    "\tresult = fivetrials(X_sample,y_sample,models[i],sample=SMOTE(random_state=42))\n",
    "\n",
    "\tG_mean=result[0]\n",
    "\tBacc=result[1]\n",
    "# summarize performance\n",
    "\trecall=result[3]\n",
    "\tFbeta=result[2]\n",
    "\tT=result[-1]\n",
    "\tacc_s=np.mean(np.array(result)[:4,:],axis=0)\n",
    "\t# summarize and store\n",
    "\tprint('>%s: Average G-mean:%.3f(%.3f) ' % ('SMOTE'+names[i],np.mean(G_mean),np.std(G_mean)))\n",
    "\tprint('>%s: Average Balanced_Acc: %.3f(%.3f) ' % ('SMOTE'+names[i],np.mean(Bacc),np.std(Bacc)))\n",
    "\tprint('>%s: Average Fbeta: %.3f(%.3f)' % ('SMOTE'+names[i],np.mean(Fbeta),np.std(Fbeta)))\n",
    "\tprint('>%s: Average Recall: %.3f(%.3f)' % ('SMOTE'+names[i],np.mean(recall),np.std(recall)))    \n",
    "\tprint('>%s: Average Training time: %.3f(%.3f)' % ('SMOTE'+names[i],np.mean(T),np.std(T)))\n",
    "\tprint('>%s: Average accuracy_score: %.3f(%.3f)' % ('SMOTE'+names[i],np.mean(acc_s),np.std(acc_s)))\n",
    "\tprint('>%s: Average Score: %.3f(%.3f)' % ('SMOTE'+names[i],np.mean(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1)),np.std(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1))))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">RUSLR: Average G-mean:0.870(0.011) \n",
      ">RUSLR: Average Balanced_Acc: 0.870(0.011) \n",
      ">RUSLR: Average Fbeta: 0.643(0.030)\n",
      ">RUSLR: Average Recall: 0.871(0.024)\n",
      ">RUSLR: Average Training time: 0.016(0.010)\n",
      ">RUSLR: Average accuracy_score: 0.813(0.013)\n",
      ">RUSLR: Average Score: 0.815(0.013)\n",
      ">RUSSVM: Average G-mean:0.865(0.014) \n",
      ">RUSSVM: Average Balanced_Acc: 0.866(0.013) \n",
      ">RUSSVM: Average Fbeta: 0.649(0.032)\n",
      ">RUSSVM: Average Recall: 0.845(0.047)\n",
      ">RUSSVM: Average Training time: 0.013(0.009)\n",
      ">RUSSVM: Average accuracy_score: 0.806(0.019)\n",
      ">RUSSVM: Average Score: 0.808(0.018)\n",
      ">RUSBAG: Average G-mean:0.866(0.013) \n",
      ">RUSBAG: Average Balanced_Acc: 0.866(0.013) \n",
      ">RUSBAG: Average Fbeta: 0.631(0.027)\n",
      ">RUSBAG: Average Recall: 0.874(0.015)\n",
      ">RUSBAG: Average Training time: 2.046(0.326)\n",
      ">RUSBAG: Average accuracy_score: 0.809(0.016)\n",
      ">RUSBAG: Average Score: 0.805(0.016)\n",
      ">RUSAdaboost: Average G-mean:0.839(0.012) \n",
      ">RUSAdaboost: Average Balanced_Acc: 0.839(0.012) \n",
      ">RUSAdaboost: Average Fbeta: 0.582(0.018)\n",
      ">RUSAdaboost: Average Recall: 0.840(0.029)\n",
      ">RUSAdaboost: Average Training time: 0.126(0.037)\n",
      ">RUSAdaboost: Average accuracy_score: 0.775(0.016)\n",
      ">RUSAdaboost: Average Score: 0.776(0.016)\n",
      ">RUSCART: Average G-mean:0.838(0.033) \n",
      ">RUSCART: Average Balanced_Acc: 0.840(0.031) \n",
      ">RUSCART: Average Fbeta: 0.591(0.072)\n",
      ">RUSCART: Average Recall: 0.847(0.016)\n",
      ">RUSCART: Average Training time: 0.002(0.000)\n",
      ">RUSCART: Average accuracy_score: 0.779(0.030)\n",
      ">RUSCART: Average Score: 0.781(0.030)\n",
      ">RUSRF: Average G-mean:0.861(0.011) \n",
      ">RUSRF: Average Balanced_Acc: 0.861(0.011) \n",
      ">RUSRF: Average Fbeta: 0.643(0.022)\n",
      ">RUSRF: Average Recall: 0.833(0.029)\n",
      ">RUSRF: Average Training time: 0.010(0.002)\n",
      ">RUSRF: Average accuracy_score: 0.799(0.015)\n",
      ">RUSRF: Average Score: 0.801(0.014)\n",
      ">RUSMLP: Average G-mean:0.855(0.013) \n",
      ">RUSMLP: Average Balanced_Acc: 0.856(0.012) \n",
      ">RUSMLP: Average Fbeta: 0.598(0.030)\n",
      ">RUSMLP: Average Recall: 0.889(0.016)\n",
      ">RUSMLP: Average Training time: 0.168(0.040)\n",
      ">RUSMLP: Average accuracy_score: 0.800(0.013)\n",
      ">RUSMLP: Average Score: 0.800(0.013)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import fbeta_score\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "models, names = get_models()\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# evaluate each model\n",
    "for i in range(len(models)):\n",
    "\t# evaluate the model and store results\n",
    "\tresult = fivetrials(X_sample,y_sample,models[i],sample=RandomUnderSampler(random_state=0))\n",
    "\n",
    "\tG_mean=result[0]\n",
    "\tBacc=result[1]\n",
    "# summarize performance\n",
    "\trecall=result[3]\n",
    "\tFbeta=result[2]\n",
    "\tT=result[-1]\n",
    "\tacc_s=np.mean(np.array(result)[:4,:],axis=0)\n",
    "\t# summarize and store\n",
    "\tprint('>%s: Average G-mean:%.3f(%.3f) ' % ('RUS'+names[i],np.mean(G_mean),np.std(G_mean)))\n",
    "\tprint('>%s: Average Balanced_Acc: %.3f(%.3f) ' % ('RUS'+names[i],np.mean(Bacc),np.std(Bacc)))\n",
    "\tprint('>%s: Average Fbeta: %.3f(%.3f)' % ('RUS'+names[i],np.mean(Fbeta),np.std(Fbeta)))\n",
    "\tprint('>%s: Average Recall: %.3f(%.3f)' % ('RUS'+names[i],np.mean(recall),np.std(recall)))    \n",
    "\tprint('>%s: Average Training time: %.3f(%.3f)' % ('RUS'+names[i],np.mean(T),np.std(T)))\n",
    "\tprint('>%s: Average accuracy_score: %.3f(%.3f)' % ('RUS'+names[i],np.mean(acc_s),np.std(acc_s)))\n",
    "\tprint('>%s: Average Score: %.3f(%.3f)' % ('RUS'+names[i],np.mean(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1)),np.std(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1))))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Class solution_XX() is the algorithm based on Xiaohua,Xuan[2019], the improvement is that\n",
    "we add a parameter of \"penalty\".'''\n",
    "class solution_XX:\n",
    "    \n",
    "    \n",
    "    def __init__(self,penalty = None,Lambda = 0.03,a = 0.5,epochs = 200):\n",
    "        self.W = None\n",
    "        self.penalty = penalty\n",
    "        self.Lambda = Lambda\n",
    "        self.a = a\n",
    "        self.epochs =epochs\n",
    "        self.sigmoid = lambda x:1/(1 + np.exp(-x))\n",
    "        \n",
    "#     err=(y_tr-s.sigmoid(np.dot(X_train,w)))**2\n",
    "#     err[index]=2.6*err[index]\n",
    "\n",
    "    def f_XX(self,X,Y):\n",
    "        if self.penalty=='l1':f=np.array([np.mean((self.sigmoid(np.dot(x,self.W))-y)**2 )+self.Lambda*np.sum(np.abs(self.W)) for x,y in zip(X,Y)])\n",
    "        elif self.penalty=='l2':f=np.array([np.mean((self.sigmoid(np.dot(x,self.W))-y)**2 )+self.Lambda*np.sum(self.W**2) for x,y in zip(X,Y)])#pre_Xtrain,pre_Ytrain\n",
    "        else:f=np.array([np.mean((self.sigmoid(np.dot(x,self.W))-y)**2 ) for x,y in zip(X,Y)])\n",
    "\n",
    "        return f         \n",
    "    def Gf_XX(self,X,Y):#To compute the Derivative matrix, the shape of which is N*2\n",
    "        if self.penalty=='l1':d=np.array([x.T.dot(0.02*(self.sigmoid(np.dot(x,self.W))-y)*self.sigmoid(np.dot(x,self.W))*(1-self.sigmoid(np.dot(x,self.W))))+self.Lambda*np.sign(self.W )for x,y in zip(X,Y)])\n",
    "        elif self.penalty=='l2':\n",
    "            d=np.array([x.T.dot(0.02*(self.sigmoid(np.dot(x,self.W))-y)*self.sigmoid(np.dot(x,self.W))*(1-self.sigmoid(np.dot(x,self.W))))+2*self.Lambda*self.W for x,y in zip(X,Y)])\n",
    "#     return d.reshape(20,12289)\n",
    "        else:d=np.array([x.T.dot(0.02*(self.sigmoid(np.dot(x,self.W))-y)*self.sigmoid(np.dot(x,self.W))*(1-self.sigmoid(np.dot(x,self.W)))) for x,y in zip(X,Y)])\n",
    "        return d\n",
    "    def direction_XX(self,X,Y):\n",
    "        gra=self.Gf_XX(X,Y)\n",
    "        p=matrix(gra.dot(gra.T),tc='d')\n",
    "        q=matrix(-self.f_XX(X,Y),tc='d')\n",
    "        G=matrix(np.diag(np.array([-1]*(Y.shape[0]))),tc='d')#N=20\n",
    "        h=matrix(np.array([[0]]*(Y.shape[0])),tc='d')\n",
    "        A=matrix([[1.0]]*(Y.shape[0]))\n",
    "        b=matrix([1.0])\n",
    "        solvers.options['show_progress'] = False\n",
    "        sol = solvers.qp(p,q,G,h,A,b)\n",
    "        t=np.array(sol['x'])\n",
    "        d= -(gra.T.dot(t))\n",
    "        return d.reshape((X_train.shape[-1],))\n",
    "\n",
    "    def fit(self,X,Y):\n",
    "        \n",
    "        call=[]\n",
    "        pre=[]\n",
    "        loss=[]\n",
    "        testloss=[]\n",
    "        np.random.seed(1324)\n",
    "        self.W=np.random.random((X_train.shape[-1],))*2-1\n",
    "#         self.W=w_2\n",
    "        n=y_te[y_te==1.].shape[0]\n",
    "        for k in range(200):\n",
    "    #     while np.linalg.norm(d)//10**(-8) >= 10:\n",
    "            d=self.direction_XX(X,Y)\n",
    "#             print(np.linalg.norm(d))\n",
    "            if np.linalg.norm(d)//10**(-7) < 25:\n",
    "                break\n",
    "            sigma=0.8\n",
    "            f_1=np.max(self.f_XX(X,Y))\n",
    "            w=self.W\n",
    "            self.W=d*sigma+w\n",
    "            while np.max(self.f_XX(X,Y))>np.max(f_1):\n",
    "                sigma=sigma*0.8\n",
    "                self.W=d*sigma+w\n",
    "            self.W=d*sigma+w\n",
    "\n",
    "            \n",
    "    #         output=output.reshape(4000,)\n",
    "    #         pt=max(output[output==1.].shape[0],1)\n",
    "    #         m=0\n",
    "    #         for j in range(4000):\n",
    "    #             if output[j]==test_label[j]==1:\n",
    "    #                 m+=1\n",
    "\n",
    "\n",
    "    # # b1=np.random.random((1,10))*2-1\n",
    "\n",
    "\n",
    "            \n",
    "    #         call.append(m/n)\n",
    "    #         pre.append(m/pt)\n",
    "    #         loss.append(max(f_X(w)))\n",
    "    #         testloss.append(ff_X(w))\n",
    "        return self.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fivetrials_XX(X,y,model=solution_XX()):\n",
    "\n",
    "    \n",
    "    G_m,bacc,fbeta,rec,pre,T=list(),list(),list(),list(),list(),list()\n",
    "    \n",
    "#     steps = [('p', PowerTransformer()), ('m',model)]\n",
    "    for train_index,test_index in cv.split(X, y):\n",
    "        \n",
    "        start=time.time()\n",
    "\n",
    "        pre_X,pre_Y=split(X[test_index],y[test_index])\n",
    "\n",
    "\n",
    "        w_x=model.fit(pre_X,pre_Y)\n",
    "        end=time.time()\n",
    "        prdict_y=model.sigmoid(X[train_index].dot(w_x))\n",
    "        prdict_y[prdict_y>=0.5]=1\n",
    "        prdict_y[prdict_y<0.5]=0\n",
    "\n",
    "        print(classification_report(y[train_index],prdict_y))\n",
    "        \n",
    "        rec.append(recall_score(y[train_index],prdict_y))\n",
    "        G_m.append(np.sqrt(recall_score(y[train_index],prdict_y)*recall_score(y[train_index],prdict_y,pos_label=0)))\n",
    "        pre.append(precision_score(y[train_index],prdict_y))\n",
    "    #     auc.append(roc_auc_score(y[test_index],prob_y[:,-1]))\n",
    "        bacc.append(balanced_accuracy_score(y[train_index],prdict_y))\n",
    "        fbeta.append(fbeta_score(y[train_index],prdict_y,beta=2))\n",
    "        T.append(end-start)\n",
    "#     pipeline = Pipeline(steps=steps)\n",
    "   \n",
    "#     print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_L)))\n",
    "#     print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_L)))\n",
    "#     print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_L,beta=3.66)))\n",
    "#     print('Score :{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end_d-start_d)))\n",
    "\n",
    "\n",
    "\n",
    "    return G_m,bacc,fbeta,rec,pre,T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11183, 9)"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sample_XX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.90      0.94      8738\n",
      "         1.0       0.16      0.79      0.26       208\n",
      "\n",
      "    accuracy                           0.90      8946\n",
      "   macro avg       0.58      0.84      0.60      8946\n",
      "weighted avg       0.97      0.90      0.93      8946\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.91      0.95      8738\n",
      "         1.0       0.19      0.83      0.30       208\n",
      "\n",
      "    accuracy                           0.91      8946\n",
      "   macro avg       0.59      0.87      0.63      8946\n",
      "weighted avg       0.98      0.91      0.94      8946\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.93      0.96      8738\n",
      "         1.0       0.20      0.81      0.33       208\n",
      "\n",
      "    accuracy                           0.92      8946\n",
      "   macro avg       0.60      0.87      0.64      8946\n",
      "weighted avg       0.98      0.92      0.94      8946\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.90      0.95      8739\n",
      "         1.0       0.17      0.86      0.29       208\n",
      "\n",
      "    accuracy                           0.90      8947\n",
      "   macro avg       0.58      0.88      0.62      8947\n",
      "weighted avg       0.98      0.90      0.93      8947\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.91      0.95      8739\n",
      "         1.0       0.19      0.88      0.32       208\n",
      "\n",
      "    accuracy                           0.91      8947\n",
      "   macro avg       0.60      0.90      0.64      8947\n",
      "weighted avg       0.98      0.91      0.94      8947\n",
      "\n",
      ">max_mean loss:: Average G-mean:0.871(0.018) \n",
      ">max_mean loss:: Average Balanced_Acc: 0.872(0.017) \n",
      ">max_mean loss:: Average Fbeta: 0.486(0.027)\n",
      ">max_mean loss:: Average Recall: 0.834(0.033)\n",
      ">max_mean loss:: Average Training time: 1.003(0.064)\n",
      ">max_mean loss:: Average accuracy_score: 0.766(0.022)\n",
      ">max_mean loss:: Average Score: 0.768(0.022)\n"
     ]
    }
   ],
   "source": [
    "result_XX = fivetrials_XX(X_sample_XX,y_sample)\n",
    "\n",
    "G_mean=result_XX[0]\n",
    "Bacc=result_XX[1]\n",
    "# summarize performance\n",
    "recall=result_XX[3]\n",
    "Fbeta=result_XX[2]\n",
    "T=result_XX[-1]\n",
    "acc_s=np.mean(np.array(result_XX)[:4,:],axis=0)\n",
    "\t# summarize and store\n",
    "print('>%s: Average G-mean:%.3f(%.3f) ' % ('max_mean loss:',np.mean(G_mean),np.std(G_mean)))\n",
    "print('>%s: Average Balanced_Acc: %.3f(%.3f) ' % ('max_mean loss:',np.mean(Bacc),np.std(Bacc)))\n",
    "print('>%s: Average Fbeta: %.3f(%.3f)' % ('max_mean loss:',np.mean(Fbeta),np.std(Fbeta)))\n",
    "print('>%s: Average Recall: %.3f(%.3f)' % ('max_mean loss:',np.mean(recall),np.std(recall)))    \n",
    "print('>%s: Average Training time: %.3f(%.3f)' % ('max_mean loss:',np.mean(T),np.std(T)))\n",
    "print('>%s: Average accuracy_score: %.3f(%.3f)' % ('max_mean loss:',np.mean(acc_s),np.std(acc_s)))\n",
    "print('>%s: Average Score: %.3f(%.3f)' % ('max_mean loss:',np.mean(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1)),np.std(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1))))\n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## min-max算法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanuncertainty(x,n):\n",
    "    r=[]\n",
    "    for i in range(0,len(x)+1-n,n//5):\n",
    "        r.append(np.mean(x[i:i+n]))\n",
    "    return min(r),max(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sigmoid_(x,u):\n",
    "    return 1.0/(1.0+np.exp(-x-u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select(arr,n,k):##arr为1-D\n",
    "    per=[]\n",
    "    A=abs(arr-k)\n",
    "\n",
    "    for j in np.sort(A)[:n]:\n",
    "        per.append(list(A).index(j))\n",
    "    return arr[per]\n",
    "def equa_m(x,n,r):\n",
    "    pre_in=(np.dot(x_tr,model_in.coef_.T)+model_in.intercept_).reshape(x_tr.shape[0],)\n",
    "    ini_err=y_tr-sigmoid_(pre_in,x)##ini_err is the predicted err of training set, based on LR\n",
    "\n",
    "    for k in range(ini_err.shape[0]):\n",
    "    \n",
    "        if y_tr[k]==1:\n",
    "            ini_err[k]=r*ini_err[k]\n",
    "\n",
    "    \n",
    "    return meanuncertainty(ini_err,n)[1]\n",
    "def equa_fivem(x,n,r,Mod,x_train,y_train):\n",
    "    pre_in=(np.dot(x_train,Mod.coef_.T)+Mod.intercept_).reshape(x_train.shape[0],)\n",
    "    \n",
    "    ini_err=y_train-sigmoid_(pre_in,x)##ini_err is the predicted err of training set, based on LR\n",
    "\n",
    "    for k in range(ini_err.shape[0]):\n",
    "        if y_train[k]==1:\n",
    "            ini_err[k]=r*ini_err[k]\n",
    "\n",
    "    \n",
    "    return meanuncertainty(ini_err,n)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2236,)"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_trr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.56952322])"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fsolve(lambda x:equa_m(x,100,0.5*y_sample.shape[0]/sum(y_sample)),0.5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_lr=np.array([fsolve(lambda x:equa_m(x,n,0.5*y_sample.shape[0]/sum(y_sample)),0.5 )\n",
    "                  for n in range(100,2000,10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_names=['Balanced_acc','F2_score','Recall','Precision']\n",
    "def evaluate_model_lr(X,y,m,model=LogisticRegression()):\n",
    "    \n",
    "    bacc,f2,rec,pre=list(),list(),list(),list()\n",
    "    for train_index,test_index in cv.split(X,y):\n",
    "#                 X_resampled_smote,y_resampled_smote=SMOTE().fit_resample(X[train_index], y[train_index])\n",
    "                model.fit(X[train_index],y[train_index])\n",
    "                prob_y=sigmoid_((np.dot(X[test_index],model.coef_.T)+model.intercept_),m)\n",
    "                \n",
    "                \n",
    "                prdict_y=np.round(prob_y)\n",
    "                rec.append(recall_score(y[test_index],prdict_y))\n",
    "                pre.append(precision_score(y[test_index],prdict_y))\n",
    "                \n",
    "                bacc.append(balanced_accuracy_score(y[test_index],prdict_y))\n",
    "                f2.append(fbeta_score(y[test_index],prdict_y,beta=2))\n",
    " \n",
    "    return bacc,f2,rec,pre\n",
    "    \n",
    "#                 X_resampled_smote,y_resampled_smote=SMOTE().fit_resample(X[train_index], y[train_index])\n",
    "           \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fivetrails_mean(k,X,y,model=LogisticRegression()):\n",
    "    mean=mean_lr.reshape(-1)\n",
    "    G_m,bacc,f2,rec,pre,T=list(),list(),list(),list(),list(),list()\n",
    "    for train_index,test_index in cv.split(X,y):\n",
    "                s_1=time.time()\n",
    "                model.fit(X[test_index],y[test_index])\n",
    "                t_1=time.time()\n",
    "                all_results = []\n",
    "                c=0\n",
    "                \n",
    "                \n",
    "                for i in mean:\n",
    "                    result=np.mean(evaluate_model_lr(X[test_index],y[test_index],i),axis=1)\n",
    "                    metric_res = {'window':10+5*c,'upper_mean': i}\n",
    "                    c+=1\n",
    "                    for name, value in zip(metrics_names, result):\n",
    "                #             print(name, ': ', value)\n",
    "                            metric_res[name] = value\n",
    "\n",
    "\n",
    "                    all_results.append(metric_res)\n",
    "                eva=pd.DataFrame(all_results)\n",
    "#                 eva.to_csv('{:}: datapre_diabetesCV_which_mu_to_set_inLRBIAS.csv'.format(k))\n",
    "                bias=eva[eva.iloc[:,-4]==(eva.iloc[:,-4]).max()]\n",
    "                mean=select(mean_lr.reshape(-1),10,bias['upper_mean'].mean())\n",
    "                print(\"optimal upper_mean by CV: %.3f \"%(bias['upper_mean'].mean()))\n",
    "                prob_y=sigmoid_((np.dot(X[train_index],model.coef_.T)+model.intercept_).reshape(X[train_index].shape[0],),3.6\n",
    "                             )\n",
    "                \n",
    "                \n",
    "                \n",
    "                prdict_y=np.round(prob_y)\n",
    "                print(classification_report(y[train_index],prdict_y))\n",
    "                rec.append(recall_score(y[train_index],prdict_y))\n",
    "                pre.append(precision_score(y[train_index],prdict_y))\n",
    "                G_m.append(np.sqrt(recall_score(y[train_index],prdict_y)*recall_score(y[train_index],prdict_y,pos_label=0)))\n",
    "                bacc.append(balanced_accuracy_score(y[train_index],prdict_y))\n",
    "                f2.append(fbeta_score(y[train_index],prdict_y,beta=max(2,np.log(k))))\n",
    "                T.append(t_1-s_1)\n",
    "    return G_m,bacc,f2,rec,pre,T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal upper_mean by CV: 3.864 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.90      0.94      8738\n",
      "         1.0       0.17      0.87      0.28       208\n",
      "\n",
      "    accuracy                           0.90      8946\n",
      "   macro avg       0.58      0.88      0.61      8946\n",
      "weighted avg       0.98      0.90      0.93      8946\n",
      "\n",
      "optimal upper_mean by CV: 3.824 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.87      0.93      8738\n",
      "         1.0       0.14      0.89      0.25       208\n",
      "\n",
      "    accuracy                           0.87      8946\n",
      "   macro avg       0.57      0.88      0.59      8946\n",
      "weighted avg       0.98      0.87      0.92      8946\n",
      "\n",
      "optimal upper_mean by CV: 3.786 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.91      0.95      8738\n",
      "         1.0       0.18      0.84      0.30       208\n",
      "\n",
      "    accuracy                           0.91      8946\n",
      "   macro avg       0.59      0.88      0.62      8946\n",
      "weighted avg       0.98      0.91      0.94      8946\n",
      "\n",
      "optimal upper_mean by CV: 3.803 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.89      0.94      8739\n",
      "         1.0       0.16      0.89      0.27       208\n",
      "\n",
      "    accuracy                           0.89      8947\n",
      "   macro avg       0.58      0.89      0.60      8947\n",
      "weighted avg       0.98      0.89      0.92      8947\n",
      "\n",
      "optimal upper_mean by CV: 3.777 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.89      0.94      8739\n",
      "         1.0       0.16      0.89      0.27       208\n",
      "\n",
      "    accuracy                           0.89      8947\n",
      "   macro avg       0.58      0.89      0.60      8947\n",
      "weighted avg       0.98      0.89      0.92      8947\n",
      "\n",
      ">LR_mean:: Average G-mean:0.883(0.005) \n",
      ">LR_mean:: Average Balanced_Acc: 0.883(0.005) \n",
      ">LR_mean:: Average Fbeta: 0.674(0.006)\n",
      ">LR_mean:: Average Recall: 0.876(0.020)\n",
      ">LR_mean:: Average Training time: 0.009(0.001)\n",
      ">LR_mean:: Average Score: 0.831(0.007)\n"
     ]
    }
   ],
   "source": [
    "result = Fivetrails_mean(y_sample.shape[0]/sum(y_sample)-1,X_sample,y_sample)\n",
    "# \n",
    "G_mean=result[0]\n",
    "Bacc=result[1]\n",
    "# summarize performance\n",
    "recall=result[3]\n",
    "Fbeta=result[2]\n",
    "T=result[-1]\n",
    "acc_s=np.mean(np.array(result)[:4,:],axis=0)\n",
    "\n",
    "print('>%s: Average G-mean:%.3f(%.3f) ' % ('LR_mean:',np.mean(G_mean),np.std(G_mean)))\n",
    "print('>%s: Average Balanced_Acc: %.3f(%.3f) ' % ('LR_mean:',np.mean(Bacc),np.std(Bacc)))\n",
    "print('>%s: Average Fbeta: %.3f(%.3f)' % ('LR_mean:',np.mean(Fbeta),np.std(Fbeta)))\n",
    "print('>%s: Average Recall: %.3f(%.3f)' % ('LR_mean:',np.mean(recall),np.std(recall)))\n",
    "print('>%s: Average Training time: %.3f(%.3f)' % ('LR_mean:',np.mean(T),np.std(T)))\n",
    "print('>%s: Average Score: %.3f(%.3f)' % ('LR_mean:',np.mean(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1)),np.std(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equa_xh(x,n):\n",
    "    pre_in=np.dot(X_train,W_X_2).reshape(x_tr.shape[0],)\n",
    "    ini_err=y_tr-sigmoid_(pre_in,x)\n",
    "    for k in range(x_tr.shape[0]):\n",
    "    \n",
    "        if y_tr[k]==1:\n",
    "            ini_err[k]=20*ini_err[k]\n",
    "\n",
    "    \n",
    "    return meanuncertainty(ini_err,n)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96681692])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fsolve(lambda x:equa_xh(x,80),0.5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_xh=np.array([fsolve(lambda x:equa_xh(x,n),0.5 ) for n in range(10,2000,10)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 波动率不确定性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_u(x,arr):\n",
    "    p=[]\n",
    "    for c in arr:\n",
    "        \n",
    "        if c <0:\n",
    "            p.append(2*x[1]*st.norm.cdf(c/x[1])/(x[0]+x[1]))\n",
    "        else:\n",
    "            p.append(1-2*x[0]*st.norm.cdf(-c/x[0])/(x[0]+x[1]))\n",
    "    return np.array(p)\n",
    "def F_L(x,arr):\n",
    "    p=[]\n",
    "    for c in arr:\n",
    "        \n",
    "        if c <0:\n",
    "            p.append(2*x[0]*st.norm.cdf(c/x[0])/(x[0]+x[1]))\n",
    "        else:\n",
    "            p.append(1-2*x[1]*st.norm.cdf(-c/x[1])/(x[0]+x[1]))\n",
    "    return np.array(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def equa_v(x,n,r):\n",
    "    pre_in=(np.dot(x_tr,model_in.coef_.T)+model_in.intercept_).reshape(x_tr.shape[0],) \n",
    "    \n",
    "    err_u=y_tr-F_u(x,pre_in)\n",
    "    err_L=y_tr-F_L(x,pre_in)\n",
    "    for k in range(x_tr.shape[0]):\n",
    "    \n",
    "        if y_tr[k]==1:\n",
    "            err_u[k]=r*err_u[k]\n",
    "            err_L[k]=r*err_L[k]\n",
    "    \n",
    "    return np.array([meanuncertainty(err_u,n)[1],meanuncertainty(err_L,n)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.36816582e-03,  6.12875882e+00])"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fsolve(lambda x:equa_v(x,50,10),[0.5,1.5] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_lr=np.array([fsolve(lambda x:equa_v(x,n,10),[0.5,1.5] ) for n in range(50,500,10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(var_lr).to_csv('/Users/lvjingzhe/Desktop/璇/modified_althogram/code/0513_lrvar_mean_cancer.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_evaluate_model(X,y,m,model=LogisticRegression()):\n",
    "    \n",
    "    G_m,bacc,f2,rec,pre=list(),list(),list(),list(),list()\n",
    "   \n",
    "#   \n",
    "    for train_index,test_index in cv.split(X,y):\n",
    "#                 X_resampled_smote,y_resampled_smote=SMOTE().fit_resample(X[train_index], y[train_index])\n",
    "                model.fit(X[train_index],y[train_index])\n",
    "                prob_y=F_u(m,(np.dot(X[test_index],model.coef_.T)+model.intercept_).reshape(X[test_index].shape[0],))\n",
    "        \n",
    "                \n",
    "                prdict_y=np.round(prob_y)\n",
    "                rec.append(recall_score(y[test_index],prdict_y))\n",
    "                pre.append(precision_score(y[test_index],prdict_y))\n",
    "                G_m.append(np.sqrt(recall_score(y[test_index],prdict_y)*recall_score(y[test_index],prdict_y,pos_label=0)))\n",
    "\n",
    "                bacc.append(balanced_accuracy_score(y[test_index],prdict_y))\n",
    "                f2.append(fbeta_score(y[test_index],prdict_y,beta=2))\n",
    " \n",
    "    return G_m,bacc,f2,rec,pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fivetrails_var(X,y,model=LogisticRegression()):\n",
    "    \n",
    "    G_m,bacc,f2,rec,pre,T=list(),list(),list(),list(),list(),list()\n",
    "    for train_index,test_index in cv.split(X,y):\n",
    "                s_1=time.time()\n",
    "#                 X_resampled_smote,y_resampled_smote=SMOTE().fit_resample(X[train_index], y[train_index])\n",
    "                model.fit(X[test_index],y[test_index])\n",
    "                t_1=time.time()\n",
    "#                 mean_u=np.array([fsolve(lambda x:equa_fivem(x,n,0.5*(k+1),model,X[train_index],y[train_index]),0.5) for n in range(int(y[train_index].shape[0]*0.2),int(y[train_index].shape[0]*0.8),5)])\n",
    "                all_results = []\n",
    "                c=0\n",
    "                # for k in mean_lr:\n",
    "            #     strat=time.time()\n",
    "                for i in var_lr:\n",
    "                    result=np.mean(var_evaluate_model(X[train_index],y[train_index],i),axis=1)\n",
    "                    metric_res = {'window':10+5*c,'upper_mean': i}\n",
    "                    c+=1\n",
    "                    for name, value in zip(metrics_names, result):\n",
    "                #             print(name, ': ', value)\n",
    "                            metric_res[name] = value\n",
    "\n",
    "\n",
    "                    all_results.append(metric_res)\n",
    "                eva=pd.DataFrame(all_results)\n",
    "#                 eva.to_csv('{:}: datapre_diabetesCV_which_mu_to_set_inLRBIAS.csv'.format(k))\n",
    "                bias=eva[eva.iloc[:,-4]==(eva.iloc[:,-4]).max()]\n",
    "                print(\"optimal volatility by CV:\",np.array(bias['upper_mean']))\n",
    "#                 end=time.time()\n",
    "                \n",
    "                prob_y=F_u(np.array(bias['upper_mean'])[0],(np.dot(X[train_index],model.coef_.T)+model.intercept_).reshape(X[train_index].shape[0],))\n",
    "                \n",
    "                \n",
    "                prdict_y=np.round(prob_y)\n",
    "                print(classification_report(y[train_index],prdict_y))\n",
    "                rec.append(recall_score(y[train_index],prdict_y))\n",
    "                pre.append(precision_score(y[train_index],prdict_y))\n",
    "                G_m.append(np.sqrt(recall_score(y[train_index],prdict_y)*recall_score(y[train_index],prdict_y,pos_label=0)))\n",
    "                bacc.append(balanced_accuracy_score(y[train_index],prdict_y))\n",
    "                f2.append(fbeta_score(y[train_index],prdict_y,beta=max(2,np.log(y_sample.shape[0]/sum(y_sample)-1))))\n",
    "                T.append(t_1-s_1)\n",
    "    return G_m,bacc,f2,rec,pre,T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal volatility by CV: [array([0.13648798, 5.00105588])]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.93      0.96      8738\n",
      "         1.0       0.21      0.82      0.34       208\n",
      "\n",
      "    accuracy                           0.93      8946\n",
      "   macro avg       0.60      0.87      0.65      8946\n",
      "weighted avg       0.98      0.93      0.95      8946\n",
      "\n",
      "optimal volatility by CV: [array([0.13648798, 5.00105588])]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.91      0.95      8738\n",
      "         1.0       0.19      0.88      0.32       208\n",
      "\n",
      "    accuracy                           0.91      8946\n",
      "   macro avg       0.60      0.90      0.64      8946\n",
      "weighted avg       0.98      0.91      0.94      8946\n",
      "\n",
      "optimal volatility by CV: [array([0.13648798, 5.00105588])]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.94      0.96      8738\n",
      "         1.0       0.23      0.79      0.35       208\n",
      "\n",
      "    accuracy                           0.93      8946\n",
      "   macro avg       0.61      0.86      0.66      8946\n",
      "weighted avg       0.98      0.93      0.95      8946\n",
      "\n",
      "optimal volatility by CV: [array([0.19034103, 5.3814667 ])]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.90      0.94      8739\n",
      "         1.0       0.17      0.88      0.28       208\n",
      "\n",
      "    accuracy                           0.90      8947\n",
      "   macro avg       0.58      0.89      0.61      8947\n",
      "weighted avg       0.98      0.90      0.93      8947\n",
      "\n",
      "optimal volatility by CV: [array([0.19034103, 5.3814667 ])]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.90      0.95      8739\n",
      "         1.0       0.17      0.88      0.29       208\n",
      "\n",
      "    accuracy                           0.90      8947\n",
      "   macro avg       0.59      0.89      0.62      8947\n",
      "weighted avg       0.98      0.90      0.93      8947\n",
      "\n",
      ">LR_volatility:: Average G-mean:0.882(0.013) \n",
      ">LR_volatility:: Average Balanced_Acc: 0.883(0.012) \n",
      ">LR_volatility:: Average Fbeta: 0.692(0.011)\n",
      ">LR_volatility:: Average Recall: 0.851(0.037)\n",
      ">LR_volatility:: Average Training time: 0.018(0.017)\n",
      ">LR_volatility:: Average Score: 0.829(0.017)\n"
     ]
    }
   ],
   "source": [
    "result_v = Fivetrails_var(X_sample,y_sample)\n",
    "# \n",
    "G_mean=result_v[0]\n",
    "Bacc=result_v[1]\n",
    "# summarize performance\n",
    "recall=result_v[3]\n",
    "Fbeta=result_v[2]\n",
    "T=result_v[-1]\n",
    "acc_s=np.mean(np.array(result_v)[:4,:],axis=0)\n",
    "\n",
    "print('>%s: Average G-mean:%.3f(%.3f) ' % ('LR_volatility:',np.mean(G_mean),np.std(G_mean)))\n",
    "print('>%s: Average Balanced_Acc: %.3f(%.3f) ' % ('LR_volatility:',np.mean(Bacc),np.std(Bacc)))\n",
    "print('>%s: Average Fbeta: %.3f(%.3f)' % ('LR_volatility:',np.mean(Fbeta),np.std(Fbeta)))\n",
    "print('>%s: Average Recall: %.3f(%.3f)' % ('LR_volatility:',np.mean(recall),np.std(recall)))    \n",
    "print('>%s: Average Training time: %.3f(%.3f)' % ('LR_volatility:',np.mean(T),np.std(T)))\n",
    "print('>%s: Average Score: %.3f(%.3f)' % ('LR_volatility:',np.mean(0.99*acc_s+0.01/(np.array(result_v)[-1,:]/10*9+1)),np.std(0.99*acc_s+0.01/(np.array(result_v)[-1,:]/10*9+1))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_xh=np.dot(X_train,W_X).reshape(x_tr.shape[0],)\n",
    "def equa_vxh(x,n):\n",
    "    \n",
    "    err_u=y_tr-F_u(x,pre_xh)\n",
    "    err_L=y_tr-F_L(x,pre_xh)\n",
    "#     for k in range(y_tr.shape[0]):\n",
    "    \n",
    "#         if y_tr[k]==1:\n",
    "#             err_u[k]=20*err_u[k]\n",
    "#             err_L[k]=20*err_L[k]\n",
    "    \n",
    "    return np.array([meanuncertainty(err_u,n)[1],meanuncertainty(err_L,n)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_varxh(X,y,m,model=solution_XX()):\n",
    "    \n",
    "    G_m,bacc,f2,rec,pre=list(),list(),list(),list(),list()\n",
    "   \n",
    "\n",
    "    for train_index,test_index in cv.split(X,y):\n",
    "        \n",
    "        N=X[train_index].shape[0]\n",
    "        n=y[train_index][y[train_index]==1].shape[0]\n",
    "        c_1=np.concatenate((X[train_index][y[train_index]==0],X[train_index][y[train_index]==1]))\n",
    "        l_1=np.concatenate((np.array([0]*(N-n)),np.array([1]*n)))\n",
    "        M=int((N-n)/41)\n",
    "        indice=[k*M for k in range(1,41)]\n",
    "        indice.append(N-n)\n",
    "        pre_cv=np.array(np.split(c_1,indice))\n",
    "        pre_y=np.array(np.split(l_1,indice))\n",
    "        w= model.fit(pre_cv,pre_y)\n",
    "#         w_x.append(w)\n",
    "        prob_y=F_u(m,X_train[test_index].dot(w))\n",
    "        \n",
    "        prdict_y=np.round(prob_y)\n",
    "        rec.append(recall_score(y[test_index],prdict_y))\n",
    "        pre.append(precision_score(y[test_index],prdict_y))\n",
    "        G_m.append(np.sqrt(recall_score(y[test_index],prdict_y)*recall_score(y[test_index],prdict_y,pos_label=0)))\n",
    "\n",
    "        bacc.append(balanced_accuracy_score(y[test_index],prdict_y))\n",
    "        f2.append(fbeta_score(y[test_index],prdict_y,beta=3.66))\n",
    " \n",
    "    return G_m,bacc,f2,rec,pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
