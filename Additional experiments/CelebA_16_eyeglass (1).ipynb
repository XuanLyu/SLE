{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lvjingzhe/anaconda3/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/Users/lvjingzhe/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n",
      "/Users/lvjingzhe/anaconda3/lib/python3.7/site-packages/dask/dataframe/utils.py:13: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n",
      "/Users/lvjingzhe/anaconda3/lib/python3.7/site-packages/distributed/config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  defaults = yaml.load(f)\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as st\n",
    "from sympy.solvers import solve\n",
    "from sympy import Symbol\n",
    "from scipy.optimize import fsolve\n",
    "import testjx\n",
    "import numpy as np\n",
    "import cvxopt\n",
    "import cv2\n",
    "import os \n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "from cvxopt import matrix\n",
    "from cvxopt import solvers\n",
    "import face_recognition\n",
    "import sklearn.metrics as sm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# from scikitplot import plotters as skplt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from statsmodels.discrete.discrete_model import Logit, Probit, MNLogit\n",
    "from pylab import mpl\n",
    "\n",
    "# import warning\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib as mpl\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import skimage\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import preprocessing\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from os import listdir\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "from collections import Counter\n",
    "import imblearn\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "sn.set()\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC # SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "from xgboost import XGBClassifier # XGBClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score,f1_score,recall_score,cohen_kappa_score,precision_score\n",
    "from sklearn.utils import compute_class_weight\n",
    "from sklearn.preprocessing import MinMaxScaler,LabelBinarizer\n",
    "from sklearn.ensemble import AdaBoostClassifier # AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier # KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier # RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.compat.v1 as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.applications.vgg16 import VGG16 # VGG16\n",
    "from tensorflow.keras.applications.vgg19 import VGG19 # VGG19\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50 # ResNet50\n",
    "from tensorflow.keras.applications.xception import Xception # Xception\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet # MobileNet\n",
    "from tensorflow.keras.applications.nasnet import NASNetMobile # NASNetMobile\n",
    "from tensorflow.keras.applications.densenet import DenseNet169 # DenseNet169\n",
    "from tensorflow.keras.applications.densenet import DenseNet121 # DenseNet121\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2 # MobileNetV2\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3 # InceptionV3\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Flatten, Activation, GlobalAveragePooling2D,Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction for image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16207"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "13766+2441"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0516 23:37:34.885970 4576364032 deprecation.py:506] From /Users/lvjingzhe/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time:300.895672082901\n"
     ]
    }
   ],
   "source": [
    "''' Feature extraction---VGG19 network'''\n",
    "train=[]\n",
    "# for i in range(16207):\n",
    "for i in range(20000):\n",
    "    img_ = cv2.imread('/Users/lvjingzhe/Downloads/celebA/train/%d.jpg' %(i))\n",
    "\n",
    "    train.append(img_)\n",
    "train_image=np.array(train)\n",
    "import time\n",
    "start=time.time()\n",
    "base_model= VGG19( weights='imagenet', include_top=False,input_shape=(64,64,3))\n",
    "x = base_model.output\n",
    "# x = Dropout(0.2)(x)\n",
    "predictions = Flatten()(x)\n",
    "\n",
    "model_feat = Model(inputs=base_model.input,outputs=predictions)\n",
    "\n",
    "train_features = model_feat.predict(train_image/255)\n",
    "# test_features=model_feat.predict(test_image/255)\n",
    "end=time.time()\n",
    "print('running time:{}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 64, 64, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 2048)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Shape of input length and feature dimension after VGG19'''\n",
    "train_features.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2441.0"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Divide into two sets: training set(x_tr,y_tr), and testing set (x_te,y_te)'''\n",
    "\n",
    "train_label=np.concatenate((np.array([0]*19000),np.array([1]*1000)))\n",
    "                                  \n",
    "    \n",
    "x_tr, x_te, y_tr, y_te = train_test_split(train_features,train_label,test_size = 0.9,\n",
    "                                                  shuffle = True,\n",
    "                                                  random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 2048)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E=np.hstack((train_features,train_label.reshape(20000,1)))\n",
    "np.random.seed(4123)\n",
    "np.random.shuffle(E)\n",
    "X_sample=E[:,:-1]\n",
    "y_sample=E[:,-1]\n",
    "X_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample_XX=np.c_[X_sample,np.ones(X_sample.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.c_[x_tr,np.ones(x_tr.shape[0])]\n",
    "X_test=np.c_[x_te,np.ones(x_te.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label.shape[0]/sum(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Split the training set into 20 groups, one of which contains the toral glassed ones.\n",
    "This step is useful in max-mean loss method\n",
    " (x_tr,y_tr) becomes (pre_XX,pre_YY) after \"Split\", which is the input of max-mean loss method.'''\n",
    "def split(x,y):\n",
    "    k=int(y.shape[0]/sum(y))-1\n",
    "    n=y[y==1].shape[0]\n",
    "    m=int((x.shape[0]-n)/k)\n",
    "    indice=[i*m for i in range(1,k)]\n",
    "    indice.append(x.shape[0]-n)\n",
    "    x_c=np.concatenate((x[y==0],x[y==1]))\n",
    "    y_c=np.concatenate((np.array([0]*(x.shape[0]-n)),np.array([1]*n)))\n",
    "    return np.array(np.split(x_c,indice)),np.array(np.split(y_c,indice))\n",
    "\n",
    "\n",
    "n=y_tr[y_tr==1].shape[0]\n",
    "XX=np.concatenate((X_train[y_tr==0],X_train[y_tr==1]))\n",
    "YY=np.concatenate((np.array([0]*(X_train.shape[0]-n)),np.array([1]*n)))\n",
    "m=int((X_train.shape[0]-n)/19)\n",
    "indice=[k*m for k in range(1,19)]\n",
    "indice.append(X_train.shape[0]-n)\n",
    "\n",
    "pre_XX=np.array(np.split(XX,indice))\n",
    "pre_YY=np.array(np.split(YY,indice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Split training set into 5 cross-validation sets'''\n",
    "cv= RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOTA methods：LR、SVM、AUSTBOOST、BAGGING...+Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99     17121\n",
      "           1       0.79      0.61      0.69       879\n",
      "\n",
      "    accuracy                           0.97     18000\n",
      "   macro avg       0.88      0.80      0.84     18000\n",
      "weighted avg       0.97      0.97      0.97     18000\n",
      "\n",
      "Running time:0.91 s\n",
      "Balanced-Accuracy on testing set：79.85%\n",
      "Recall on testing set：60.52%\n",
      "F-measure on testing set：62.02%\n"
     ]
    }
   ],
   "source": [
    "'''model_in symbols the result of traditional logistic regression'''\n",
    "\n",
    "import time\n",
    "start_i=time.time()\n",
    "model_in=LogisticRegression()\n",
    "\n",
    "model_in.fit(x_tr,y_tr)\n",
    "end_i=time.time()\n",
    "y_train_proba=model_in.predict_proba(x_tr)\n",
    "y_train_label=model_in.predict(x_tr)\n",
    "y_test_proba=model_in.predict_proba(x_te)\n",
    "\n",
    "y_test_label=model_in.predict(x_te)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_te,y_test_label))\n",
    "print(\"Running time:%.2f s\"%(end_i-start_i))\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_test_label)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_test_label)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_test_label,beta=2.94)))\n",
    "# print(classification_report(y_tr,y_train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "def get_models():\n",
    "\tmodels, names = list(), list()\n",
    "\t# LR\n",
    "\tmodels.append(LogisticRegression())\n",
    "\tnames.append('LR')\n",
    "\t# SVM\n",
    "\tmodels.append(SVC(probability=True))\n",
    "\tnames.append('SVM')\n",
    "\t# Bagging\n",
    "\tmodels.append(BaggingClassifier())\n",
    "\tnames.append('BAG')\n",
    "\tmodels.append(AdaBoostClassifier(random_state=0))\n",
    "\tnames.append('Adaboost')\n",
    "\t# RF\n",
    "\tmodels.append(\n",
    "\ttree.DecisionTreeClassifier(max_leaf_nodes=6,\n",
    "                               random_state=0))\n",
    "\tnames.append('CART')\n",
    "\tmodels.append(RandomForestClassifier(criterion='entropy',n_estimators=6))\n",
    "\tnames.append('RF')\n",
    "\t# GBM\n",
    "\tmodels.append(MLPClassifier(random_state=0, max_iter=200,hidden_layer_sizes=(50,)))\n",
    "    \n",
    "\tnames.append('MLP')\n",
    "# \tmodels.append(solution_XX())\n",
    "# \tnames.append('max-mean loss')\n",
    "\treturn models, names\n",
    "def evaluate_model(X,y,model):\n",
    "#     steps = [('p', PowerTransformer()), ('m',model)]\n",
    "    \n",
    "#     pipeline = Pipeline(steps=steps)\n",
    "    pipeline = model\n",
    "    import time\n",
    "    start=time.time()\n",
    "    pipeline.fit(X,y)\n",
    "    end=time.time()\n",
    "    prdict_y=pipeline.predict(x_te)\n",
    "    prob_y=pipeline.predict_proba(x_te)\n",
    "    rec=recall_score(y_te,prdict_y)\n",
    "    G_m=np.sqrt(recall_score(y_te,prdict_y)*recall_score(y_te,prdict_y,pos_label=0))\n",
    "    pre=precision_score(y_te,prdict_y)\n",
    "#     auc.append(roc_auc_score(y[test_index],prob_y[:,-1]))\n",
    "    bacc=balanced_accuracy_score(y_te,prdict_y)\n",
    "    fbeta=fbeta_score(y_te,prdict_y,beta=2)\n",
    "#     print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_L)))\n",
    "#     print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_L)))\n",
    "#     print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_L,beta=3.66)))\n",
    "#     print('Score :{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end_d-start_d)))\n",
    "\n",
    "\n",
    "\n",
    "    return G_m,bacc,fbeta,rec,pre,end-start\n",
    "import time\n",
    "def fivetrials(X,y,model,sample='none'):\n",
    "    pipeline = model\n",
    "    \n",
    "    G_m,bacc,fbeta,rec,pre,T=list(),list(),list(),list(),list(),list()\n",
    "    if sample!='none':\n",
    "#     steps = [('p', PowerTransformer()), ('m',model)]\n",
    "        for train_index,test_index in cv.split(X, y):\n",
    "        \n",
    "            start=time.time()\n",
    "            resample_x,resample_y=sample.fit_resample(X[test_index],y[test_index])\n",
    "            \n",
    "            pipeline.fit(resample_x,resample_y)\n",
    "            end=time.time()\n",
    "            prdict_y=pipeline.predict(X[train_index])\n",
    "            prob_y=pipeline.predict_proba(X[train_index])\n",
    "            rec.append(recall_score(y[train_index],prdict_y))\n",
    "            G_m.append(np.sqrt(recall_score(y[train_index],prdict_y)*recall_score(y[train_index],prdict_y,pos_label=0)))\n",
    "            pre.append(precision_score(y[train_index],prdict_y))\n",
    "        #     auc.append(roc_auc_score(y[test_index],prob_y[:,-1]))\n",
    "            bacc.append(balanced_accuracy_score(y[train_index],prdict_y))\n",
    "            fbeta.append(fbeta_score(y[train_index],prdict_y,beta=max(2,np.log(y_sample.shape[0]/sum(y_sample)-1))))\n",
    "            T.append(end-start)\n",
    "#     pipeline = Pipeline(steps=steps)\n",
    "    else:\n",
    "        for train_index,test_index in cv.split(X, y):\n",
    "    \n",
    "            start=time.time()\n",
    "            pipeline.fit(X[test_index],y[test_index])\n",
    "            end=time.time()\n",
    "            prdict_y=pipeline.predict(X[train_index])\n",
    "            prob_y=pipeline.predict_proba(X[train_index])\n",
    "            rec.append(recall_score(y[train_index],prdict_y))\n",
    "            G_m.append(np.sqrt(recall_score(y[train_index],prdict_y)*recall_score(y[train_index],prdict_y,pos_label=0)))\n",
    "            pre.append(precision_score(y[train_index],prdict_y))\n",
    "        #     auc.append(roc_auc_score(y[test_index],prob_y[:,-1]))\n",
    "            bacc.append(balanced_accuracy_score(y[train_index],prdict_y))\n",
    "            fbeta.append(fbeta_score(y[train_index],prdict_y,beta=max(2,np.log(y_sample.shape[0]/sum(y_sample)-1))))\n",
    "            T.append(end-start)\n",
    "#     print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_L)))\n",
    "#     print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_L)))\n",
    "#     print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_L,beta=3.66)))\n",
    "#     print('Score :{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end_d-start_d)))\n",
    "\n",
    "\n",
    "\n",
    "    return G_m,bacc,fbeta,rec,pre,T\n",
    "# print('>%s: Mean Balanced_Acc: %.3f (%.3f)' % (names[-1],mean(Bacc),std(Bacc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">BAG: Average G-mean:0.814(0.011) \n",
      ">BAG: Average Balanced_Acc: 0.815(0.011) \n",
      ">BAG: Average Fbeta: 0.612(0.018)\n",
      ">BAG: Average Recall: 0.785(0.018)\n",
      ">BAG: Average Training time: 0.950(0.054)\n",
      ">BAG: Average accuracy_score: 0.756(0.014)\n",
      ">BAG: Average Score: 0.754(0.014)\n"
     ]
    }
   ],
   "source": [
    "models, names = get_models()\n",
    "i=2\n",
    "result = fivetrials(X_sample,y_sample,models[2],sample=RandomUnderSampler(random_state=0))\n",
    "\n",
    "G_mean=result[0]\n",
    "Bacc=result[1]\n",
    "# summarize performance\n",
    "recall=result[3]\n",
    "Fbeta=result[2]\n",
    "T=result[-1]\n",
    "acc_s=np.mean(np.array(result)[:4,:],axis=0)\n",
    "# summarize and store\n",
    "print('>%s: Average G-mean:%.3f(%.3f) ' % (names[2],np.mean(G_mean),np.std(G_mean)))\n",
    "print('>%s: Average Balanced_Acc: %.3f(%.3f) ' % (names[2],np.mean(Bacc),np.std(Bacc)))\n",
    "print('>%s: Average Fbeta: %.3f(%.3f)' % (names[2],np.mean(Fbeta),np.std(Fbeta)))\n",
    "print('>%s: Average Recall: %.3f(%.3f)' % (names[i],np.mean(recall),np.std(recall)))    \n",
    "print('>%s: Average Training time: %.3f(%.3f)' % (names[i],np.mean(T),np.std(T)))\n",
    "print('>%s: Average accuracy_score: %.3f(%.3f)' % (names[i],np.mean(acc_s),np.std(acc_s)))\n",
    "print('>%s: Average Score: %.3f(%.3f)' % (names[i],np.mean(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1)),np.std(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1))))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">LR: Average G-mean:0.772(0.016) \n",
      ">LR: Average Balanced_Acc: 0.796(0.012) \n",
      ">LR: Average Fbeta: 0.616(0.023)\n",
      ">LR: Average Recall: 0.600(0.025)\n",
      ">LR: Average Training time: 0.640(0.351)\n",
      ">LR: Average accuracy_score: 0.696(0.019)\n",
      ">LR: Average Score: 0.696(0.019)\n",
      ">SVM: Average G-mean:0.508(0.017) \n",
      ">SVM: Average Balanced_Acc: 0.629(0.009) \n",
      ">SVM: Average Fbeta: 0.280(0.018)\n",
      ">SVM: Average Recall: 0.258(0.017)\n",
      ">SVM: Average Training time: 10.907(0.416)\n",
      ">SVM: Average accuracy_score: 0.419(0.015)\n",
      ">SVM: Average Score: 0.415(0.015)\n",
      ">BAG: Average G-mean:0.560(0.008) \n",
      ">BAG: Average Balanced_Acc: 0.655(0.004) \n",
      ">BAG: Average Fbeta: 0.336(0.009)\n",
      ">BAG: Average Recall: 0.316(0.010)\n",
      ">BAG: Average Training time: 5076.050(529.705)\n",
      ">BAG: Average accuracy_score: 0.467(0.008)\n",
      ">BAG: Average Score: 0.462(0.008)\n",
      ">Adaboost: Average G-mean:0.686(0.012) \n",
      ">Adaboost: Average Balanced_Acc: 0.733(0.008) \n",
      ">Adaboost: Average Fbeta: 0.492(0.016)\n",
      ">Adaboost: Average Recall: 0.476(0.017)\n",
      ">Adaboost: Average Training time: 19.007(0.629)\n",
      ">Adaboost: Average accuracy_score: 0.597(0.013)\n",
      ">Adaboost: Average Score: 0.591(0.013)\n",
      ">CART: Average G-mean:0.552(0.036) \n",
      ">CART: Average Balanced_Acc: 0.650(0.019) \n",
      ">CART: Average Fbeta: 0.326(0.040)\n",
      ">CART: Average Recall: 0.309(0.041)\n",
      ">CART: Average Training time: 1.165(0.166)\n",
      ">CART: Average accuracy_score: 0.459(0.034)\n",
      ">CART: Average Score: 0.460(0.034)\n",
      ">RF: Average G-mean:0.402(0.036) \n",
      ">RF: Average Balanced_Acc: 0.581(0.014) \n",
      ">RF: Average Fbeta: 0.178(0.031)\n",
      ">RF: Average Recall: 0.163(0.028)\n",
      ">RF: Average Training time: 0.306(0.039)\n",
      ">RF: Average accuracy_score: 0.331(0.027)\n",
      ">RF: Average Score: 0.335(0.027)\n",
      ">MLP: Average G-mean:0.769(0.015) \n",
      ">MLP: Average Balanced_Acc: 0.794(0.011) \n",
      ">MLP: Average Fbeta: 0.612(0.021)\n",
      ">MLP: Average Recall: 0.598(0.024)\n",
      ">MLP: Average Training time: 8.314(1.214)\n",
      ">MLP: Average accuracy_score: 0.693(0.018)\n",
      ">MLP: Average Score: 0.688(0.017)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import fbeta_score\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "models, names = get_models()\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# evaluate each model\n",
    "for i in range(len(models)):\n",
    "\t# evaluate the model and store results\n",
    "\tresult = fivetrials(X_sample,y_sample,models[i])\n",
    "\n",
    "\tG_mean=result[0]\n",
    "\tBacc=result[1]\n",
    "# summarize performance\n",
    "\trecall=result[3]\n",
    "\tFbeta=result[2]\n",
    "\tT=result[-1]\n",
    "\tacc_s=np.mean(np.array(result)[:4,:],axis=0)\n",
    "\t# summarize and store\n",
    "\tprint('>%s: Average G-mean:%.3f(%.3f) ' % (names[i],np.mean(G_mean),np.std(G_mean)))\n",
    "\tprint('>%s: Average Balanced_Acc: %.3f(%.3f) ' % (names[i],np.mean(Bacc),np.std(Bacc)))\n",
    "\tprint('>%s: Average Fbeta: %.3f(%.3f)' % (names[i],np.mean(Fbeta),np.std(Fbeta)))\n",
    "\tprint('>%s: Average Recall: %.3f(%.3f)' % (names[i],np.mean(recall),np.std(recall)))    \n",
    "\tprint('>%s: Average Training time: %.3f(%.3f)' % (names[i],np.mean(T),np.std(T)))\n",
    "\tprint('>%s: Average accuracy_score: %.3f(%.3f)' % (names[i],np.mean(acc_s),np.std(acc_s)))\n",
    "\tprint('>%s: Average Score: %.3f(%.3f)' % (names[i],np.mean(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1)),np.std(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1))))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">SMOTELR: Average G-mean:0.823(0.010) \n",
      ">SMOTELR: Average Balanced_Acc: 0.836(0.008) \n",
      ">SMOTELR: Average Fbeta: 0.686(0.014)\n",
      ">SMOTELR: Average Recall: 0.693(0.019)\n",
      ">SMOTELR: Average Training time: 0.902(0.057)\n",
      ">SMOTELR: Average accuracy_score: 0.760(0.013)\n",
      ">SMOTELR: Average Score: 0.758(0.013)\n",
      ">SMOTESVM: Average G-mean:0.844(0.010) \n",
      ">SMOTESVM: Average Balanced_Acc: 0.852(0.008) \n",
      ">SMOTESVM: Average Fbeta: 0.709(0.015)\n",
      ">SMOTESVM: Average Recall: 0.739(0.019)\n",
      ">SMOTESVM: Average Training time: 84.566(3.324)\n",
      ">SMOTESVM: Average accuracy_score: 0.786(0.013)\n",
      ">SMOTESVM: Average Score: 0.779(0.013)\n",
      ">SMOTEBAG: Average G-mean:0.649(0.018) \n",
      ">SMOTEBAG: Average Balanced_Acc: 0.708(0.012) \n",
      ">SMOTEBAG: Average Fbeta: 0.443(0.023)\n",
      ">SMOTEBAG: Average Recall: 0.427(0.024)\n",
      ">SMOTEBAG: Average Training time: 6385.379(302.973)\n",
      ">SMOTEBAG: Average accuracy_score: 0.557(0.019)\n",
      ">SMOTEBAG: Average Score: 0.551(0.019)\n",
      ">SMOTEAdaboost: Average G-mean:0.725(0.010) \n",
      ">SMOTEAdaboost: Average Balanced_Acc: 0.756(0.007) \n",
      ">SMOTEAdaboost: Average Fbeta: 0.536(0.014)\n",
      ">SMOTEAdaboost: Average Recall: 0.543(0.014)\n",
      ">SMOTEAdaboost: Average Training time: 36.083(0.152)\n",
      ">SMOTEAdaboost: Average accuracy_score: 0.640(0.011)\n",
      ">SMOTEAdaboost: Average Score: 0.634(0.011)\n",
      ">SMOTECART: Average G-mean:0.730(0.037) \n",
      ">SMOTECART: Average Balanced_Acc: 0.742(0.032) \n",
      ">SMOTECART: Average Fbeta: 0.503(0.045)\n",
      ">SMOTECART: Average Recall: 0.629(0.093)\n",
      ">SMOTECART: Average Training time: 2.599(0.117)\n",
      ">SMOTECART: Average accuracy_score: 0.651(0.050)\n",
      ">SMOTECART: Average Score: 0.648(0.049)\n",
      ">SMOTERF: Average G-mean:0.555(0.018) \n",
      ">SMOTERF: Average Balanced_Acc: 0.651(0.010) \n",
      ">SMOTERF: Average Fbeta: 0.328(0.020)\n",
      ">SMOTERF: Average Recall: 0.311(0.020)\n",
      ">SMOTERF: Average Training time: 0.830(0.032)\n",
      ">SMOTERF: Average accuracy_score: 0.461(0.017)\n",
      ">SMOTERF: Average Score: 0.462(0.017)\n",
      ">SMOTEMLP: Average G-mean:0.800(0.014) \n",
      ">SMOTEMLP: Average Balanced_Acc: 0.817(0.011) \n",
      ">SMOTEMLP: Average Fbeta: 0.654(0.020)\n",
      ">SMOTEMLP: Average Recall: 0.650(0.025)\n",
      ">SMOTEMLP: Average Training time: 5.392(0.123)\n",
      ">SMOTEMLP: Average accuracy_score: 0.730(0.018)\n",
      ">SMOTEMLP: Average Score: 0.725(0.018)\n"
     ]
    }
   ],
   "source": [
    "models, names = get_models()\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# evaluate each model\n",
    "for i in range(len(models)):\n",
    "\t# evaluate the model and store results\n",
    "\tresult = fivetrials(X_sample,y_sample,models[i],sample=SMOTE(random_state=0))\n",
    "\n",
    "\tG_mean=result[0]\n",
    "\tBacc=result[1]\n",
    "# summarize performance\n",
    "\trecall=result[3]\n",
    "\tFbeta=result[2]\n",
    "\tT=result[-1]\n",
    "\tacc_s=np.mean(np.array(result)[:4,:],axis=0)\n",
    "\t# summarize and store\n",
    "\tprint('>%s: Average G-mean:%.3f(%.3f) ' % ('SMOTE'+names[i],np.mean(G_mean),np.std(G_mean)))\n",
    "\tprint('>%s: Average Balanced_Acc: %.3f(%.3f) ' % ('SMOTE'+names[i],np.mean(Bacc),np.std(Bacc)))\n",
    "\tprint('>%s: Average Fbeta: %.3f(%.3f)' % ('SMOTE'+names[i],np.mean(Fbeta),np.std(Fbeta)))\n",
    "\tprint('>%s: Average Recall: %.3f(%.3f)' % ('SMOTE'+names[i],np.mean(recall),np.std(recall)))    \n",
    "\tprint('>%s: Average Training time: %.3f(%.3f)' % ('SMOTE'+names[i],np.mean(T),np.std(T)))\n",
    "\tprint('>%s: Average accuracy_score: %.3f(%.3f)' % ('SMOTE'+names[i],np.mean(acc_s),np.std(acc_s)))\n",
    "\tprint('>%s: Average Score: %.3f(%.3f)' % ('SMOTE'+names[i],np.mean(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1)),np.std(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1))))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">RUSLR: Average G-mean:0.870(0.003) \n",
      ">RUSLR: Average Balanced_Acc: 0.871(0.003) \n",
      ">RUSLR: Average Fbeta: 0.700(0.007)\n",
      ">RUSLR: Average Recall: 0.875(0.018)\n",
      ">RUSLR: Average Training time: 0.116(0.025)\n",
      ">RUSLR: Average accuracy_score: 0.829(0.005)\n",
      ">RUSLR: Average Score: 0.830(0.005)\n",
      ">RUSSVM: Average G-mean:0.844(0.006) \n",
      ">RUSSVM: Average Balanced_Acc: 0.845(0.006) \n",
      ">RUSSVM: Average Fbeta: 0.652(0.016)\n",
      ">RUSSVM: Average Recall: 0.861(0.030)\n",
      ">RUSSVM: Average Training time: 0.454(0.060)\n",
      ">RUSSVM: Average accuracy_score: 0.800(0.006)\n",
      ">RUSSVM: Average Score: 0.800(0.006)\n",
      ">RUSBAG: Average G-mean:0.840(0.010) \n",
      ">RUSBAG: Average Balanced_Acc: 0.840(0.009) \n",
      ">RUSBAG: Average Fbeta: 0.643(0.018)\n",
      ">RUSBAG: Average Recall: 0.860(0.017)\n",
      ">RUSBAG: Average Training time: 80.929(3.574)\n",
      ">RUSBAG: Average accuracy_score: 0.796(0.009)\n",
      ">RUSBAG: Average Score: 0.788(0.009)\n",
      ">RUSAdaboost: Average G-mean:0.832(0.003) \n",
      ">RUSAdaboost: Average Balanced_Acc: 0.832(0.003) \n",
      ">RUSAdaboost: Average Fbeta: 0.633(0.005)\n",
      ">RUSAdaboost: Average Recall: 0.835(0.013)\n",
      ">RUSAdaboost: Average Training time: 1.333(0.035)\n",
      ">RUSAdaboost: Average accuracy_score: 0.783(0.005)\n",
      ">RUSAdaboost: Average Score: 0.780(0.005)\n",
      ">RUSCART: Average G-mean:0.764(0.021) \n",
      ">RUSCART: Average Balanced_Acc: 0.765(0.021) \n",
      ">RUSCART: Average Fbeta: 0.533(0.029)\n",
      ">RUSCART: Average Recall: 0.761(0.057)\n",
      ">RUSCART: Average Training time: 0.106(0.007)\n",
      ">RUSCART: Average accuracy_score: 0.706(0.028)\n",
      ">RUSCART: Average Score: 0.708(0.027)\n",
      ">RUSRF: Average G-mean:0.767(0.007) \n",
      ">RUSRF: Average Balanced_Acc: 0.771(0.007) \n",
      ">RUSRF: Average Fbeta: 0.548(0.011)\n",
      ">RUSRF: Average Recall: 0.694(0.014)\n",
      ">RUSRF: Average Training time: 0.057(0.013)\n",
      ">RUSRF: Average accuracy_score: 0.695(0.008)\n",
      ">RUSRF: Average Score: 0.697(0.008)\n",
      ">RUSMLP: Average G-mean:0.865(0.002) \n",
      ">RUSMLP: Average Balanced_Acc: 0.866(0.002) \n",
      ">RUSMLP: Average Fbeta: 0.691(0.007)\n",
      ">RUSMLP: Average Recall: 0.871(0.020)\n",
      ">RUSMLP: Average Training time: 1.037(0.033)\n",
      ">RUSMLP: Average accuracy_score: 0.823(0.005)\n",
      ">RUSMLP: Average Score: 0.820(0.005)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import fbeta_score\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "models, names = get_models()\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# evaluate each model\n",
    "for i in range(len(models)):\n",
    "\t# evaluate the model and store results\n",
    "\tresult = fivetrials(X_sample,y_sample,models[i],sample=RandomUnderSampler(random_state=0))\n",
    "\n",
    "\tG_mean=result[0]\n",
    "\tBacc=result[1]\n",
    "# summarize performance\n",
    "\trecall=result[3]\n",
    "\tFbeta=result[2]\n",
    "\tT=result[-1]\n",
    "\tacc_s=np.mean(np.array(result)[:4,:],axis=0)\n",
    "\t# summarize and store\n",
    "\tprint('>%s: Average G-mean:%.3f(%.3f) ' % ('RUS'+names[i],np.mean(G_mean),np.std(G_mean)))\n",
    "\tprint('>%s: Average Balanced_Acc: %.3f(%.3f) ' % ('RUS'+names[i],np.mean(Bacc),np.std(Bacc)))\n",
    "\tprint('>%s: Average Fbeta: %.3f(%.3f)' % ('RUS'+names[i],np.mean(Fbeta),np.std(Fbeta)))\n",
    "\tprint('>%s: Average Recall: %.3f(%.3f)' % ('RUS'+names[i],np.mean(recall),np.std(recall)))    \n",
    "\tprint('>%s: Average Training time: %.3f(%.3f)' % ('RUS'+names[i],np.mean(T),np.std(T)))\n",
    "\tprint('>%s: Average accuracy_score: %.3f(%.3f)' % ('RUS'+names[i],np.mean(acc_s),np.std(acc_s)))\n",
    "\tprint('>%s: Average Score: %.3f(%.3f)' % ('RUS'+names[i],np.mean(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1)),np.std(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1))))\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DRM(P)   \n",
    "Mathod in ”Discriminative Ridge Machine: A Classifier for High-Dimensional Data or Imbalanced Data“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Poly_close_solution:\n",
    "    def __init__(self,kernel='self._poly',penalty = None,gamma=1,degree = 2,coef0=0,alpha=1,beta=1):\n",
    "        self.W = None\n",
    "        self.penalty = penalty\n",
    "        self.d = degree\n",
    "        self.b = coef0\n",
    "        self.alpha=alpha\n",
    "        self.beta=beta\n",
    "        self.g=gamma\n",
    "        self.kernel=kernel\n",
    "        \n",
    "    def _poly(self,X_1,X_2):\n",
    "        return (self.g*(X_1.dot(X_2.T))+self.b)**self.d\n",
    "    def _rbf(self,x,y):\n",
    "        return np.exp(-self.g*np.sum((x[...,None,:]-y)**2,axis=2))\n",
    "\n",
    "    def poly_B_matrix(self,X,Y):\n",
    "        n=Y[Y==1].shape[0]\n",
    "        X_sample_split=np.array(np.split(X,np.array([X.shape[0]-n])))\n",
    "        B=np.zeros((X.shape[0],X.shape[0]))\n",
    "        I=0\n",
    "        for  m in X_sample_split:\n",
    "            I+=m.shape[0]\n",
    "            B[I-m.shape[0]:I,I-m.shape[0]:I]=eval(self.kernel+'(m,m)')/m.shape[0]\n",
    "        return B\n",
    "    \n",
    "    def K_x(self,X,x_t):\n",
    "        return eval(self.kernel+'(X,x_t)')\n",
    "    \n",
    "    def QplusBeta(self,X,Y):\n",
    "        s=time.time()\n",
    "        K=eval(self.kernel+'(X,X)')\n",
    "        H_p=np.diag(np.diagonal(K))\n",
    "        B_p=self.poly_B_matrix(X,Y)\n",
    "        Qplus_beta=K+self.alpha*(H_p-B_p)+np.diag([self.beta]*X.shape[0])\n",
    "        e=time.time()\n",
    "        return K,Qplus_beta\n",
    "    def fit(self,invQ,X,x_t):\n",
    "        \n",
    "        \n",
    "        W=invQ.dot(self.K_x(X,x_t))##求逆耗时\n",
    "        \n",
    "        return W.reshape(-1)\n",
    "    def delta_phi(self,K,invQ,X,Y,x_t,j):\n",
    "        \n",
    "        w=self.fit(invQ,X,x_t)\n",
    "        w_noty=w\n",
    "\n",
    "        w_y=np.zeros((X.shape[0]))\n",
    "        indice=np.argwhere(Y==j).reshape(-1)\n",
    "        w_noty[indice]=0\n",
    "        w_y[indice]=w[indice]\n",
    "        delta=np.dot(w_y,K.dot(w_y))+np.dot(w_noty,K.dot(w_noty))-2*np.dot(w_y,self.K_x(X,x_t))\n",
    "        return delta\n",
    "    def predict(self,K,invQ,X,Y,x_t):\n",
    "        delta=[]\n",
    "        for k in np.unique(Y):\n",
    "            delta.append(self.delta_phi(K,invQ,X,Y,x_t,k))\n",
    "        return np.argmin(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "from numpy.linalg import inv,det\n",
    "metrics_names=['G-mean','Balanced_acc','F1_score','Recall','Precision']\n",
    "def DRM_FiveCV(X,y,model=Poly_close_solution(\n",
    "                                )):\n",
    "    \n",
    "    G_m,bacc,f2,rec,pre=list(),list(),list(),list(),list()\n",
    "    for train_index,test_index in cv.split(X,y):\n",
    "#                 s_1=time.time()\n",
    "                K,Q_beta=model.QplusBeta(X[train_index],y[train_index])\n",
    "                invQ=inv(Q_beta)\n",
    "                y_test_label=list()\n",
    "                for i in range(y[test_index].shape[0]):\n",
    "#                 X_resampled_smote,y_resampled_smote=SMOTE().fit_resample(X[train_index], y[train_index])\n",
    "                    y_test_label.append(model.predict(K,invQ,X[train_index],y[train_index],X[test_index][i]))\n",
    "#                 y_train_proba=model.predict_proba(X[train_index])\n",
    "#                 y_train_label=model.predict(X[train_index])\n",
    "#                 t_1=time.time()\n",
    "#                 print(\"running time for each CV:%.2f s\"%(t_1-s_1))\n",
    "                \n",
    "#     print(classification_report(y,y_pre))\n",
    "                rec.append(recall_score(y[test_index],y_test_label))\n",
    "                pre.append(precision_score(y[test_index],y_test_label))\n",
    "                G_m.append(math.sqrt(recall_score(y[test_index],y_test_label)*recall_score(y[test_index],y_test_label,pos_label=0)))\n",
    "                bacc.append(balanced_accuracy_score(y[test_index],y_test_label))\n",
    "                f2.append(fbeta_score(y[test_index],y_test_label,beta=max(2,np.log(y_sample.shape[0]/sum(y_sample)-1))))\n",
    " \n",
    "    return G_m,bacc,f2,rec,pre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 2048)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DRM_L_fivefold-cv time:1564.84\n"
     ]
    }
   ],
   "source": [
    "X_tr_ord=np.concatenate((x_tr[y_tr==0],x_tr[y_tr==1]))\n",
    "y_tr_ord=np.concatenate((y_tr[y_tr==0],y_tr[y_tr==1]))\n",
    "all_results_Ldrm = []\n",
    "import time\n",
    "import math\n",
    "# for k in mean_lr:\n",
    "start=time.time()\n",
    "\n",
    "for k in parameter:\n",
    "    \n",
    "    for d in degree:\n",
    "        Lo_score=np.mean(DRM_FiveCV(X_tr_ord,y_tr_ord,model=Poly_close_solution(alpha=k,degree=d\n",
    "                            )),axis=1)\n",
    "\n",
    "        metric_res = {'alpha':k,'degree':d}\n",
    "\n",
    "        for name, value in zip(metrics_names, Lo_score):\n",
    "    #             print(name, ': ', value)\n",
    "                metric_res[name] = value\n",
    "\n",
    "\n",
    "        all_results_Ldrm.append(metric_res)\n",
    "end=time.time()\n",
    "print(\"DRM_L_fivefold-cv time:%.2f\"%(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(all_results_Ldrm).to_csv('/Users/lvjingzhe/Desktop/璇/modified_althogram/code/0514CV_select_DRML_celebglass.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree=np.array([2,3,4,5,8,10])\n",
    "gamma=np.array([0.001,0.01,0.1,1,10])\n",
    "parameter=np.array([0.01,0.1,1,10,100])\n",
    "def fivetrials_DRM(X,y):\n",
    "    \n",
    "    \n",
    "    G_m,bacc,fbeta,rec,pre,T=list(),list(),list(),list(),list(),list()\n",
    "    \n",
    "    for train_index,test_index in cv.split(X, y):\n",
    "\n",
    "        start=time.time()\n",
    "        P_model=Poly_close_solution(alpha=0.1,beta=1,degree=2)\n",
    "        K,Q_beta=P_model.QplusBeta(X[test_index],y[test_index])\n",
    "        invQ=inv(Q_beta)\n",
    "        prdict_y=[]\n",
    "        for i in range(y[train_index].shape[0]):\n",
    "            prdict_y.append(P_model.predict(K,invQ,X[test_index],y[test_index],X[train_index][i]))\n",
    "        end=time.time()\n",
    "        \n",
    "        print(classification_report(y[train_index],prdict_y))\n",
    "        rec.append(recall_score(y[train_index],prdict_y))\n",
    "        G_m.append(np.sqrt(recall_score(y[train_index],prdict_y)*recall_score(y[train_index],prdict_y,pos_label=0)))\n",
    "        pre.append(precision_score(y[train_index],prdict_y))\n",
    "    #     auc.append(roc_auc_score(y[test_index],prob_y[:,-1]))\n",
    "        bacc.append(balanced_accuracy_score(y[train_index],prdict_y))\n",
    "        fbeta.append(fbeta_score(y[train_index],prdict_y,beta=max(2,np.log(y_sample.shape[0]/sum(y_sample)-1))))\n",
    "        T.append(end-start)\n",
    "#     pipeline = Pipeline(steps=steps)\n",
    "   \n",
    "#     print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_L)))\n",
    "#     print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_L)))\n",
    "#     print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_L,beta=3.66)))\n",
    "#     print('Score :{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end_d-start_d)))\n",
    "\n",
    "\n",
    "\n",
    "    return G_m,bacc,fbeta,rec,pre,T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sample_ord=np.concatenate((X_sample[y_sample==0],X_sample[y_sample==1]))\n",
    "y_sample_ord=np.concatenate((y_sample[y_sample==0],y_sample[y_sample==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      1.00      0.99     15200\n",
      "         1.0       0.94      0.52      0.67       800\n",
      "\n",
      "    accuracy                           0.97     16000\n",
      "   macro avg       0.96      0.76      0.83     16000\n",
      "weighted avg       0.97      0.97      0.97     16000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      1.00      0.99     15200\n",
      "         1.0       0.93      0.52      0.67       800\n",
      "\n",
      "    accuracy                           0.97     16000\n",
      "   macro avg       0.95      0.76      0.83     16000\n",
      "weighted avg       0.97      0.97      0.97     16000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      1.00      0.99     15200\n",
      "         1.0       0.93      0.51      0.66       800\n",
      "\n",
      "    accuracy                           0.97     16000\n",
      "   macro avg       0.95      0.75      0.82     16000\n",
      "weighted avg       0.97      0.97      0.97     16000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      1.00      0.99     15200\n",
      "         1.0       0.92      0.53      0.68       800\n",
      "\n",
      "    accuracy                           0.97     16000\n",
      "   macro avg       0.95      0.77      0.83     16000\n",
      "weighted avg       0.97      0.97      0.97     16000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.97      1.00      0.99     15200\n",
      "         1.0       0.95      0.49      0.65       800\n",
      "\n",
      "    accuracy                           0.97     16000\n",
      "   macro avg       0.96      0.74      0.82     16000\n",
      "weighted avg       0.97      0.97      0.97     16000\n",
      "\n",
      ">DRM:: Average G-mean:0.717(0.010) \n",
      ">DRM:: Average Balanced_Acc: 0.756(0.007) \n",
      ">DRM:: Average Fbeta: 0.540(0.014)\n",
      ">DRM:: Average Recall: 0.514(0.015)\n",
      ">DRM:: Average Training time: 3771.795(261.516)\n",
      ">DRM:: Average accuracy_score: 0.632(0.012)\n",
      ">DRM:: Average Score: 0.627(0.012)\n"
     ]
    }
   ],
   "source": [
    "result_DRM = fivetrials_DRM(X_sample_ord,y_sample_ord)\n",
    "\n",
    "G_mean=result_DRM[0]\n",
    "Bacc=result_DRM[1]\n",
    "# summarize performance\n",
    "recall=result_DRM[3]\n",
    "Fbeta=result_DRM[2]\n",
    "T=result_DRM[-1]\n",
    "acc_s=np.mean(np.array(result_DRM)[:4,:],axis=0)\n",
    "\t# summarize and store\n",
    "print('>%s: Average G-mean:%.3f(%.3f) ' % ('DRM:',np.mean(G_mean),np.std(G_mean)))\n",
    "print('>%s: Average Balanced_Acc: %.3f(%.3f) ' % ('DRM:',np.mean(Bacc),np.std(Bacc)))\n",
    "print('>%s: Average Fbeta: %.3f(%.3f)' % ('DRM:',np.mean(Fbeta),np.std(Fbeta)))\n",
    "print('>%s: Average Recall: %.3f(%.3f)' % ('DRM:',np.mean(recall),np.std(recall)))    \n",
    "print('>%s: Average Training time: %.3f(%.3f)' % ('DRM:',np.mean(T),np.std(T)))\n",
    "print('>%s: Average accuracy_score: %.3f(%.3f)' % ('DRM:',np.mean(acc_s),np.std(acc_s)))\n",
    "print('>%s: Average Score: %.3f(%.3f)' % ('DRM:',np.mean(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1)),np.std(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1))))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fivetrials_DRM_(X,y):\n",
    "    \n",
    "    \n",
    "    T_tr,T_te=list(),list()\n",
    "    \n",
    "    for train_index,test_index in cv.split(X, y):\n",
    "\n",
    "        start=time.time()\n",
    "        P_model=Poly_close_solution(alpha=0.1,beta=1,degree=2)\n",
    "        K,Q_beta=P_model.QplusBeta(X[test_index],y[test_index])\n",
    "        invQ=inv(Q_beta)\n",
    "        \n",
    "        end=time.time()\n",
    "        s_1=time.time()\n",
    "\n",
    "        for i in range(5):\n",
    "           \n",
    "            prdict_y=P_model.predict(K,invQ,X[test_index],y[test_index],X[train_index][i])\n",
    "        t_1=time.time()\n",
    "        \n",
    "        T_te.append(0.2*(t_1-s_1))\n",
    "        T_tr.append(end-start)\n",
    "#     pipeline = Pipeline(steps=steps)\n",
    "   \n",
    "#     print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_L)))\n",
    "#     print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_L)))\n",
    "#     print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_L,beta=3.66)))\n",
    "#     print('Score :{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end_d-start_d)))\n",
    "\n",
    "\n",
    "\n",
    "    return T_tr,T_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fivetrials_DRM_preT(X,y):\n",
    "    \n",
    "    \n",
    "    T=list()\n",
    "    \n",
    "    for train_index,test_index in cv.split(X, y):\n",
    "\n",
    "        P_model=Poly_close_solution(alpha=0.1,beta=1,degree=2)\n",
    "        K,Q_beta=P_model.QplusBeta(X[test_index],y[test_index])\n",
    "        invQ=inv(Q_beta)\n",
    "        start=time.time()\n",
    "\n",
    "        for i in range(5):\n",
    "           \n",
    "            prdict_y=P_model.predict(K,invQ,X[test_index],y[test_index],X[train_index][i])\n",
    "        end=time.time()\n",
    "        \n",
    "        T.append(0.2*(end-start))\n",
    "#     pipeline = Pipeline(steps=steps)\n",
    "   \n",
    "#     print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_L)))\n",
    "#     print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_L)))\n",
    "#     print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_L,beta=3.66)))\n",
    "#     print('Score :{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end_d-start_d)))\n",
    "\n",
    "\n",
    "\n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 2048)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sample_ord.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">DRM:: Average Training time: 2.494(0.085)\n",
      ">DRM:: Average testing time: 0.238(0.010)\n"
     ]
    }
   ],
   "source": [
    "T_tr,T_te = fivetrials_DRM_(X_sample_ord,y_sample_ord)\n",
    "print('>%s: Average Training time: %.3f(%.3f)' % ('DRM:',np.mean(T_tr),np.std(T_tr)))\n",
    "print('>%s: Average testing time: %.3f(%.3f)' % ('DRM:',np.mean(T_te),np.std(T_te)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">DRM:: Average testing time: 0.220(0.010)\n"
     ]
    }
   ],
   "source": [
    "T_predict = fivetrials_DRM_preT(X_sample_ord,y_sample_ord)\n",
    "print('>%s: Average testing time: %.3f(%.3f)' % ('DRM:',np.mean(T_predict),np.std(T_predict)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max-mean loss method\n",
    "\" Q. Xu, X. M. Xuan, Nonlinear regression without iid assumption\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Class solution_XX() is the algorithm based on Xiaohua,Xuan[2019], the improvement is that\n",
    "we add a parameter of \"penalty\".'''\n",
    "class solution_XX:\n",
    "    \n",
    "    \n",
    "    def __init__(self,penalty = None,Lambda = 0.03,a = 0.5,epochs = 200):\n",
    "        self.W = None\n",
    "        self.penalty = penalty\n",
    "        self.Lambda = Lambda\n",
    "        self.a = a\n",
    "        self.epochs =epochs\n",
    "        self.sigmoid = lambda x:1/(1 + np.exp(-x))\n",
    "        \n",
    "#     err=(y_tr-s.sigmoid(np.dot(X_train,w)))**2\n",
    "#     err[index]=2.6*err[index]\n",
    "\n",
    "    def f_XX(self,X,Y):\n",
    "        if self.penalty=='l1':f=np.array([np.mean((self.sigmoid(np.dot(x,self.W))-y)**2 )+self.Lambda*np.sum(np.abs(self.W)) for x,y in zip(X,Y)])\n",
    "        elif self.penalty=='l2':f=np.array([np.mean((self.sigmoid(np.dot(x,self.W))-y)**2 )+self.Lambda*np.sum(self.W**2) for x,y in zip(X,Y)])#pre_Xtrain,pre_Ytrain\n",
    "        else:f=np.array([np.mean((self.sigmoid(np.dot(x,self.W))-y)**2 ) for x,y in zip(X,Y)])\n",
    "\n",
    "        return f         \n",
    "    def Gf_XX(self,X,Y):#To compute the Derivative matrix, the shape of which is N*2\n",
    "        if self.penalty=='l1':d=np.array([x.T.dot(0.02*(self.sigmoid(np.dot(x,self.W))-y)*self.sigmoid(np.dot(x,self.W))*(1-self.sigmoid(np.dot(x,self.W))))+self.Lambda*np.sign(self.W )for x,y in zip(X,Y)])\n",
    "        elif self.penalty=='l2':\n",
    "            d=np.array([x.T.dot(0.02*(self.sigmoid(np.dot(x,self.W))-y)*self.sigmoid(np.dot(x,self.W))*(1-self.sigmoid(np.dot(x,self.W))))+2*self.Lambda*self.W for x,y in zip(X,Y)])\n",
    "#     return d.reshape(20,12289)\n",
    "        else:d=np.array([x.T.dot(0.02*(self.sigmoid(np.dot(x,self.W))-y)*self.sigmoid(np.dot(x,self.W))*(1-self.sigmoid(np.dot(x,self.W)))) for x,y in zip(X,Y)])\n",
    "        return d\n",
    "    def direction_XX(self,X,Y):\n",
    "        gra=self.Gf_XX(X,Y)\n",
    "        p=matrix(gra.dot(gra.T),tc='d')\n",
    "        q=matrix(-self.f_XX(X,Y),tc='d')\n",
    "        G=matrix(np.diag(np.array([-1]*(Y.shape[0]))),tc='d')#N=20\n",
    "        h=matrix(np.array([[0]]*(Y.shape[0])),tc='d')\n",
    "        A=matrix([[1.0]]*(Y.shape[0]))\n",
    "        b=matrix([1.0])\n",
    "        solvers.options['show_progress'] = False\n",
    "        sol = solvers.qp(p,q,G,h,A,b)\n",
    "        t=np.array(sol['x'])\n",
    "        d= -(gra.T.dot(t))\n",
    "        return d.reshape((X_train.shape[-1],))\n",
    "\n",
    "    def fit(self,X,Y):\n",
    "        \n",
    "        call=[]\n",
    "        pre=[]\n",
    "        loss=[]\n",
    "        testloss=[]\n",
    "        np.random.seed(1324)\n",
    "        self.W=np.random.random((X_train.shape[-1],))*2-1\n",
    "#         self.W=w_2\n",
    "        n=y_te[y_te==1.].shape[0]\n",
    "        for k in range(200):\n",
    "    #     while np.linalg.norm(d)//10**(-8) >= 10:\n",
    "            d=self.direction_XX(X,Y)\n",
    "#             print(np.linalg.norm(d))\n",
    "            if np.linalg.norm(d)//10**(-7) < 25:\n",
    "                break\n",
    "            sigma=0.8\n",
    "            f_1=np.max(self.f_XX(X,Y))\n",
    "            w=self.W\n",
    "            self.W=d*sigma+w\n",
    "            while np.max(self.f_XX(X,Y))>np.max(f_1):\n",
    "                sigma=sigma*0.8\n",
    "                self.W=d*sigma+w\n",
    "            self.W=d*sigma+w\n",
    "\n",
    "            \n",
    "    \n",
    "        return self.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fivetrials_XX(X,y,model=solution_XX()):\n",
    "    pipeline = model\n",
    "    \n",
    "    G_m,bacc,fbeta,rec,pre,T=list(),list(),list(),list(),list(),list()\n",
    "    \n",
    "    for train_index,test_index in cv.split(X, y):\n",
    "        \n",
    "        start=time.time()\n",
    "\n",
    "        pre_X,pre_Y=split(X[test_index],y[test_index])\n",
    "#         print(pre_X.shape)\n",
    "\n",
    "        w_x=pipeline.fit(pre_X,pre_Y)\n",
    "        end=time.time()\n",
    "        prdict_y=pipeline.sigmoid(X[train_index].dot(w_x))\n",
    "        prdict_y[prdict_y>=0.5]=1\n",
    "        prdict_y[prdict_y<0.5]=0\n",
    "\n",
    "        print(classification_report(y[train_index],prdict_y))\n",
    "        \n",
    "        rec.append(recall_score(y[train_index],prdict_y))\n",
    "        G_m.append(np.sqrt(recall_score(y[train_index],prdict_y)*recall_score(y[train_index],prdict_y,pos_label=0)))\n",
    "        pre.append(precision_score(y[train_index],prdict_y))\n",
    "    #     auc.append(roc_auc_score(y[test_index],prob_y[:,-1]))\n",
    "        bacc.append(balanced_accuracy_score(y[train_index],prdict_y))\n",
    "        fbeta.append(fbeta_score(y[train_index],prdict_y,beta=max(2,np.log(y_sample.shape[0]/sum(y_sample)-1))))\n",
    "        T.append(end-start)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return G_m,bacc,fbeta,rec,pre,T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(x,y):\n",
    "    k=int(y.shape[0]/sum(y))-1\n",
    "    n=y[y==1].shape[0]\n",
    "    m=int((x.shape[0]-n)/k)\n",
    "    indice=[i*m for i in range(1,k)]\n",
    "    indice.append(x.shape[0]-n)\n",
    "    x_c=np.concatenate((x[y==0],x[y==1]))\n",
    "    y_c=np.concatenate((np.array([0]*(x.shape[0]-n)),np.array([1]*n)))\n",
    "    return np.array(np.split(x_c,indice)),np.array(np.split(y_c,indice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.90      0.94     15200\n",
      "         1.0       0.28      0.73      0.41       800\n",
      "\n",
      "    accuracy                           0.89     16000\n",
      "   macro avg       0.63      0.82      0.67     16000\n",
      "weighted avg       0.95      0.89      0.91     16000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.91      0.94     15200\n",
      "         1.0       0.30      0.76      0.43       800\n",
      "\n",
      "    accuracy                           0.90     16000\n",
      "   macro avg       0.64      0.83      0.69     16000\n",
      "weighted avg       0.95      0.90      0.92     16000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.92      0.95     15200\n",
      "         1.0       0.34      0.72      0.46       800\n",
      "\n",
      "    accuracy                           0.91     16000\n",
      "   macro avg       0.66      0.82      0.71     16000\n",
      "weighted avg       0.95      0.91      0.93     16000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.89      0.94     15200\n",
      "         1.0       0.27      0.79      0.41       800\n",
      "\n",
      "    accuracy                           0.89     16000\n",
      "   macro avg       0.63      0.84      0.67     16000\n",
      "weighted avg       0.95      0.89      0.91     16000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.89      0.93     15200\n",
      "         1.0       0.26      0.77      0.39       800\n",
      "\n",
      "    accuracy                           0.88     16000\n",
      "   macro avg       0.62      0.83      0.66     16000\n",
      "weighted avg       0.95      0.88      0.91     16000\n",
      "\n",
      ">max_mean loss:: Average G-mean:0.825(0.008) \n",
      ">max_mean loss:: Average Balanced_Acc: 0.828(0.007) \n",
      ">max_mean loss:: Average Fbeta: 0.646(0.011)\n",
      ">max_mean loss:: Average Recall: 0.754(0.023)\n",
      ">max_mean loss:: Average Training time: 2.459(0.381)\n",
      ">max_mean loss:: Average accuracy_score: 0.763(0.011)\n",
      ">max_mean loss:: Average Score: 0.757(0.011)\n"
     ]
    }
   ],
   "source": [
    "result_XX = fivetrials_XX(X_sample_XX,y_sample)\n",
    "\n",
    "G_mean=result_XX[0]\n",
    "Bacc=result_XX[1]\n",
    "# summarize performance\n",
    "recall=result_XX[3]\n",
    "Fbeta=result_XX[2]\n",
    "T=result_XX[-1]\n",
    "acc_s=np.mean(np.array(result_XX)[:4,:],axis=0)\n",
    "\t# summarize and store\n",
    "print('>%s: Average G-mean:%.3f(%.3f) ' % ('max_mean loss:',np.mean(G_mean),np.std(G_mean)))\n",
    "print('>%s: Average Balanced_Acc: %.3f(%.3f) ' % ('max_mean loss:',np.mean(Bacc),np.std(Bacc)))\n",
    "print('>%s: Average Fbeta: %.3f(%.3f)' % ('max_mean loss:',np.mean(Fbeta),np.std(Fbeta)))\n",
    "print('>%s: Average Recall: %.3f(%.3f)' % ('max_mean loss:',np.mean(recall),np.std(recall)))    \n",
    "print('>%s: Average Training time: %.3f(%.3f)' % ('max_mean loss:',np.mean(T),np.std(T)))\n",
    "print('>%s: Average accuracy_score: %.3f(%.3f)' % ('max_mean loss:',np.mean(acc_s),np.std(acc_s)))\n",
    "print('>%s: Average Score: %.3f(%.3f)' % ('max_mean loss:',np.mean(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1)),np.std(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1))))\n",
    "          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean_uncertain method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_(x,u):\n",
    "    return 1.0/(1.0+np.exp(-x-u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanuncertainty(x,n):\n",
    "    r=[]\n",
    "    for i in range(0,len(x)+1-n,n//5):\n",
    "        r.append(np.mean(x[i:i+n]))\n",
    "    return min(r),max(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equa_m(x,n,r):\n",
    "    pre_in=(np.dot(x_tr,model_in.coef_.T)+model_in.intercept_).reshape(x_tr.shape[0],)\n",
    "    ini_err=y_tr-sigmoid_(pre_in,x)##ini_err is the predicted err of training set, based on LR\n",
    "\n",
    "    for k in range(ini_err.shape[0]):\n",
    "    \n",
    "        if y_tr[k]==1:\n",
    "            ini_err[k]=r*ini_err[k]\n",
    "\n",
    "    \n",
    "    return meanuncertainty(ini_err,n)[1]\n",
    "def equa_fivem(x,n,r,Mod,x_train,y_train):\n",
    "    pre_in=(np.dot(x_train,Mod.coef_.T)+Mod.intercept_).reshape(x_train.shape[0],)\n",
    "    \n",
    "    ini_err=y_train-sigmoid_(pre_in,x)##ini_err is the predicted err of training set, based on LR\n",
    "\n",
    "    for k in range(ini_err.shape[0]):\n",
    "        if y_train[k]==1:\n",
    "            ini_err[k]=r*ini_err[k]\n",
    "\n",
    "    \n",
    "    return meanuncertainty(ini_err,n)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(mean_lr).to_csv('/Users/lvjingzhe/Desktop/璇/modified_althogram/code/0513_lr_mean_celebglass.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.14573387])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fsolve(lambda x:equa_m(x,20,0.5*y_sample.shape[0]/sum(y_sample)),0.5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_lr=np.array([fsolve(lambda x:equa_m(x,n,0.5*y_sample.shape[0]/sum(y_sample)),0.5 )\n",
    "                  for n in range(20,500,10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_lr=np.array([fsolve(lambda x:equa_v(x,n,0.5*y_sample.shape[0]/sum(y_sample)),[0.5,1.5] )\n",
    "                  for n in range(50,1000,10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(var_lr).to_csv('/Users/lvjingzhe/Desktop/璇/modified_althogram/code/0513_lrvar_mean_celebglass.csv')\n",
    "# pd.DataFrame(var_xh).to_csv('/Users/lvjingzhe/Desktop/璇/modified_althogram/code/0512_xhvar_mean_celebhat.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select(arr,n,k):##arr为1-D\n",
    "    per=[]\n",
    "    A=abs(arr-k)\n",
    "\n",
    "    for j in np.sort(A)[:n]:\n",
    "        per.append(list(A).index(j))\n",
    "    return arr[per]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice optimal upper mean on  Cross-validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"We use 'evaluate_modelxh' to obtain the average performance in 5-fold cross-validation set, for fixed upper mean 'm' \"\"\"\n",
    "\n",
    "metrics_names=['Balanced_acc','F2_score','Recall','Precision']\n",
    "def evaluate_model_lr(X,y,m,model=LogisticRegression()):\n",
    "    \n",
    "    bacc,f2,rec,pre=list(),list(),list(),list()\n",
    "    for train_index,test_index in cv.split(X,y):\n",
    "#                 X_resampled_smote,y_resampled_smote=SMOTE().fit_resample(X[train_index], y[train_index])\n",
    "                model.fit(X[train_index],y[train_index])\n",
    "                prob_y=sigmoid_((np.dot(X[test_index],model.coef_.T)+model.intercept_),m)\n",
    "                \n",
    "                \n",
    "                prdict_y=np.round(prob_y)\n",
    "                rec.append(recall_score(y[test_index],prdict_y))\n",
    "                pre.append(precision_score(y[test_index],prdict_y))\n",
    "                \n",
    "                bacc.append(balanced_accuracy_score(y[test_index],prdict_y))\n",
    "                f2.append(fbeta_score(y[test_index],prdict_y,beta=2))\n",
    " \n",
    "    return bacc,f2,rec,pre\n",
    "    \n",
    "#                 X_resampled_smote,y_resampled_smote=SMOTE().fit_resample(X[train_index], y[train_index])\n",
    "           \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "c=2\n",
    "# for k in mean_lr:\n",
    "strat=time.time()\n",
    "for k in mean_lr.reshape(-1):\n",
    "    result=np.mean(evaluate_model_lr(x_tr,y_tr,k),axis=1)\n",
    "    metric_res = {'window':10*c,'upper_mean': k}\n",
    "    c+=1\n",
    "    for name, value in zip(metrics_names, result):\n",
    "#             print(name, ': ', value)\n",
    "            metric_res[name] = value\n",
    "       \n",
    "\n",
    "    all_results.append(metric_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal window size N by fivefold CV: 20.000 \n"
     ]
    }
   ],
   "source": [
    "eva=pd.DataFrame(all_results)\n",
    "#                 eva.to_csv('{:}: datapre_diabetesCV_which_mu_to_set_inLRBIAS.csv'.format(k))\n",
    "bias=eva[eva.iloc[:,-4]==(eva.iloc[:,-4]).max()]\n",
    "N_optimal=bias['window'].values[0]\n",
    "print(\"optimal window size N by fivefold CV: %.3f \"%(bias['window']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "eva.to_csv('/Users/lvjingzhe/Desktop/璇/modified_althogram/code/0531_CV_which_mu_toset_in_CelebA_16.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_names=['Balanced_acc','F2_score','Recall','Precision']\n",
    "def Fivetrails_mean(k,X,y,model=LogisticRegression()):\n",
    "    mean=mean_lr.reshape(-1)\n",
    "    G_m,bacc,f2,rec,pre,T=list(),list(),list(),list(),list(),list()\n",
    "    for train_index,test_index in cv.split(X,y):\n",
    "                s_1=time.time()\n",
    "\n",
    "                model.fit(X[test_index],y[test_index])\n",
    "                mean_u=fsolve(lambda x:equa_fivem(x,N_optimal,0.5*(k+1),model,X[test_index],y[test_index]),0.5 ) \n",
    "                t_1=time.time()\n",
    "\n",
    "                print(\"The upper_mean : %.3f \"%(mean_u))\n",
    "                prob_y=sigmoid_((np.dot(X[train_index],model.coef_.T)+model.intercept_).reshape(X[train_index].shape[0],),mean_u\n",
    "                             )\n",
    "                \n",
    "                \n",
    "                \n",
    "                prdict_y=np.round(prob_y)\n",
    "                print(classification_report(y[train_index],prdict_y))\n",
    "                rec.append(recall_score(y[train_index],prdict_y))\n",
    "                pre.append(precision_score(y[train_index],prdict_y))\n",
    "                G_m.append(np.sqrt(recall_score(y[train_index],prdict_y)*recall_score(y[train_index],prdict_y,pos_label=0)))\n",
    "                bacc.append(balanced_accuracy_score(y[train_index],prdict_y))\n",
    "                f2.append(fbeta_score(y[train_index],prdict_y,beta=max(2,np.log(k))))\n",
    "                T.append(t_1-s_1)\n",
    "    return G_m,bacc,f2,rec,pre,T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tesing performance of Mean-uncertain method on five random trails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The upper_mean : 4.090 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.90      0.95     15200\n",
      "         1.0       0.32      0.89      0.47       800\n",
      "\n",
      "    accuracy                           0.90     16000\n",
      "   macro avg       0.66      0.89      0.71     16000\n",
      "weighted avg       0.96      0.90      0.92     16000\n",
      "\n",
      "The upper_mean : 3.997 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.90      0.94     15200\n",
      "         1.0       0.32      0.87      0.47       800\n",
      "\n",
      "    accuracy                           0.90     16000\n",
      "   macro avg       0.65      0.89      0.71     16000\n",
      "weighted avg       0.96      0.90      0.92     16000\n",
      "\n",
      "The upper_mean : 4.131 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.90      0.94     15200\n",
      "         1.0       0.32      0.89      0.47       800\n",
      "\n",
      "    accuracy                           0.90     16000\n",
      "   macro avg       0.66      0.90      0.71     16000\n",
      "weighted avg       0.96      0.90      0.92     16000\n",
      "\n",
      "The upper_mean : 3.183 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.92      0.96     15200\n",
      "         1.0       0.37      0.86      0.51       800\n",
      "\n",
      "    accuracy                           0.92     16000\n",
      "   macro avg       0.68      0.89      0.73     16000\n",
      "weighted avg       0.96      0.92      0.93     16000\n",
      "\n",
      "The upper_mean : 4.124 \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.89      0.94     15200\n",
      "         1.0       0.29      0.88      0.44       800\n",
      "\n",
      "    accuracy                           0.89     16000\n",
      "   macro avg       0.64      0.89      0.69     16000\n",
      "weighted avg       0.96      0.89      0.91     16000\n",
      "\n",
      ">LR_mean:: Average G-mean:0.890(0.004) \n",
      ">LR_mean:: Average Balanced_Acc: 0.891(0.004) \n",
      ">LR_mean:: Average Fbeta: 0.745(0.009)\n",
      ">LR_mean:: Average Recall: 0.879(0.011)\n",
      ">LR_mean:: Average Training time: 0.961(0.334)\n",
      ">LR_mean:: Average Score: 0.848(0.005)\n"
     ]
    }
   ],
   "source": [
    "result = Fivetrails_mean(y_sample.shape[0]/sum(y_sample)-1,X_sample,y_sample)\n",
    "# \n",
    "G_mean=result[0]\n",
    "Bacc=result[1]\n",
    "# summarize performance\n",
    "recall=result[3]\n",
    "Fbeta=result[2]\n",
    "T=result[-1]\n",
    "acc_s=np.mean(np.array(result)[:4,:],axis=0)\n",
    "\n",
    "print('>%s: Average G-mean:%.3f(%.3f) ' % ('LR_mean:',np.mean(G_mean),np.std(G_mean)))\n",
    "print('>%s: Average Balanced_Acc: %.3f(%.3f) ' % ('LR_mean:',np.mean(Bacc),np.std(Bacc)))\n",
    "print('>%s: Average Fbeta: %.3f(%.3f)' % ('LR_mean:',np.mean(Fbeta),np.std(Fbeta)))\n",
    "print('>%s: Average Recall: %.3f(%.3f)' % ('LR_mean:',np.mean(recall),np.std(recall)))\n",
    "print('>%s: Average Training time: %.3f(%.3f)' % ('LR_mean:',np.mean(T),np.std(T)))\n",
    "print('>%s: Average Score: %.3f(%.3f)' % ('LR_mean:',np.mean(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1)),np.std(0.99*acc_s+0.01/(np.array(result)[-1,:]/10*9+1))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volatility-uncertain method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''F_u and F_L compute the maximal and minimal probability of Y=1 '''\n",
    " \n",
    "def F_u(x,arr):\n",
    "    p=[]\n",
    "    for c in arr:\n",
    "        \n",
    "        if c <0:\n",
    "            p.append(2*x[1]*st.norm.cdf(c/x[1])/(x[0]+x[1]))\n",
    "        else:\n",
    "            p.append(1-2*x[0]*st.norm.cdf(-c/x[0])/(x[0]+x[1]))\n",
    "    return np.array(p)\n",
    "def F_L(x,arr):\n",
    "    p=[]\n",
    "    for c in arr:\n",
    "        \n",
    "        if c <0:\n",
    "            p.append(2*x[0]*st.norm.cdf(c/x[0])/(x[0]+x[1]))\n",
    "        else:\n",
    "            p.append(1-2*x[1]*st.norm.cdf(-c/x[1])/(x[0]+x[1]))\n",
    "    return np.array(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''equa_v returns the maximal and minimal err obtained by LR for fixed window size n'''\n",
    "\n",
    "def equa_v(x,n,r):\n",
    " \n",
    "    pre_in=(np.dot(x_tr,model_in.coef_.T)+model_in.intercept_).reshape(x_tr.shape[0],)\n",
    "    err_u=y_tr-F_u(x,pre_in)\n",
    "    err_L=y_tr-F_L(x,pre_in)\n",
    "    for k in range(y_tr.shape[0]):\n",
    "    \n",
    "        if y_tr[k]==1:\n",
    "            err_u[k]=r*err_u[k]\n",
    "            err_L[k]=r*err_L[k]\n",
    "    \n",
    "    return np.array([meanuncertainty(err_u,n)[1],meanuncertainty(err_L,n)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''equa_vxh returns the maximal and minimal err obtained by max-mean loss for fixed window size n'''\n",
    "\n",
    "def equa_vxh(x,n):\n",
    "    pre_in=np.dot(X_train,w_2).reshape(x_tr.shape[0],)\n",
    "    err_u=y_tr-F_u(x,pre_in)\n",
    "    err_L=y_tr-F_L(x,pre_in)\n",
    "#     for k in range(y_tr.shape[0]):\n",
    "    \n",
    "#         if y_tr[k]==1:\n",
    "#             err_u[k]=10*err_u[k]\n",
    "#             err_L[k]=10*err_L[k]\n",
    "    \n",
    "    return np.array([meanuncertainty(err_u,n)[1],meanuncertainty(err_L,n)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.11068971, 2.31470281])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fsolve(lambda x:equa_v(x,80),[0.5,1.5] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice optimal window size on  Cross-validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_evaluate_model(X,y,m,model=LogisticRegression()):\n",
    "    \n",
    "    G_m,bacc,f2,rec,pre=list(),list(),list(),list(),list()\n",
    "   \n",
    "#   \n",
    "    for train_index,test_index in cv.split(X,y):\n",
    "#                 X_resampled_smote,y_resampled_smote=SMOTE().fit_resample(X[train_index], y[train_index])\n",
    "                model.fit(X[train_index],y[train_index])\n",
    "                prob_y=F_u(m,(np.dot(X[test_index],model.coef_.T)+model.intercept_).reshape(X[test_index].shape[0],))\n",
    "        \n",
    "                \n",
    "                prdict_y=np.round(prob_y)\n",
    "                rec.append(recall_score(y[test_index],prdict_y))\n",
    "                pre.append(precision_score(y[test_index],prdict_y))\n",
    "                G_m.append(np.sqrt(recall_score(y[test_index],prdict_y)*recall_score(y[test_index],prdict_y,pos_label=0)))\n",
    "\n",
    "                bacc.append(balanced_accuracy_score(y[test_index],prdict_y))\n",
    "                f2.append(fbeta_score(y[test_index],prdict_y,beta=2))\n",
    " \n",
    "    return G_m,bacc,f2,rec,pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Fivetrails_var(X,y,model=LogisticRegression()):\n",
    "    \n",
    "    G_m,bacc,f2,rec,pre,T=list(),list(),list(),list(),list(),list()\n",
    "    for train_index,test_index in cv.split(X,y):\n",
    "                s_1=time.time()\n",
    "#                 X_resampled_smote,y_resampled_smote=SMOTE().fit_resample(X[train_index], y[train_index])\n",
    "                model.fit(X[test_index],y[test_index])\n",
    "                t_1=time.time()\n",
    "#                 mean_u=np.array([fsolve(lambda x:equa_fivem(x,n,0.5*(k+1),model,X[train_index],y[train_index]),0.5) for n in range(int(y[train_index].shape[0]*0.2),int(y[train_index].shape[0]*0.8),5)])\n",
    "                all_results = []\n",
    "                c=0\n",
    "                # for k in mean_lr:\n",
    "            #     strat=time.time()\n",
    "                for i in var_lr[:10]:\n",
    "                    result=np.mean(var_evaluate_model(X[test_index],y[test_index],i),axis=1)\n",
    "                    metric_res = {'window':10+5*c,'upper_mean': i}\n",
    "                    c+=1\n",
    "                    for name, value in zip(metrics_names, result):\n",
    "                #             print(name, ': ', value)\n",
    "                            metric_res[name] = value\n",
    "\n",
    "\n",
    "                    all_results.append(metric_res)\n",
    "                eva=pd.DataFrame(all_results)\n",
    "#                 eva.to_csv('{:}: datapre_diabetesCV_which_mu_to_set_inLRBIAS.csv'.format(k))\n",
    "                bias=eva[eva.iloc[:,-4]==(eva.iloc[:,-4]).max()]\n",
    "                print(\"optimal volatility by CV:\",np.array(bias['upper_mean']))\n",
    "#                 end=time.time()\n",
    "                \n",
    "                prob_y=F_u(np.array(bias['upper_mean'])[0],(np.dot(X[train_index],model.coef_.T)+model.intercept_).reshape(X[train_index].shape[0],))\n",
    "                \n",
    "                \n",
    "                prdict_y=np.round(prob_y)\n",
    "                print(classification_report(y[train_index],prdict_y))\n",
    "                rec.append(recall_score(y[train_index],prdict_y))\n",
    "                pre.append(precision_score(y[train_index],prdict_y))\n",
    "                G_m.append(np.sqrt(recall_score(y[train_index],prdict_y)*recall_score(y[train_index],prdict_y,pos_label=0)))\n",
    "                bacc.append(balanced_accuracy_score(y[train_index],prdict_y))\n",
    "                f2.append(fbeta_score(y[train_index],prdict_y,beta=max(2,np.log(y_sample.shape[0]/sum(y_sample)-1))))\n",
    "                T.append(t_1-s_1)\n",
    "    return G_m,bacc,f2,rec,pre,T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing performance of Volatility-uncertain method on five random trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal volatility by CV: [array([0.30299818, 2.27316259]) array([0.43553068, 2.46769799])]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.99      0.98     15200\n",
      "         1.0       0.71      0.68      0.69       800\n",
      "\n",
      "    accuracy                           0.97     16000\n",
      "   macro avg       0.85      0.83      0.84     16000\n",
      "weighted avg       0.97      0.97      0.97     16000\n",
      "\n",
      "optimal volatility by CV: [array([0.43553068, 2.46769799])]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.98      0.98     15200\n",
      "         1.0       0.64      0.70      0.67       800\n",
      "\n",
      "    accuracy                           0.97     16000\n",
      "   macro avg       0.81      0.84      0.82     16000\n",
      "weighted avg       0.97      0.97      0.97     16000\n",
      "\n",
      "optimal volatility by CV: [array([0.31025349, 2.49134363])]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.98      0.98      0.98     15200\n",
      "         1.0       0.61      0.71      0.66       800\n",
      "\n",
      "    accuracy                           0.96     16000\n",
      "   macro avg       0.80      0.84      0.82     16000\n",
      "weighted avg       0.97      0.96      0.96     16000\n",
      "\n",
      "optimal volatility by CV: [array([0.31025349, 2.49134363])]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.97      0.98     15200\n",
      "         1.0       0.60      0.75      0.66       800\n",
      "\n",
      "    accuracy                           0.96     16000\n",
      "   macro avg       0.79      0.86      0.82     16000\n",
      "weighted avg       0.97      0.96      0.96     16000\n",
      "\n",
      "optimal volatility by CV: [array([0.31025349, 2.49134363])]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.98      0.98     15200\n",
      "         1.0       0.63      0.72      0.67       800\n",
      "\n",
      "    accuracy                           0.96     16000\n",
      "   macro avg       0.81      0.85      0.83     16000\n",
      "weighted avg       0.97      0.96      0.97     16000\n",
      "\n",
      ">LR_volatility:: Average G-mean:0.835(0.011) \n",
      ">LR_volatility:: Average Balanced_Acc: 0.845(0.009) \n",
      ">LR_volatility:: Average Fbeta: 0.703(0.015)\n",
      ">LR_volatility:: Average Recall: 0.713(0.022)\n",
      ">LR_volatility:: Average Training time: 0.562(0.021)\n",
      ">LR_volatility:: Average Score: 0.773(0.014)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result_v = Fivetrails_var(X_sample,y_sample)\n",
    "# \n",
    "G_mean=result_v[0]\n",
    "Bacc=result_v[1]\n",
    "# summarize performance\n",
    "recall=result_v[3]\n",
    "Fbeta=result_v[2]\n",
    "T=result_v[-1]\n",
    "acc_s=np.mean(np.array(result_v)[:4,:],axis=0)\n",
    "\n",
    "print('>%s: Average G-mean:%.3f(%.3f) ' % ('LR_volatility:',np.mean(G_mean),np.std(G_mean)))\n",
    "print('>%s: Average Balanced_Acc: %.3f(%.3f) ' % ('LR_volatility:',np.mean(Bacc),np.std(Bacc)))\n",
    "print('>%s: Average Fbeta: %.3f(%.3f)' % ('LR_volatility:',np.mean(Fbeta),np.std(Fbeta)))\n",
    "print('>%s: Average Recall: %.3f(%.3f)' % ('LR_volatility:',np.mean(recall),np.std(recall)))    \n",
    "print('>%s: Average Training time: %.3f(%.3f)' % ('LR_volatility:',np.mean(T),np.std(T)))\n",
    "print('>%s: Average Score: %.3f(%.3f)' % ('LR_volatility:',np.mean(0.99*acc_s+0.01/(np.array(result_v)[-1,:]/10*9+1)),np.std(0.99*acc_s+0.01/(np.array(result_v)[-1,:]/10*9+1))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "编辑元数据",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
