{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "from sympy.solvers import solve\n",
    "from sympy import Symbol\n",
    "from scipy.optimize import fsolve\n",
    "import testjx\n",
    "import numpy as np\n",
    "import cvxopt\n",
    "import cv2\n",
    "import os \n",
    "import random\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "from cvxopt import matrix\n",
    "from cvxopt import solvers\n",
    "import face_recognition\n",
    "import sklearn.metrics as sm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# from scikitplot import plotters as skplt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from statsmodels.discrete.discrete_model import Logit, Probit, MNLogit\n",
    "from pylab import mpl\n",
    "\n",
    "import warningfrom sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib as mpl\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import skimage\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import preprocessing\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from os import listdir\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "from collections import Counter\n",
    "import imblearn\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "sn.set()\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC # SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "from xgboost import XGBClassifier # XGBClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score,f1_score,recall_score,cohen_kappa_score,precision_score\n",
    "from sklearn.utils import compute_class_weight\n",
    "from sklearn.preprocessing import MinMaxScaler,LabelBinarizer\n",
    "from sklearn.ensemble import AdaBoostClassifier # AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier # KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier # RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.compat.v1 as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.applications.vgg16 import VGG16 # VGG16\n",
    "from tensorflow.keras.applications.vgg19 import VGG19 # VGG19\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50 # ResNet50\n",
    "from tensorflow.keras.applications.xception import Xception # Xception\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet # MobileNet\n",
    "from tensorflow.keras.applications.nasnet import NASNetMobile # NASNetMobile\n",
    "from tensorflow.keras.applications.densenet import DenseNet169 # DenseNet169\n",
    "from tensorflow.keras.applications.densenet import DenseNet121 # DenseNet121\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2 # MobileNetV2\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3 # InceptionV3\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Flatten, Activation, GlobalAveragePooling2D,Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('/Users/lvjingzhe/Downloads/celebA/train/')\n",
    "train = []\n",
    "    \n",
    "for i in range(20000):\n",
    "# for i in range(20000):\n",
    "    img_ = cv2.imread('/Users/lvjingzhe/Downloads/celebA/train/%d.jpg' %(i))\n",
    "\n",
    "    train.append(img_)\n",
    "\n",
    "# files = os.listdir('/Users/lvjingzhe/Downloads/celebA/test/')\n",
    "# test = []\n",
    "    \n",
    "# for i in range(2000,2200):\n",
    "#     img_ = cv2.imread('/Users/lvjingzhe/Downloads/celebA/test/%d.jpg' %(i))\n",
    "\n",
    "#     test.append(img_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image=np.array(train)\n",
    "train_label=np.concatenate((np.array([0]*19000),np.array([1]*1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0127 09:47:16.641349 140736062219136 deprecation.py:506] From /Users/lvjingzhe/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time:255.947163105011\n"
     ]
    }
   ],
   "source": [
    "''' Feature extraction---VGG19 network'''\n",
    "\n",
    "\n",
    "import time\n",
    "start=time.time()\n",
    "base_model= VGG19( weights='imagenet', include_top=False,input_shape=(64,64,3))\n",
    "x = base_model.output\n",
    "# x = Dropout(0.2)(x)\n",
    "predictions = Flatten()(x)\n",
    "\n",
    "model_feat = Model(inputs=base_model.input,outputs=predictions)\n",
    "\n",
    "train_features = model_feat.predict(train_image/255)\n",
    "# test_features=model_feat.predict(test_image/255)\n",
    "end=time.time()\n",
    "print('running time:{}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 2048)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Shape of input length and feature dimension after VGG19'''\n",
    "train_features.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Divide into two sets: training set(x_tr,y_tr), and testing set (x_te,y_te)'''\n",
    "x_tr, x_te, y_tr, y_te = train_test_split(train_features,train_label,test_size = 0.2,\n",
    "                                                  shuffle = True,\n",
    "                                                  random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test=np.c_[x_te,np.ones(x_te.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Split the training set into 20 groups, one of which contains the toral glassed ones.\n",
    "This step is useful in max-mean loss method\n",
    " (x_tr,y_tr) becomes (pre_XX,pre_YY) after \"Split\", which is the input of max-mean loss method.'''\n",
    "def split(x):\n",
    "    n=y_tr[y_tr==1].shape[0]\n",
    "    m=int((X_train.shape[0]-n)/19)\n",
    "    indice=[k*m for k in range(1,19)]\n",
    "    indice.append(X_train.shape[0]-n)\n",
    "    x_c=np.concatenate((x[y_tr==0],x[y_tr==1]))\n",
    "    return np.array(np.split(x_c,indice))\n",
    "\n",
    "X_train=np.c_[x_tr,np.ones(x_tr.shape[0])]\n",
    "n=y_tr[y_tr==1].shape[0]\n",
    "XX=np.concatenate((X_train[y_tr==0],X_train[y_tr==1]))\n",
    "YY=np.concatenate((np.array([0]*(X_train.shape[0]-n)),np.array([1]*n)))\n",
    "m=int((X_train.shape[0]-n)/19)\n",
    "indice=[k*m for k in range(1,19)]\n",
    "indice.append(X_train.shape[0]-n)\n",
    "\n",
    "pre_XX=np.array(np.split(XX,indice))\n",
    "pre_YY=np.array(np.split(YY,indice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Split training set into 5 cross-validation sets'''\n",
    "cv= RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional classifiers：LR、CS_LR、AUSTBOOST、BAGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      3812\n",
      "           1       0.80      0.69      0.74       188\n",
      "\n",
      "    accuracy                           0.98      4000\n",
      "   macro avg       0.89      0.84      0.86      4000\n",
      "weighted avg       0.98      0.98      0.98      4000\n",
      "\n",
      "Running time:3.21 s\n",
      "Balanced-Accuracy on testing set：84.14%\n",
      "Recall on testing set：69.15%\n",
      "F-measure on testing set：70.12%\n"
     ]
    }
   ],
   "source": [
    "'''model_in symbols the result of traditional logistic regression'''\n",
    "\n",
    "import time\n",
    "start_i=time.time()\n",
    "model_in=LogisticRegression()\n",
    "\n",
    "model_in.fit(x_tr,y_tr)\n",
    "end_i=time.time()\n",
    "y_train_proba=model_in.predict_proba(x_tr)\n",
    "y_train_label=model_in.predict(x_tr)\n",
    "y_test_proba=model_in.predict_proba(x_te)\n",
    "\n",
    "y_test_label=model_in.predict(x_te)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_te,y_test_label))\n",
    "print(\"Running time:%.2f s\"%(end_i-start_i))\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_test_label)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_test_label)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_test_label,beta=2.94)))\n",
    "# print(classification_report(y_tr,y_train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      3812\n",
      "           1       0.98      0.48      0.65       188\n",
      "\n",
      "    accuracy                           0.98      4000\n",
      "   macro avg       0.98      0.74      0.82      4000\n",
      "weighted avg       0.98      0.98      0.97      4000\n",
      "\n",
      "Running time:181.00秒\n",
      "Balanced-Accuracy on testing set：74.18%\n",
      "Recall on testing set：48.40%\n",
      "F-measure on testing set：51.08%\n",
      "Score of SVM：0.573\n"
     ]
    }
   ],
   "source": [
    "##SVM\n",
    "from sklearn.metrics import fbeta_score\n",
    "import time\n",
    "start=time.time()\n",
    "model_svc=SVC(probability=True)\n",
    "\n",
    "model_svc.fit(x_tr,y_tr)\n",
    "end=time.time()\n",
    "y_train_proba=model_svc.predict_proba(x_tr)\n",
    "y_train_label=model_svc.predict(x_tr)\n",
    "y_test_proba=model_svc.predict_proba(x_te)\n",
    "\n",
    "y_test_label=model_svc.predict(x_te)\n",
    "# y_test_proba=model_in.predict_proba(image_test)\n",
    "# y_test_label=model_in.predict(image_test)\n",
    "bacc=balanced_accuracy_score(y_te,y_test_label)\n",
    "fbeta=fbeta_score(y_te,y_test_label,beta=2.94)\n",
    "recall=recall_score(y_te,y_test_label)\n",
    "# from sklearn.metrics import classification_report\n",
    "print(classification_report(y_te,y_test_label))\n",
    "print(\"Running time:%.2f秒\"%(end-start))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_test_label)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_test_label)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_test_label,beta=2.94)))\n",
    "print('Score of SVM：{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end-start)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''(X_resampled, y_resampled) is the training set after randomly resampling method'''\n",
    "from imblearn.over_sampling import RandomOverSampler,SVMSMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "\n",
    "X_resampled, y_resampled = rus.fit_resample(x_tr,y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.88      0.93      3812\n",
      "           1       0.27      0.90      0.42       188\n",
      "\n",
      "    accuracy                           0.88      4000\n",
      "   macro avg       0.63      0.89      0.68      4000\n",
      "weighted avg       0.96      0.88      0.91      4000\n",
      "\n",
      "Running time:6.73秒\n",
      "Balanced-Accuracy on testing set：89.31%\n",
      "Recall on testing set：90.43%\n",
      "F-measure on testing set：73.03%\n",
      "Score of RUS-SVM:0.836\n"
     ]
    }
   ],
   "source": [
    "##RUS-SVM\n",
    "import time\n",
    "start=time.time()\n",
    "model_svc_r=SVC(gamma='scale',probability=True)\n",
    "# model_in.fit(pt.fit_transform(train_image_),train_label_)\n",
    "model_svc_r.fit(X_resampled, y_resampled)\n",
    "end=time.time()\n",
    "y_train_proba=model_svc_r.predict_proba(x_tr)\n",
    "y_train_label=model_svc_r.predict(x_tr)\n",
    "y_test_proba=model_svc_r.predict_proba(x_te)\n",
    "# print('AUC:{}'.format(roc_auc_score(y_te,y_test_proba[:,-1])))\n",
    "y_test_label=model_svc_r.predict(x_te)\n",
    "bacc=balanced_accuracy_score(y_te,y_test_label)\n",
    "fbeta=fbeta_score(y_te,y_test_label,beta=2.94)\n",
    "recall=recall_score(y_te,y_test_label)\n",
    "print(classification_report(y_te,y_test_label))\n",
    "print(\"Running time:%.2f秒\"%(end-start))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_test_label)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_test_label)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_test_label,beta=2.94)))\n",
    "print('Score of RUS-SVM:{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end-start)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier :CS-SVM\n",
      "Running time:483.40秒\n",
      "Balanced-Accuracy on testing set：90.13%\n",
      "Recall on testing set：86.17%\n",
      "F-measure on testing set：77.65%\n",
      "Score of SVM:0.838\n"
     ]
    }
   ],
   "source": [
    "##SVM+cost-learning\n",
    "import time\n",
    "start=time.time()\n",
    "model_svc3=SVC(class_weight='balanced',probability=True)\n",
    "# model_in.fit(pt.fit_transform(train_image_),train_label_)\n",
    "# model_svc3.fit(X_resampled_smote,y_resampled_smote)\n",
    "model_svc3.fit(x_tr,y_tr)\n",
    "end=time.time()\n",
    "y_train_proba=model_svc3.predict_proba(x_tr)\n",
    "y_train_label=model_svc3.predict(x_tr)\n",
    "y_test_proba=model_svc3.predict_proba(x_te)\n",
    "\n",
    "y_test_label=model_svc3.predict(x_te)\n",
    "\n",
    "bacc=balanced_accuracy_score(y_te,y_test_label)\n",
    "fbeta=fbeta_score(y_te,y_test_label,beta=2.94)\n",
    "recall=recall_score(y_te,y_test_label)\n",
    "print('Classifier :CS-SVM')\n",
    "print(\"Running time:%.2f秒\"%(end-start))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_test_label)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_test_label)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_test_label,beta=2.94)))\n",
    "print('Score :{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end-start)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.89      0.94      3812\n",
      "           1       0.26      0.78      0.39       188\n",
      "\n",
      "    accuracy                           0.88      4000\n",
      "   macro avg       0.62      0.83      0.66      4000\n",
      "weighted avg       0.95      0.88      0.91      4000\n",
      "\n",
      "Classifier :RUS-Boost\n",
      "Running time:8.22 s\n",
      "Balanced-Accuracy on testing set：83.32%\n",
      "Recall on testing set：77.66%\n",
      "F-measure on testing set：64.26%\n",
      "Score:0.745\n"
     ]
    }
   ],
   "source": [
    "##RUSBoost\n",
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "rusb = RUSBoostClassifier(random_state=0)\n",
    "start=time.time()\n",
    "rusb.fit(x_tr,y_tr)\n",
    "end=time.time()\n",
    "y_train_proba=rusb.predict_proba(x_tr)\n",
    "y_train_label=rusb.predict(x_tr)\n",
    "y_test_proba=rusb.predict_proba(x_te)\n",
    "# print('AUC:{}'.format(roc_auc_score(y_te,y_test_proba[:,-1])))\n",
    "y_test_label=rusb.predict(x_te)\n",
    "# y_test_proba=model_in.predict_proba(image_test)\n",
    "# y_test_label=model_in.predict(image_test)\n",
    "bacc=balanced_accuracy_score(y_te,y_test_label)\n",
    "fbeta=fbeta_score(y_te,y_test_label,beta=2.94)\n",
    "recall=recall_score(y_te,y_test_label)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_te,y_test_label))\n",
    "print('Classifier :RUS-Boost')\n",
    "print(\"Running time:%.2f s\"%(end-start))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_test_label)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_test_label)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_test_label,beta=2.94)))\n",
    "print('Score:{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end-start)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier :MLP\n",
      "Running time:24.46 s\n",
      "Balanced-Accuracy on testing set：84.09%\n",
      "Recall on testing set：69.15%\n",
      "F-measure on testing set：69.96%\n",
      "Score:0.737\n"
     ]
    }
   ],
   "source": [
    "##MLP\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp=MLPClassifier(random_state=0, max_iter=200,hidden_layer_sizes=(50,))\n",
    "start=time.time()\n",
    "\n",
    "\n",
    "mlp.fit(x_tr,y_tr)\n",
    "end=time.time()\n",
    "y_train_proba=mlp.predict_proba(x_tr)\n",
    "y_train_label=mlp.predict(x_tr)\n",
    "y_test_proba=mlp.predict_proba(x_te)\n",
    "# print('AUC:{}'.format(roc_auc_score(y_te,y_test_proba[:,-1])))\n",
    "y_test_label=mlp.predict(x_te)\n",
    "# y_test_proba=model_in.predict_proba(image_test)\n",
    "# y_test_label=model_in.predict(image_test)\n",
    "bacc=balanced_accuracy_score(y_te,y_test_label)\n",
    "fbeta=fbeta_score(y_te,y_test_label,beta=2.94)\n",
    "recall=recall_score(y_te,y_test_label)\n",
    "print('Classifier :MLP')\n",
    "print(\"Running time:%.2f s\"%(end-start))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_test_label)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_test_label)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_test_label,beta=2.94)))\n",
    "print('Score:{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end-start)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X_resampled_smote,y_resampled_smote  = SMOTE().fit_resample(x_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      3812\n",
      "           1       0.73      0.73      0.73       188\n",
      "\n",
      "    accuracy                           0.97      4000\n",
      "   macro avg       0.86      0.86      0.86      4000\n",
      "weighted avg       0.97      0.97      0.97      4000\n",
      "\n",
      "Classifier :SMOTE-MLP\n",
      "Running time:16.76 s\n",
      "Balanced-Accuracy on testing set：86.05%\n",
      "Recall on testing set：73.40%\n",
      "F-measure on testing set：73.40%\n",
      "Score:0.769\n"
     ]
    }
   ],
   "source": [
    "##MLP+SMOTE\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp=MLPClassifier(random_state=0, max_iter=200,hidden_layer_sizes=(50,))\n",
    "start=time.time()\n",
    "\n",
    "\n",
    "mlp.fit(X_resampled_smote,y_resampled_smote)\n",
    "end=time.time()\n",
    "y_train_proba=mlp.predict_proba(x_tr)\n",
    "y_train_label=mlp.predict(x_tr)\n",
    "y_test_proba=mlp.predict_proba(x_te)\n",
    "\n",
    "y_test_label=mlp.predict(x_te)\n",
    "# y_test_proba=model_in.predict_proba(image_test)\n",
    "# y_test_label=model_in.predict(image_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_te,y_test_label))\n",
    "bacc=balanced_accuracy_score(y_te,y_test_label)\n",
    "fbeta=fbeta_score(y_te,y_test_label,beta=2.94)\n",
    "recall=recall_score(y_te,y_test_label)\n",
    "print('Classifier :SMOTE-MLP')\n",
    "print(\"Running time:%.2f s\"%(end-start))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_test_label)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_test_label)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_test_label,beta=2.94)))\n",
    "print('Score:{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end-start)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier :RUS-MLP\n",
      "Running time:3.10 s\n",
      "Balanced-Accuracy on testing set：90.13%\n",
      "Recall on testing set：92.02%\n",
      "F-measure on testing set：74.28%\n",
      "Score :0.849\n"
     ]
    }
   ],
   "source": [
    "##MLP+RUS\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp=MLPClassifier(random_state=10, max_iter=200,hidden_layer_sizes=(50,))\n",
    "start=time.time()\n",
    "\n",
    "\n",
    "mlp.fit(X_resampled,y_resampled)\n",
    "end=time.time()\n",
    "y_train_proba=mlp.predict_proba(x_tr)\n",
    "y_train_label=mlp.predict(x_tr)\n",
    "y_test_proba=mlp.predict_proba(x_te)\n",
    "\n",
    "y_test_label=mlp.predict(x_te)\n",
    "# y_test_proba=model_in.predict_proba(image_test)\n",
    "# y_test_label=model_in.predict(image_test)\n",
    "\n",
    "bacc=balanced_accuracy_score(y_te,y_test_label)\n",
    "fbeta=fbeta_score(y_te,y_test_label,beta=2.94)\n",
    "recall=recall_score(y_te,y_test_label)\n",
    "print('Classifier :RUS-MLP')\n",
    "print(\"Running time:%.2f s\"%(end-start))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_test_label)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_test_label)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_test_label,beta=2.94)))\n",
    "print('Score :{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end-start)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:0.6801505882878257\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      3812\n",
      "           1       0.37      0.39      0.38       188\n",
      "\n",
      "    accuracy                           0.94      4000\n",
      "   macro avg       0.67      0.68      0.67      4000\n",
      "weighted avg       0.94      0.94      0.94      4000\n",
      "\n",
      "Classifier :CS-Cart\n",
      "Running time:15.60 s\n",
      "Balanced-Accuracy on testing set：68.02%\n",
      "Recall on testing set：39.36%\n",
      "F-measure on testing set：39.08%\n",
      "Score of CS-Cart:0.484\n"
     ]
    }
   ],
   "source": [
    "##CART+cost-learing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(class_weight='balanced',random_state=0)\n",
    "start=time.time()\n",
    "\n",
    "\n",
    "clf.fit(x_tr,y_tr)\n",
    "end=time.time()\n",
    "y_train_proba=clf.predict_proba(x_tr)\n",
    "y_train_label=clf.predict(x_tr)\n",
    "y_test_proba=clf.predict_proba(x_te)\n",
    "print('AUC:{}'.format(roc_auc_score(y_te,y_test_proba[:,-1])))\n",
    "y_test_label=clf.predict(x_te)\n",
    "# y_test_proba=model_in.predict_proba(image_test)\n",
    "# y_test_label=model_in.predict(image_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_te,y_test_label))\n",
    "bacc=balanced_accuracy_score(y_te,y_test_label)\n",
    "fbeta=fbeta_score(y_te,y_test_label,beta=2.94)\n",
    "recall=recall_score(y_te,y_test_label)\n",
    "print('Classifier :CS-Cart')\n",
    "print(\"Running time:%.2f s\"%(end-start))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_test_label)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_test_label)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_test_label,beta=2.94)))\n",
    "print('Score of CS-Cart:{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end-start)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:0.7087277578084883\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96      3812\n",
      "           1       0.31      0.47      0.38       188\n",
      "\n",
      "    accuracy                           0.93      4000\n",
      "   macro avg       0.64      0.71      0.67      4000\n",
      "weighted avg       0.94      0.93      0.93      4000\n",
      "\n",
      "Classifier :SMOTE-Cart\n",
      "Running time:110.93 s\n",
      "Balanced-Accuracy on testing set：70.87%\n",
      "Recall on testing set：46.81%\n",
      "F-measure on testing set：44.52%\n",
      "Score :0.535\n"
     ]
    }
   ],
   "source": [
    "##CART+SMOTE\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "start=time.time()\n",
    "\n",
    "\n",
    "clf.fit(X_resampled_smote,y_resampled_smote)\n",
    "end=time.time()\n",
    "y_train_proba=clf.predict_proba(x_tr)\n",
    "y_train_label=clf.predict(x_tr)\n",
    "y_test_proba=clf.predict_proba(x_te)\n",
    "print('AUC:{}'.format(roc_auc_score(y_te,y_test_proba[:,-1])))\n",
    "y_test_label=clf.predict(x_te)\n",
    "# y_test_proba=model_in.predict_proba(image_test)\n",
    "# y_test_label=model_in.predict(image_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_te,y_test_label))\n",
    "bacc=balanced_accuracy_score(y_te,y_test_label)\n",
    "fbeta=fbeta_score(y_te,y_test_label,beta=2.94)\n",
    "recall=recall_score(y_te,y_test_label)\n",
    "print('Classifier :SMOTE-Cart')\n",
    "print(\"Running time:%.2f s\"%(end-start))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_test_label)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_test_label)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_test_label,beta=2.94)))\n",
    "print('Score :{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end-start)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:0.7916210845928869\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.77      0.87      3812\n",
      "           1       0.15      0.81      0.25       188\n",
      "\n",
      "    accuracy                           0.77      4000\n",
      "   macro avg       0.57      0.79      0.56      4000\n",
      "weighted avg       0.95      0.77      0.84      4000\n",
      "\n",
      "Classifier :RUS-Cart\n",
      "Running time:1.25秒\n",
      "Balanced-Accuracy on testing set：79.16%\n",
      "Recall on testing set：81.38%\n",
      "F-measure on testing set：55.53%\n",
      "Score of RUS-Cart:0.721\n"
     ]
    }
   ],
   "source": [
    "##CART+RUS\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "start=time.time()\n",
    "\n",
    "\n",
    "clf.fit(X_resampled,y_resampled)\n",
    "end=time.time()\n",
    "y_train_proba=clf.predict_proba(x_tr)\n",
    "y_train_label=clf.predict(x_tr)\n",
    "y_test_proba=clf.predict_proba(x_te)\n",
    "print('AUC:{}'.format(roc_auc_score(y_te,y_test_proba[:,-1])))\n",
    "y_test_label=clf.predict(x_te)\n",
    "# y_test_proba=model_in.predict_proba(image_test)\n",
    "# y_test_label=model_in.predict(image_test)\n",
    "bacc=balanced_accuracy_score(y_te,y_test_label)\n",
    "fbeta=fbeta_score(y_te,y_test_label,beta=2.94)\n",
    "recall=recall_score(y_te,y_test_label)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_te,y_test_label))\n",
    "print('Classifier :RUS-Cart')\n",
    "print(\"Running time:%.2f秒\"%(end-start))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_test_label)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_test_label)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_test_label,beta=2.94)))\n",
    "print('Score of RUS-Cart:{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end-start)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1005,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7290672131147541"
      ]
     },
     "execution_count": 1005,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.555+0.792+0.81)*0.3+0.1/1.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imbalanced_ensemble.ensemble import SMOTEBoostClassifier\n",
    "smb = SMOTEBoostClassifier(random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "smote_tomek = SMOTETomek(random_state=42)\n",
    "X_resampled, y_resampled = smote_tomek.fit_resample(x_tr,y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:0.918534136322029\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97      3812\n",
      "           1       0.46      0.63      0.53       188\n",
      "\n",
      "    accuracy                           0.95      4000\n",
      "   macro avg       0.72      0.80      0.75      4000\n",
      "weighted avg       0.96      0.95      0.95      4000\n",
      "\n",
      "Classifier :SMOTE-Boost\n",
      "Running time:234.80秒\n",
      "Balanced-Accuracy on testing set：79.81%\n",
      "Recall on testing set：63.30%\n",
      "F-measure on testing set：60.91%\n",
      "Score :0.673\n"
     ]
    }
   ],
   "source": [
    "##SMOTEBoost\n",
    "start=time.time()\n",
    "smb.fit(x_tr,y_tr)\n",
    "end=time.time()\n",
    "y_train_proba=smb.predict_proba(x_tr)\n",
    "y_train_label=smb.predict(x_tr)\n",
    "y_test_proba=smb.predict_proba(x_te)\n",
    "print('AUC:{}'.format(roc_auc_score(y_te,y_test_proba[:,-1])))\n",
    "y_test_label=smb.predict(x_te)\n",
    "# y_test_proba=model_in.predict_proba(image_test)\n",
    "# y_test_label=model_in.predict(image_test)\n",
    "bacc=balanced_accuracy_score(y_te,y_test_label)\n",
    "fbeta=fbeta_score(y_te,y_test_label,beta=2.94)\n",
    "recall=recall_score(y_te,y_test_label)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_te,y_test_label))\n",
    "print('Classifier :SMOTE-Boost')\n",
    "print(\"Running time:%.2f秒\"%(end-start))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_test_label)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_test_label)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_test_label,beta=2.94)))\n",
    "print('Score :{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end-start)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:0.9448515047665825\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.85      0.92      3812\n",
      "           1       0.23      0.90      0.37       188\n",
      "\n",
      "    accuracy                           0.86      4000\n",
      "   macro avg       0.61      0.88      0.64      4000\n",
      "weighted avg       0.96      0.86      0.89      4000\n",
      "\n",
      "Classifier :RUS-Boost\n",
      "Running time:6.59秒\n",
      "Balanced-Accuracy on testing set：87.67%\n",
      "Recall on testing set：89.89%\n",
      "F-measure on testing set：69.38%\n",
      "Score :0.816\n"
     ]
    }
   ],
   "source": [
    "adb=AdaBoostClassifier(random_state=0)\n",
    "start=time.time()\n",
    "adb.fit(X_resampled, y_resampled )\n",
    "end=time.time()\n",
    "y_train_proba=adb.predict_proba(x_tr)\n",
    "y_train_label=adb.predict(x_tr)\n",
    "y_test_proba=adb.predict_proba(x_te)\n",
    "print('AUC:{}'.format(roc_auc_score(y_te,y_test_proba[:,-1])))\n",
    "y_test_label=adb.predict(x_te)\n",
    "# y_test_proba=model_in.predict_proba(image_test)\n",
    "# y_test_label=model_in.predict(image_test)\n",
    "bacc=balanced_accuracy_score(y_te,y_test_label)\n",
    "fbeta=fbeta_score(y_te,y_test_label,beta=2.94)\n",
    "recall=recall_score(y_te,y_test_label)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_te,y_test_label))\n",
    "print('Classifier :RUS-Boost')\n",
    "print(\"Running time:%.2f秒\"%(end-start))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_test_label)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_test_label)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_test_label,beta=2.94)))\n",
    "print('Score :{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end-start)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      3812\n",
      "           1       0.73      0.55      0.63       188\n",
      "\n",
      "    accuracy                           0.97      4000\n",
      "   macro avg       0.85      0.77      0.81      4000\n",
      "weighted avg       0.97      0.97      0.97      4000\n",
      "\n",
      "Classifier :Adaboost\n",
      "Running time:81.01秒\n",
      "Balanced-Accuracy on testing set：76.90%\n",
      "Recall on testing set：54.79%\n",
      "F-measure on testing set：56.25%\n",
      "Score:0.62\n"
     ]
    }
   ],
   "source": [
    "##ADABoost\n",
    "adb=AdaBoostClassifier(random_state=0)\n",
    "start=time.time()\n",
    "adb.fit(x_tr,y_tr)\n",
    "end=time.time()\n",
    "y_train_proba=adb.predict_proba(x_tr)\n",
    "y_train_label=adb.predict(x_tr)\n",
    "y_test_proba=adb.predict_proba(x_te)\n",
    "\n",
    "y_test_label=adb.predict(x_te)\n",
    "# y_test_proba=model_in.predict_proba(image_test)\n",
    "# y_test_label=model_in.predict(image_test)\n",
    "bacc=balanced_accuracy_score(y_te,y_test_label)\n",
    "fbeta=fbeta_score(y_te,y_test_label,beta=2.94)\n",
    "recall=recall_score(y_te,y_test_label)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_te,y_test_label))\n",
    "\n",
    "print('Classifier :Adaboost')\n",
    "print(\"Running time:%.2f秒\"%(end-start))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_test_label)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_test_label)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_test_label,beta=2.94)))\n",
    "print('Score:{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end-start)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:0.872678104976446\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      3812\n",
      "           1       0.84      0.44      0.58       188\n",
      "\n",
      "    accuracy                           0.97      4000\n",
      "   macro avg       0.91      0.72      0.78      4000\n",
      "weighted avg       0.97      0.97      0.97      4000\n",
      "\n",
      "Classifier :Bagging\n",
      "Running time:285.68秒\n",
      "Balanced-Accuracy on testing set：71.86%\n",
      "Recall on testing set：44.15%\n",
      "F-measure on testing set：46.43%\n",
      "Score:0.536\n"
     ]
    }
   ],
   "source": [
    "##BAgging\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "bag = BaggingClassifier(\n",
    "                                random_state=0)\n",
    "\n",
    "\n",
    "start=time.time()\n",
    "\n",
    "\n",
    "bag.fit(x_tr,y_tr)\n",
    "end=time.time()\n",
    "y_train_proba=bag.predict_proba(x_tr)\n",
    "y_train_label=bag.predict(x_tr)\n",
    "y_test_proba=bag.predict_proba(x_te)\n",
    "print('AUC:{}'.format(roc_auc_score(y_te,y_test_proba[:,-1])))\n",
    "y_test_label=bag.predict(x_te)\n",
    "# y_test_proba=model_in.predict_proba(image_test)\n",
    "# y_test_label=model_in.predict(image_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_te,y_test_label))\n",
    "bacc=balanced_accuracy_score(y_te,y_test_label)\n",
    "fbeta=fbeta_score(y_te,y_test_label,beta=2.94)\n",
    "recall=recall_score(y_te,y_test_label)\n",
    "print('Classifier :Bagging')\n",
    "print(\"Running time:%.2f秒\"%(end-start))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_test_label)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_test_label)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_test_label,beta=2.94)))\n",
    "print('Score:{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end-start)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.86      0.92      3812\n",
      "           1       0.23      0.85      0.36       188\n",
      "\n",
      "    accuracy                           0.86      4000\n",
      "   macro avg       0.61      0.85      0.64      4000\n",
      "weighted avg       0.96      0.86      0.90      4000\n",
      "\n",
      "Classifier :RUS-Bag\n",
      "Running time:8.17秒\n",
      "Balanced-Accuracy on testing set：85.40%\n",
      "Recall on testing set：84.57%\n",
      "F-measure on testing set：66.41%\n",
      "Score:0.781\n"
     ]
    }
   ],
   "source": [
    "##Bagging+RUS\n",
    "bag = BaggingClassifier(\n",
    "                                random_state=0)\n",
    "\n",
    "\n",
    "start=time.time()\n",
    "\n",
    "\n",
    "bag.fit(X_resampled,y_resampled)\n",
    "end=time.time()\n",
    "y_train_proba=bag.predict_proba(x_tr)\n",
    "y_train_label=bag.predict(x_tr)\n",
    "y_test_proba=bag.predict_proba(x_te)\n",
    "\n",
    "y_test_label=bag.predict(x_te)\n",
    "# y_test_proba=model_in.predict_proba(image_test)\n",
    "# y_test_label=model_in.predict(image_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_te,y_test_label))\n",
    "bacc=balanced_accuracy_score(y_te,y_test_label)\n",
    "fbeta=fbeta_score(y_te,y_test_label,beta=2.94)\n",
    "recall=recall_score(y_te,y_test_label)\n",
    "print('Classifier :RUS-Bag')\n",
    "print(\"Running time:%.2f秒\"%(end-start))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_test_label)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_test_label)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_test_label,beta=2.94)))\n",
    "print('Score:{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end-start)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(799,)"
      ]
     },
     "execution_count": 821,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split(err)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Class solution_XX() is the algorithm based on Xiaohua,Xuan[2019], the improvement is that\n",
    "we add a parameter of \"penalty\".'''\n",
    "def split(x):\n",
    "    n=y_tr[y_tr==1].shape[0]\n",
    "    m=int((X_train.shape[0]-n)/19)\n",
    "    indice=[k*m for k in range(1,19)]\n",
    "    indice.append(X_train.shape[0]-n)\n",
    "    x_c=np.concatenate((x[y_tr==0],x[y_tr==1]))\n",
    "    return np.array(np.split(x_c,indice))\n",
    "\n",
    "class solution_XX:\n",
    "    \n",
    "    \n",
    "    def __init__(self,penalty = None,Lambda = 0.03,a = 0.5,epochs = 200):\n",
    "        self.W = None\n",
    "        self.penalty = penalty\n",
    "        self.Lambda = Lambda\n",
    "        self.a = a\n",
    "        self.epochs =epochs\n",
    "        self.sigmoid = lambda x:1/(1 + np.exp(-x))\n",
    "        \n",
    "#     err=(y_tr-s.sigmoid(np.dot(X_train,w)))**2\n",
    "#     err[index]=2.6*err[index]\n",
    "\n",
    "    def f_XX(self,X,Y):\n",
    "        if self.penalty=='l1':f=np.array([np.mean((self.sigmoid(np.dot(x,self.W))-y)**2 )+self.Lambda*np.sum(np.abs(self.W)) for x,y in zip(X,Y)])\n",
    "        elif self.penalty=='l2':f=np.array([np.mean((self.sigmoid(np.dot(x,self.W))-y)**2 )+self.Lambda*np.sum(self.W**2) for x,y in zip(X,Y)])#pre_Xtrain,pre_Ytrain\n",
    "        else:f=np.array([np.mean((self.sigmoid(np.dot(x,self.W))-y)**2 ) for x,y in zip(X,Y)])\n",
    "\n",
    "        return f         \n",
    "    def Gf_XX(self,X,Y):#To compute the Derivative matrix, the shape of which is N*2\n",
    "        if self.penalty=='l1':d=np.array([x.T.dot(0.02*(self.sigmoid(np.dot(x,self.W))-y)*self.sigmoid(np.dot(x,self.W))*(1-self.sigmoid(np.dot(x,self.W))))+self.Lambda*np.sign(self.W )for x,y in zip(X,Y)])\n",
    "        elif self.penalty=='l2':\n",
    "            d=np.array([x.T.dot(0.02*(self.sigmoid(np.dot(x,self.W))-y)*self.sigmoid(np.dot(x,self.W))*(1-self.sigmoid(np.dot(x,self.W))))+2*self.Lambda*self.W for x,y in zip(X,Y)])\n",
    "#     return d.reshape(20,12289)\n",
    "        else:d=np.array([x.T.dot(0.02*(self.sigmoid(np.dot(x,self.W))-y)*self.sigmoid(np.dot(x,self.W))*(1-self.sigmoid(np.dot(x,self.W)))) for x,y in zip(X,Y)])\n",
    "        return d\n",
    "    def direction_XX(self,X,Y):\n",
    "        gra=self.Gf_XX(X,Y)\n",
    "        p=matrix(gra.dot(gra.T),tc='d')\n",
    "        q=matrix(-self.f_XX(X,Y),tc='d')\n",
    "        G=matrix(np.diag(np.array([-1]*20)),tc='d')#N=20\n",
    "        h=matrix(np.array([[0]]*20),tc='d')\n",
    "        A=matrix([[1.0]]*20)\n",
    "        b=matrix([1.0])\n",
    "        solvers.options['show_progress'] = False\n",
    "        sol = solvers.qp(p,q,G,h,A,b)\n",
    "        t=np.array(sol['x'])\n",
    "        d= -(gra.T.dot(t))\n",
    "        return d.reshape((X_train.shape[-1],))\n",
    "\n",
    "    def fit(self,X,Y):\n",
    "        \n",
    "        call=[]\n",
    "        pre=[]\n",
    "        loss=[]\n",
    "        testloss=[]\n",
    "        np.random.seed(1324)\n",
    "        self.W=np.random.random((X_train.shape[-1],))*2-1\n",
    "#         self.W=w_2\n",
    "        n=y_te[y_te==1.].shape[0]\n",
    "        for k in range(200):\n",
    "    #     while np.linalg.norm(d)//10**(-8) >= 10:\n",
    "            d=self.direction_XX(X,Y)\n",
    "#             print(np.linalg.norm(d))\n",
    "            if np.linalg.norm(d)//10**(-7) < 25:\n",
    "                break\n",
    "            sigma=0.8\n",
    "            f_1=np.max(self.f_XX(X,Y))\n",
    "            w=self.W\n",
    "            self.W=d*sigma+w\n",
    "            while np.max(self.f_XX(X,Y))>np.max(f_1):\n",
    "                sigma=sigma*0.8\n",
    "                self.W=d*sigma+w\n",
    "            self.W=d*sigma+w\n",
    "\n",
    "            \n",
    "    #         output=output.reshape(4000,)\n",
    "    #         pt=max(output[output==1.].shape[0],1)\n",
    "    #         m=0\n",
    "    #         for j in range(4000):\n",
    "    #             if output[j]==test_label[j]==1:\n",
    "    #                 m+=1\n",
    "\n",
    "\n",
    "    # # b1=np.random.random((1,10))*2-1\n",
    "\n",
    "\n",
    "            \n",
    "    #         call.append(m/n)\n",
    "    #         pre.append(m/pt)\n",
    "    #         loss.append(max(f_X(w)))\n",
    "    #         testloss.append(ff_X(w))\n",
    "        return self.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier :L2 max-mean loss\n",
      "Running time:20.98 s\n",
      "Balanced-Accuracy on testing set：89.45%\n",
      "Recall on testing set：85.64%\n",
      "F-measure on testing set：76.00%\n",
      "Score:0.829\n"
     ]
    }
   ],
   "source": [
    "\n",
    "s_2=solution_XX(penalty='l2')\n",
    "start_x=time.time()\n",
    "w_2 = s_2.fit(pre_XX,pre_YY)'''w_2 is the estimator obtained by max-mean loss method+L2 regulation'''\n",
    "end_x=time.time()\n",
    "y_pred=s_2.sigmoid(X_test.dot(w_2))\n",
    "\n",
    "y_train_pred=s_2.sigmoid(X_train.dot(w_2))\n",
    "y_train_pred[y_train_pred>=0.5]=1\n",
    "y_train_pred[y_train_pred<0.5]=0\n",
    "y_pred[y_pred>=0.5]=1\n",
    "y_pred[y_pred<0.5]=0\n",
    "\n",
    "bacc=balanced_accuracy_score(y_te,y_pred)\n",
    "fbeta=fbeta_score(y_te,y_pred,beta=2.94)\n",
    "recall=recall_score(y_te,y_pred)\n",
    "print('Classifier :L2 max-mean loss')\n",
    "print(\"Running time:%.2f s\"%(end_x-start_x))\n",
    "\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_pred)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_pred)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_pred,beta=2.94)))\n",
    "print('Score:{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end_x-start_x)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier : max-mean loss\n",
      "Running time:13.82\n",
      "Balanced-Accuracy on testing set：88.20%\n",
      "Recall on testing set：83.51%\n",
      "F-measure on testing set：73.75%\n",
      "Score:0.811\n"
     ]
    }
   ],
   "source": [
    "s=solution_XX()\n",
    "start=time.time()\n",
    "w = s.fit(pre_XX,pre_YY)\n",
    "end=time.time()\n",
    "y_pred=s.sigmoid(X_test.dot(w))\n",
    "\n",
    "y_train_pred=s.sigmoid(X_train.dot(w))\n",
    "y_train_pred[y_train_pred>=0.5]=1\n",
    "y_train_pred[y_train_pred<0.5]=0\n",
    "y_pred[y_pred>=0.5]=1\n",
    "y_pred[y_pred<0.5]=0\n",
    "# # C_x.append(precision_score(y_pred,train_label_[test_index]))\n",
    "bacc=balanced_accuracy_score(y_te,y_pred)\n",
    "fbeta=fbeta_score(y_te,y_pred,beta=2.94)\n",
    "recall=recall_score(y_te,y_pred)\n",
    "print('Classifier : max-mean loss')\n",
    "print(\"Running time:%.2f\"%(end-start))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_pred)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_pred)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_pred,beta=2.94)))\n",
    "print('Score:{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end-start)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean_uncertain method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_(x,u):\n",
    "    return 1.0/(1.0+np.exp(-x-u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_in=(np.dot(x_tr,model_in.coef_.T)+model_in.intercept_).reshape(x_tr.shape[0],)\n",
    "## pre_in is the product of training set and traditional estimator obtained by LR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanuncertainty(x,n):\n",
    "    r=[]\n",
    "    for i in range(0,len(x)+1-n,n//5):\n",
    "        r.append(np.mean(x[i:i+n]))\n",
    "    return min(r),max(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equa_xh(x,n):\n",
    "    pre_in=np.dot(X_train,w_2).reshape(x_tr.shape[0],)\n",
    "    ini_err=y_tr-sigmoid_(pre_in,x)##ini_err is the predicted err of training set, based on max-mean loss.\n",
    "    err_w=shuffle(np.concatenate((np.random.choice(ini_err[y_tr==0],ini_err[y_tr==1].shape[0]),ini_err[y_tr==1])))\n",
    "    for k in range(ini_err.shape[0]):\n",
    "    \n",
    "        if y_tr[k]==1:\n",
    "            ini_err[k]=10*ini_err[k]\n",
    "\n",
    "    \n",
    "    return meanuncertainty(ini_err,n)[1]## the return is the max-mean err\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equa_m(x,n):\n",
    "    \n",
    "    ini_err=y_tr-sigmoid_(pre_in,x)##ini_err is the predicted err of training set, based on LR\n",
    "\n",
    "    for k in range(ini_err.shape[0]):\n",
    "    \n",
    "        if y_tr[k]==1:\n",
    "            ini_err[k]=10*ini_err[k]\n",
    "\n",
    "    \n",
    "    return meanuncertainty(ini_err,n)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' each meamber of meanxh represents the upper mean of max-mean loss with respect to fixed window size N. '''\n",
    "meanxh_1=np.array([fsolve(lambda x:equa_xh(x,n),0.5 ) for n in range(10,1000,10)])\n",
    "# meanxh_2=np.array([fsolve(lambda x:equa_xh(x,n),0.5 ) for n in range(1000,10000,100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(meanxh_1,index=np.array(range(10,1000,10))).to_csv('glass_upper_mean.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.375096082687378\n"
     ]
    }
   ],
   "source": [
    "'''each meamber of mean represents the upper mean of LR with respect to fixed window size N.'''\n",
    "start=time.time()\n",
    "mean_1=np.array([fsolve(lambda x:equa_m(x,n),0.5 ) for n in range(10,1000,10)])\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "# mean_2=np.array([fsolve(lambda x:equa_m(x,n),0.5 ) for n in range(1000,10000,100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_lr=np.concatenate((mean_1,mean_2))[:-20].reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 966,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.692307692307693"
      ]
     },
     "execution_count": 966,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12800/650"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice optimal upper mean on  Cross-validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"We use 'evaluate_modelxh' to obtain the average performance in 5-fold cross-validation set, for fixed upper mean 'm' \"\"\"\n",
    "\n",
    "metrics_names=['Balanced_acc','F2_score','Recall','Precision']\n",
    "def evaluate_modelxh(X,y,m,model=solution_XX()):\n",
    "    \n",
    "    bacc,f2,rec,pre=list(),list(),list(),list()\n",
    "   \n",
    "\n",
    "    for train_index,test_index in cv.split(X,y):\n",
    "        \n",
    "#         N=X[train_index].shape[0]\n",
    "#         n=y[train_index][y[train_index]==1].shape[0]\n",
    "#         c_1=np.concatenate((X[train_index][y[train_index]==0],X[train_index][y[train_index]==1]))\n",
    "#         l_1=np.concatenate((np.array([0]*(N-n)),np.array([1]*n)))\n",
    "#         M=int((N-n)/19)/\n",
    "#         indice=[k*M for k in range(1,19)]\n",
    "#         indice.append(N-n)\n",
    "#         pre_cv=np.array(np.split(c_1,indice))\n",
    "#         pre_y=np.array(np.split(l_1,indice))\n",
    "#         start=time.time()\n",
    "#         w= model.fit(pre_cv,pre_y)\n",
    "#         end=time.time()\n",
    "#         print(end-start)\n",
    "#         w_x.append(w)\n",
    "        prob_y=sigmoid_(X_train[test_index].dot(w_2),m)\n",
    "        \n",
    "        prdict_y=np.round(prob_y)\n",
    "        rec.append(recall_score(y[test_index],prdict_y))\n",
    "        pre.append(precision_score(y[test_index],prdict_y))\n",
    "\n",
    "        bacc.append(balanced_accuracy_score(y[test_index],prdict_y))\n",
    "        f2.append(fbeta_score(y[test_index],prdict_y,beta=2.94))\n",
    " \n",
    "    return bacc,f2,rec,pre\n",
    "    \n",
    "#                 X_resampled_smote,y_resampled_smote=SMOTE().fit_resample(X[train_index], y[train_index])\n",
    "           \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_resultsxh = []\n",
    "\n",
    "for k in meanxh_1:\n",
    "    result=np.mean(evaluate_modelxh(X_train,y_tr,k),axis=1)\n",
    "    metric_res = {'upper_mean': k}\n",
    "    for name, value in zip(metrics_names, result):\n",
    "#             print(name, ': ', value)\n",
    "            metric_res[name] = value\n",
    "       \n",
    "\n",
    "    all_resultsxh.append(metric_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''We compare the performance of different upper mean obtained above, the result is saved in .csv file.'''\n",
    "\n",
    "all_resultsxh_w = []\n",
    "\n",
    "for k in meanxh_1:\n",
    "    result=np.mean(evaluate_modelxh(X_train,y_tr,k),axis=1)\n",
    "    metric_res = {'upper_mean': k}\n",
    "    for name, value in zip(metrics_names, result):\n",
    "#             print(name, ': ', value)\n",
    "            metric_res[name] = value\n",
    "       \n",
    "\n",
    "    all_resultsxh_w.append(metric_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(all_resultsxh_w).to_csv('/Users/lvjingzhe/Desktop/璇/modified_althogram/code/CV_select_mu_MM_gla_simple.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metrics_names=['Balanced_acc','F2_score','AUC','Recall','Precision']\n",
    "def evaluate_model(X,y,m,model=LogisticRegression()):\n",
    "    \n",
    "    bacc,f2,auc,rec,pre=list(),list(),list(),list(),list()\n",
    "   \n",
    "#     cv = RepeatedStratifiedKFold(n_splits=10,n_repeats=1, random_state=1)\n",
    "    for train_index,test_index in cv.split(X,y):\n",
    "#                 X_resampled_smote,y_resampled_smote=SMOTE().fit_resample(X[train_index], y[train_index])\n",
    "#                 model.fit(X[train_index],y[train_index])\n",
    "                prob_y=sigmoid_((np.dot(X[test_index],model_in.coef_.T)+model_in.intercept_),m)\n",
    "                auc.append(roc_auc_score(y[test_index],prob_y))\n",
    "                \n",
    "                prdict_y=np.round(prob_y)\n",
    "                rec.append(recall_score(y[test_index],prdict_y))\n",
    "                pre.append(precision_score(y[test_index],prdict_y))\n",
    "                \n",
    "                bacc.append(balanced_accuracy_score(y[test_index],prdict_y))\n",
    "                f2.append(fbeta_score(y[test_index],prdict_y,beta=2.94))\n",
    " \n",
    "    return bacc,f2,auc,rec,pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3187.276778936386"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "all_results = []\n",
    "c=1\n",
    "# for k in mean_lr:\n",
    "strat=time.time()\n",
    "for k in mean_1.reshape(-1):\n",
    "    result=np.mean(evaluate_model(x_tr,y_tr,k),axis=1)\n",
    "    metric_res = {'window':10*c,'upper_mean': k}\n",
    "    c+=1\n",
    "    for name, value in zip(metrics_names, result):\n",
    "#             print(name, ': ', value)\n",
    "            metric_res[name] = value\n",
    "       \n",
    "\n",
    "    all_results.append(metric_res)\n",
    "end=time.time()\n",
    "end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.DataFrame(all_results).to_csv('CV_which_mu_to_set_inLRBIAS.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volatility-uncertain method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''F_u and F_L compute the maximal and minimal probability of Y=1 '''\n",
    " \n",
    "def F_u(x,arr):\n",
    "    p=[]\n",
    "    for c in arr:\n",
    "        \n",
    "        if c <0:\n",
    "            p.append(2*x[1]*st.norm.cdf(c/x[1])/(x[0]+x[1]))\n",
    "        else:\n",
    "            p.append(1-2*x[0]*st.norm.cdf(-c/x[0])/(x[0]+x[1]))\n",
    "    return np.array(p)\n",
    "def F_L(x,arr):\n",
    "    p=[]\n",
    "    for c in arr:\n",
    "        \n",
    "        if c <0:\n",
    "            p.append(2*x[0]*st.norm.cdf(c/x[0])/(x[0]+x[1]))\n",
    "        else:\n",
    "            p.append(1-2*x[1]*st.norm.cdf(-c/x[1])/(x[0]+x[1]))\n",
    "    return np.array(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''equa_v returns the maximal and minimal err obtained by LR for fixed window size n'''\n",
    "\n",
    "def equa_v(x,n):\n",
    " \n",
    "    \n",
    "    err_u=y_tr-F_u(x,pre_in)\n",
    "    err_L=y_tr-F_L(x,pre_in)\n",
    "#     for k in range(y_tr.shape[0]):\n",
    "    \n",
    "#         if y_tr[k]==1:\n",
    "#             err_u[k]=2*err_u[k]\n",
    "#             err_L[k]=19*err_L[k]\n",
    "    \n",
    "    return np.array([meanuncertainty(err_u,n)[1],meanuncertainty(err_L,n)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''equa_vxh returns the maximal and minimal err obtained by max-mean loss for fixed window size n'''\n",
    "\n",
    "def equa_vxh(x,n):\n",
    "    pre_in=np.dot(X_train,w_2).reshape(x_tr.shape[0],)\n",
    "    err_u=y_tr-F_u(x,pre_in)\n",
    "    err_L=y_tr-F_L(x,pre_in)\n",
    "#     for k in range(y_tr.shape[0]):\n",
    "    \n",
    "#         if y_tr[k]==1:\n",
    "#             err_u[k]=2*err_u[k]\n",
    "#             err_L[k]=19*err_L[k]\n",
    "    \n",
    "    return np.array([meanuncertainty(err_u,n)[1],meanuncertainty(err_L,n)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.93177559, 2.08727278])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fsolve(lambda x:equa_v(x,500),[0.5,1.5] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1094,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "var_2=np.array([\n",
    "    for n in range(500,1000,50)])\n",
    "var_3=np.array([fsolve(lambda x:equa_vxh(x,n),[0.5,1.5] ) for n in range(1000,6000,100)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1096,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''var_xh is the volatility uncertainty with respect to different windowsize n.'''\n",
    "\n",
    "var_xh=np.concatenate((var_2,var_3))\n",
    "var_xhmean=pd.DataFrame(var_xh,index=np.hstack((\n",
    "                                           np.array(range(500,1000,50)),np.array(range(1000,6000,100)))))\n",
    "var_xhmean.to_csv('var_xhmean_celeb.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice optimal window size on  Cross-validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_names=['Balanced_acc','F2_score','Recall','Precision']\n",
    "def evaluate_varxh(X,y,m,model=solution_XX(penalty='l2')):\n",
    "    \n",
    "    bacc,f2,rec,pre=list(),list(),list(),list()\n",
    "   \n",
    "\n",
    "    for train_index,test_index in cv.split(X,y):\n",
    "        \n",
    "        N=X[train_index].shape[0]\n",
    "        n=y[train_index][y[train_index]==1].shape[0]\n",
    "        c_1=np.concatenate((X[train_index][y[train_index]==0],X[train_index][y[train_index]==1]))\n",
    "        l_1=np.concatenate((np.array([0]*(N-n)),np.array([1]*n)))\n",
    "        M=int((N-n)/19)\n",
    "        indice=[k*M for k in range(1,19)]\n",
    "        indice.append(N-n)\n",
    "        pre_cv=np.array(np.split(c_1,indice))\n",
    "        pre_y=np.array(np.split(l_1,indice))\n",
    "        w= model.fit(pre_cv,pre_y)\n",
    "#         w_x.append(w)\n",
    "        prob_y=F_u(m,X_train[test_index].dot(w))\n",
    "        \n",
    "        prdict_y=np.round(prob_y)\n",
    "        rec.append(recall_score(y[test_index],prdict_y))\n",
    "        pre.append(precision_score(y[test_index],prdict_y))\n",
    "\n",
    "        bacc.append(balanced_accuracy_score(y[test_index],prdict_y))\n",
    "        f2.append(fbeta_score(y[test_index],prdict_y,beta=2.94))\n",
    " \n",
    "    return bacc,f2,rec,pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index,test_index in cv.split(X_train,y_tr):\n",
    "        \n",
    "        N=X_train[train_index].shape[0]\n",
    "        n=y_tr[train_index][y_tr[train_index]==1].shape[0]\n",
    "        c_1=np.concatenate((X_train[train_index][y_tr[train_index]==0],X_train[train_index][y_tr[train_index]==1]))\n",
    "        l_1=np.concatenate((np.array([0]*(N-n)),np.array([1]*n)))\n",
    "        M=int((N-n)/19)\n",
    "        indice=[k*M for k in range(1,19)]\n",
    "        indice.append(N-n)\n",
    "        pre_cv=np.array(np.split(c_1,indice))\n",
    "        pre_y=np.array(np.split(l_1,indice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_resultsxhvar = []\n",
    "\n",
    "for k in var_xh:\n",
    "    result=np.mean(evaluate_varxh(X_train,y_tr,k),axis=1)\n",
    "    metric_res = {'lower-var': k[0],'upper-var':k[1]}\n",
    "    for name, value in zip(metrics_names, result):\n",
    "#             print(name, ': ', value)\n",
    "            metric_res[name] = value\n",
    "       \n",
    "\n",
    "    all_resultsxhvar.append(metric_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1105,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(all_resultsxhvar).to_csv('CV_which_sigma_to_set_inxhBIAS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_1=np.array([fsolve(lambda x:equa_v(x,n),[0.5,1.5] ) for n in range(50,110,10)])\n",
    "var_2=np.array([fsolve(lambda x:equa_v(x,n),[0.5,1.5] ) for n in range(110,1000,50)])\n",
    "var_3=np.array([fsolve(lambda x:equa_v(x,n),[0.5,1.5] ) for n in range(1000,6000,100)])\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "var=np.concatenate((var_1,var_2,var_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_mean=pd.DataFrame(var,index=np.hstack((np.array(range(50,110,10)),\n",
    "                                           np.array(range(110,1000,50)),np.array(range(1000,6000,100)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_mean.to_csv('var_mean_celeb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_evaluate_model(X,y,m,model=LogisticRegression()):\n",
    "    \n",
    "    bacc,f2,auc,rec,pre=list(),list(),list(),list(),list()\n",
    "   \n",
    "#   \n",
    "    for train_index,test_index in cv.split(X,y):\n",
    "#                 X_resampled_smote,y_resampled_smote=SMOTE().fit_resample(X[train_index], y[train_index])\n",
    "                model.fit(X[train_index],y[train_index])\n",
    "                prob_y=F_u(m,(np.dot(X[test_index],model_in.coef_.T)+model_in.intercept_).reshape(X[test_index].shape[0],))\n",
    "        \n",
    "                auc.append(roc_auc_score(y[test_index],prob_y))\n",
    "                \n",
    "                prdict_y=np.round(prob_y)\n",
    "                rec.append(recall_score(y[test_index],prdict_y))\n",
    "                pre.append(precision_score(y[test_index],prdict_y))\n",
    "                \n",
    "                bacc.append(balanced_accuracy_score(y[test_index],prdict_y))\n",
    "                f2.append(fbeta_score(y[test_index],prdict_y,beta=2))\n",
    " \n",
    "    return bacc,f2,auc,rec,pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_results_var = []\n",
    "\n",
    "for k in var:\n",
    "    result=np.mean(var_evaluate_model(x_tr,y_tr,k),axis=1)\n",
    "    metric_res = {'lower_var':k[0],'upper_var': k[1]}\n",
    "    for name, value in zip(metrics_names, result):\n",
    "#             print(name, ': ', value)\n",
    "            metric_res[name] = value\n",
    "       \n",
    "\n",
    "    all_results_var.append(metric_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(all_results_var).to_csv('CV_which_sigma_to_set_inLRBIAS.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.90      0.95      3812\n",
      "           1       0.32      0.92      0.47       188\n",
      "\n",
      "    accuracy                           0.90      4000\n",
      "   macro avg       0.66      0.91      0.71      4000\n",
      "weighted avg       0.96      0.90      0.92      4000\n",
      "\n",
      "Classifier : volatility-unceratin max-mean \n",
      "Running time:20.36\n",
      "Balanced-Accuracy on testing set：91.16%\n",
      "Recall on testing set：92.02%\n",
      "F-measure on testing set：76.95%\n",
      "Score:0.859\n"
     ]
    }
   ],
   "source": [
    "##volatility-bias max-mean\n",
    "\n",
    "y_preX_v=F_u([0.075,0.487],np.dot(X_test,w_2)).reshape(x_te.shape[0],)\n",
    "\n",
    "y_preX_v[y_preX_v<0.5]=0\n",
    "y_preX_v[y_preX_v>=0.5]=1\n",
    "print(classification_report(y_te,y_preX_v))\n",
    "bacc=balanced_accuracy_score(y_te,y_preX_v)\n",
    "fbeta=fbeta_score(y_te,y_preX_v,beta=2.94)\n",
    "recall=recall_score(y_te,y_preX_v)\n",
    "print('Classifier : volatility-unceratin max-mean ')\n",
    "print(\"Running time:%.2f\"%(end_x-start_x))\n",
    "\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_preX_v)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_preX_v)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_preX_v,beta=2.94)))\n",
    "print('Score:{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end_x-start_x)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98      3812\n",
      "           1       0.65      0.78      0.71       188\n",
      "\n",
      "    accuracy                           0.97      4000\n",
      "   macro avg       0.82      0.88      0.85      4000\n",
      "weighted avg       0.97      0.97      0.97      4000\n",
      "\n",
      "Classifier : volatility-unceratin LR \n",
      "Running time:3.26\n",
      "Balanced-Accuracy on testing set：88.07%\n",
      "Recall on testing set：78.19%\n",
      "F-measure on testing set：76.63%\n",
      "Score:0.805\n"
     ]
    }
   ],
   "source": [
    "y_preX_v=F_u([0.26,2.27],np.dot(x_te,model_in.coef_.T)+model_in.intercept_).reshape(x_te.shape[0],)\n",
    "\n",
    "y_preX_v[y_preX_v<0.5]=0\n",
    "y_preX_v[y_preX_v>=0.5]=1\n",
    "print(classification_report(y_te,y_preX_v))\n",
    "bacc=balanced_accuracy_score(y_te,y_preX_v)\n",
    "fbeta=fbeta_score(y_te,y_preX_v,beta=2.94)\n",
    "recall=recall_score(y_te,y_preX_v)\n",
    "print('Classifier : volatility-unceratin LR ')\n",
    "print(\"Running time:%.2f\"%(end_i-start_i))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_preX_v)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_preX_v)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_preX_v,beta=2.94)))\n",
    "print('Score:{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end_i-start_i)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.19223182]\n",
      "0.09584379196166992\n"
     ]
    }
   ],
   "source": [
    "s=time.time()\n",
    "print(fsolve(lambda x:equa_m(x,145),0.5 ))\n",
    "end=time.time()\n",
    "print(end-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94      3812\n",
      "           1       0.29      0.93      0.44       188\n",
      "\n",
      "    accuracy                           0.89      4000\n",
      "   macro avg       0.64      0.91      0.69      4000\n",
      "weighted avg       0.96      0.89      0.92      4000\n",
      "\n",
      "Classifier : mean-unceratin LR \n",
      "Running time:3.21\n",
      "Balanced-Accuracy on testing set：90.66%\n",
      "Recall on testing set：92.55%\n",
      "F-measure on testing set：75.35%\n",
      "Score:0.856\n"
     ]
    }
   ],
   "source": [
    "##Mean-bias LR\n",
    "y_preX_m=sigmoid_((np.dot(x_te,model_in.coef_.T)+model_in.intercept_).reshape(x_te.shape[0],),4.26\n",
    "                  \n",
    "                 )\n",
    "\n",
    "y_preX_m[y_preX_m<0.5]=0\n",
    "y_preX_m[y_preX_m>=0.5]=1\n",
    "print(classification_report(y_te,y_preX_m))\n",
    "print('Classifier : mean-unceratin LR ')\n",
    "print(\"Running time:%.2f\"%(end_i-start_i))\n",
    "bacc=balanced_accuracy_score(y_te,y_preX_m)\n",
    "fbeta=fbeta_score(y_te,y_preX_m,beta=2.94)\n",
    "recall=recall_score(y_te,y_preX_m)\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_preX_m)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_preX_m)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_preX_m,beta=2.94)))\n",
    "print('Score:{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end_i-start_i)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.90      0.95      3812\n",
      "           1       0.32      0.92      0.47       188\n",
      "\n",
      "    accuracy                           0.90      4000\n",
      "   macro avg       0.66      0.91      0.71      4000\n",
      "weighted avg       0.96      0.90      0.92      4000\n",
      "\n",
      "Classifier : mean-unceratin max-mean \n",
      "Running time:20.98 s\n",
      "Balanced-Accuracy on testing set：91.13%\n",
      "Recall on testing set：92.02%\n",
      "F-measure on testing set：76.88%\n",
      "Score:0.859\n"
     ]
    }
   ],
   "source": [
    "##Mean-bias max-mean\n",
    "y_preX_x=sigmoid_(np.dot(X_test,w_2).reshape(x_te.shape[0],),0.28)\n",
    "\n",
    "y_preX_x[y_preX_x<0.5]=0\n",
    "y_preX_x[y_preX_x>=0.5]=1\n",
    "print(classification_report(y_te,y_preX_x))\n",
    "bacc=balanced_accuracy_score(y_te,y_preX_x)\n",
    "fbeta=fbeta_score(y_te,y_preX_x,beta=2.94)\n",
    "recall=recall_score(y_te,y_preX_x)\n",
    "print('Classifier : mean-unceratin max-mean ')\n",
    "print(\"Running time:%.2f s\"%(end_x-start_x))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_preX_x)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_preX_x)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_preX_x,beta=2.94)))\n",
    "print('Score:{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end_x-start_x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "编辑元数据",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
