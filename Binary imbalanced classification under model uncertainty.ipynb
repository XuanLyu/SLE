{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lvjingzhe/anaconda3/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/Users/lvjingzhe/anaconda3/lib/python3.7/site-packages/dask/config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n",
      "/Users/lvjingzhe/anaconda3/lib/python3.7/site-packages/dask/dataframe/utils.py:13: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n",
      "/Users/lvjingzhe/anaconda3/lib/python3.7/site-packages/distributed/config.py:20: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  defaults = yaml.load(f)\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as st\n",
    "from sympy.solvers import solve\n",
    "from sympy import Symbol\n",
    "from scipy.optimize import fsolve\n",
    "import testjx\n",
    "import numpy as np\n",
    "import cvxopt\n",
    "import cv2\n",
    "import os \n",
    "import random\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "from cvxopt import matrix\n",
    "from cvxopt import solvers\n",
    "import face_recognition\n",
    "import sklearn.metrics as sm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# from scikitplot import plotters as skplt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from statsmodels.discrete.discrete_model import Logit, Probit, MNLogit\n",
    "from pylab import mpl\n",
    "\n",
    "import warnings\n",
    "import matplotlib as mpl\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from collections import Counter\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import skimage\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import preprocessing\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from os import listdir\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.transform import resize\n",
    "from collections import Counter\n",
    "import imblearn\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "sn.set()\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC # SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "from xgboost import XGBClassifier # XGBClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score,f1_score,recall_score,cohen_kappa_score,precision_score\n",
    "from sklearn.utils import compute_class_weight\n",
    "from sklearn.preprocessing import MinMaxScaler,LabelBinarizer\n",
    "from sklearn.ensemble import AdaBoostClassifier # AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier # KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier # RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow.compat.v1 as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.applications.vgg16 import VGG16 # VGG16\n",
    "from tensorflow.keras.applications.vgg19 import VGG19 # VGG19\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50 # ResNet50\n",
    "from tensorflow.keras.applications.xception import Xception # Xception\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet # MobileNet\n",
    "from tensorflow.keras.applications.nasnet import NASNetMobile # NASNetMobile\n",
    "from tensorflow.keras.applications.densenet import DenseNet169 # DenseNet169\n",
    "from tensorflow.keras.applications.densenet import DenseNet121 # DenseNet121\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2 # MobileNetV2\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3 # InceptionV3\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Flatten, Activation, GlobalAveragePooling2D,Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('/Users/lvjingzhe/Downloads/celebA/train/')\n",
    "train = []\n",
    "    \n",
    "for i in range(20000):\n",
    "# for i in range(20000):\n",
    "    img_ = cv2.imread('/Users/lvjingzhe/Downloads/celebA/train/%d.jpg' %(i))\n",
    "\n",
    "    train.append(img_)\n",
    "\n",
    "# files = os.listdir('/Users/lvjingzhe/Downloads/celebA/test/')\n",
    "# test = []\n",
    "    \n",
    "# for i in range(2000,2200):\n",
    "#     img_ = cv2.imread('/Users/lvjingzhe/Downloads/celebA/test/%d.jpg' %(i))\n",
    "\n",
    "#     test.append(img_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image=np.array(train)\n",
    "train_label=np.concatenate((np.array([0]*19000),np.array([1]*1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.training.Model at 0x1d5cc3b0b8>"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0113 16:38:52.382563 140735694340992 deprecation.py:506] From /Users/lvjingzhe/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running time:275.00307393074036\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start=time.time()\n",
    "base_model= VGG19( weights='imagenet', include_top=False,input_shape=(64,64,3))\n",
    "x = base_model.output\n",
    "# x = Dropout(0.2)(x)\n",
    "predictions = Flatten()(x)\n",
    "\n",
    "model_feat = Model(inputs=base_model.input,outputs=predictions)\n",
    "\n",
    "train_features = model_feat.predict(train_image/255)\n",
    "# test_features=model_feat.predict(test_image/255)\n",
    "end=time.time()\n",
    "print('running time:{}'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 2048)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''top_model=Sequential()\n",
    "top_model.add(Dense(256,activation='relu'))\n",
    "top_model.add(Dense(2,activation='softmax'))\n",
    "model=Sequential()\n",
    "# model.add(model_feat)\n",
    "model.add(top_model)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1dc423e048>"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##VGG19网络结构\n",
    "'''import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "METRICS = [\n",
    "    keras.metrics.TruePositives(name='tp'),\n",
    "    keras.metrics.FalsePositives(name='fp'),\n",
    "    keras.metrics.TrueNegatives(name='tn'),\n",
    "    keras.metrics.FalseNegatives(name='fn'), \n",
    "    keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "    keras.metrics.Precision(name='precision'),\n",
    "    keras.metrics.Recall(name='recall'),\n",
    "    keras.metrics.AUC(name='auc'),\n",
    "]\n",
    "\n",
    "model.compile(\n",
    "            optimizer=keras.optimizers.Adam(lr=1e-3),\n",
    "            loss='categorical_crossentropy'\n",
    "            )\n",
    "model.fit(\n",
    "                        x_tr,y_tr,\n",
    "                        batch_size=2000,\n",
    "                        epochs=1000,verbose=0\n",
    "                       )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr, x_te, y_tr, y_te = train_test_split(train_features,train_label,test_size = 0.2,\n",
    "                                                  shuffle = True,\n",
    "                                                  random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.0"
      ]
     },
     "execution_count": 884,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16000/800\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=np.c_[x_te,np.ones(x_te.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# err=(y_tr-sigmoid(np.dot(X_train,w)))**2\n",
    "# err[index]=2.6*err[index]\n",
    "def split(x):\n",
    "    n=y_tr[y_tr==1].shape[0]\n",
    "    m=int((X_train.shape[0]-n)/19)\n",
    "    indice=[k*m for k in range(1,19)]\n",
    "    indice.append(X_train.shape[0]-n)\n",
    "    x_c=np.concatenate((x[y_tr==0],x[y_tr==1]))\n",
    "    return np.array(np.split(x_c,indice))\n",
    "# [np.sum(split(err)[i]) for i in range(20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_boost=np.concatenate((x_no,x_tr))\n",
    "# y_boost=np.concatenate((y_no,y_tr))\n",
    "X_train=np.c_[x_tr,np.ones(x_tr.shape[0])]\n",
    "n=y_tr[y_tr==1].shape[0]\n",
    "XX=np.concatenate((X_train[y_tr==0],X_train[y_tr==1]))\n",
    "YY=np.concatenate((np.array([0]*(X_train.shape[0]-n)),np.array([1]*n)))\n",
    "m=int((X_train.shape[0]-n)/19)\n",
    "indice=[k*m for k in range(1,19)]\n",
    "indice.append(X_train.shape[0]-n)\n",
    "# X_train=np.c_[x_boost,np.ones(x_boost.shape[0])]\n",
    "# n=y_boost[y_boost==1].shape[0]\n",
    "# XX=np.concatenate((X_train[y_boost==0],X_train[y_boost==1]))\n",
    "# YY=np.concatenate((np.array([0]*(X_train.shape[0]-n)),np.array([1]*n)))\n",
    "# m=int((X_train.shape[0]-n)/19)\n",
    "# indice=[k*m for k in range(1,19)]\n",
    "# indice.append(X_train.shape[0]-n)\n",
    "pre_XX=np.array(np.split(XX,indice))\n",
    "pre_YY=np.array(np.split(YY,indice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "cv= RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_mean_loss(y_true, y_probs,penalty = None):\n",
    "    n=y_true[y_true==1].shape[0]\n",
    "    XX=tf.concat((X_train[y_tr==0],X_train[y_tr==1]))\n",
    "    YY=np.concatenate((np.array([0]*(X_train.shape[0]-n)),np.array([1]*n)))\n",
    "    m=int((X_train.shape[0]-n)/19)\n",
    "    indice=[k*m for k in range(1,19)]\n",
    "    indice.append(X_train.shape[0]-n)\n",
    "    pre_XX=np.array(np.split(XX,indice))\n",
    "    pre_YY=np.array(np.split(YY,indice))\n",
    "\n",
    "    f=tf.reduce_max([tf.reduce_mean((y_probs-y_true)**2 ) for x,y in zip(pre_XX,pre_YY)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional classifiers：LR、CS_LR、AUSTBOOST、BAGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression_:\n",
    "    \n",
    "    #默认没有正则化，正则项参数默认为1，学习率默认为0.001，迭代次数为10001次\n",
    "    def __init__(self,penalty = None,Lambda = 0.03,a = 0.1,epochs = 200):\n",
    "        self.W = None\n",
    "        self.penalty = penalty\n",
    "        self.Lambda = Lambda\n",
    "        self.a = a\n",
    "        self.epochs =epochs\n",
    "        self.sigmoid = lambda x:1/(1 + np.exp(-x))\n",
    "        \n",
    "    def loss(self,x,y):\n",
    "        m=x.shape[0]\n",
    "        y_pred = self.sigmoid(x.dot(self.W))\n",
    "        return (-1/m) * np.sum((y*np.log(y_pred) + (1-y)*np.log(1-y_pred)))\n",
    "    \n",
    "    def fit(self,x,y):\n",
    "        lossList = []\n",
    "        #计算总数据量\n",
    "        m = x.shape[0]\n",
    "        #给x添加偏置项\n",
    "        \n",
    "        #计算总特征数\n",
    "        n = x.shape[1]\n",
    "        #初始化W的值,要变成矩阵形式\n",
    "        np.random.seed(3421)\n",
    "        self.W = np.ones((n,1))\n",
    "        #X转为矩阵形式\n",
    "       \n",
    "        #y转为矩阵形式，这步非常重要,且要是m x 1的维度格式\n",
    "        y_=y.reshape((-1,1))\n",
    "        #循环epochs次\n",
    "        for i in range(self.epochs):\n",
    "            #预测值\n",
    "            h = self.sigmoid(x.dot(self.W))\n",
    "            gradient = (x.T).dot(h - y_)/m\n",
    "            \n",
    "            \n",
    "            #加入l1和l2正则项，和之前的线性回归正则化一样\n",
    "            if self.penalty == 'l2':\n",
    "                gradient = gradient + self.Lambda * self.W\n",
    "            elif self.penalty == 'l1':\n",
    "                gradient = gradient + self.Lambda * np.sign(self.W)\n",
    "          \n",
    "            self.W = self.W-self.a * gradient\n",
    "            if i % 50 == 0:\n",
    "                lossList.append(self.loss(x,y_))\n",
    "\t\t#返回系数，和损失列表\n",
    "        return self.W,lossList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      3812\n",
      "           1       0.80      0.69      0.74       188\n",
      "\n",
      "    accuracy                           0.98      4000\n",
      "   macro avg       0.89      0.84      0.86      4000\n",
      "weighted avg       0.98      0.98      0.98      4000\n",
      "\n",
      "Running time:3.03 s\n",
      "Balanced-Accuracy on testing set：84.14%\n",
      "Recall on testing set：69.15%\n",
      "F-measure on testing set：70.12%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_i=time.time()\n",
    "model_in=LogisticRegression()\n",
    "# model_in.fit(pt.fit_transform(train_image_),train_label_)\n",
    "model_in.fit(x_tr,y_tr)\n",
    "end_i=time.time()\n",
    "y_train_proba=model_in.predict_proba(x_tr)\n",
    "y_train_label=model_in.predict(x_tr)\n",
    "y_test_proba=model_in.predict_proba(x_te)\n",
    "# print('AUC:{}'.format(roc_auc_score(y_te,y_test_proba[:,-1])))\n",
    "y_test_label=model_in.predict(x_te)\n",
    "# y_test_proba=model_in.predict_proba(image_test)\n",
    "# y_test_label=model_in.predict(image_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_te,y_test_label))\n",
    "print(\"Running time:%.2f s\"%(end_i-start_i))\n",
    "from sklearn.metrics import accuracy_score\n",
    "# print('Accuracy on training set：{:.2%}'.format(accuracy_score(y_train_label,y_tr)))\n",
    "# print('Accuracy on testing set：{:.2%}'.format(accuracy_score(y_test_label,y_te)))\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_test_label)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_test_label)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_test_label,beta=2.94)))\n",
    "# print(classification_report(y_tr,y_train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      3812\n",
      "           1       0.98      0.48      0.65       188\n",
      "\n",
      "    accuracy                           0.98      4000\n",
      "   macro avg       0.98      0.74      0.82      4000\n",
      "weighted avg       0.98      0.98      0.97      4000\n",
      "\n",
      "Running time:167.80秒\n",
      "Balanced-Accuracy on testing set：74.18%\n",
      "Recall on testing set：48.40%\n",
      "F-measure on testing set：51.08%\n",
      "Score of SVM：57.31%\n"
     ]
    }
   ],
   "source": [
    "##SVM\n",
    "from sklearn.metrics import fbeta_score\n",
    "import time\n",
    "start=time.time()\n",
    "model_svc=SVC(probability=True)\n",
    "\n",
    "model_svc.fit(x_tr,y_tr)\n",
    "end=time.time()\n",
    "y_train_proba=model_svc.predict_proba(x_tr)\n",
    "y_train_label=model_svc.predict(x_tr)\n",
    "y_test_proba=model_svc.predict_proba(x_te)\n",
    "\n",
    "y_test_label=model_svc.predict(x_te)\n",
    "# y_test_proba=model_in.predict_proba(image_test)\n",
    "# y_test_label=model_in.predict(image_test)\n",
    "bacc=balanced_accuracy_score(y_te,y_test_label)\n",
    "fbeta=fbeta_score(y_te,y_test_label,beta=2.94)\n",
    "recall=recall_score(y_te,y_test_label)\n",
    "# from sklearn.metrics import classification_report\n",
    "print(classification_report(y_te,y_test_label))\n",
    "print(\"Running time:%.2f秒\"%(end-start))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_test_label)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_test_label)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_test_label,beta=2.94)))\n",
    "print('Score of SVM：{:.2%}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end-start)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler,SVMSMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "\n",
    "X_resampled, y_resampled = rus.fit_resample(x_tr,y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time:6.83秒\n",
      "Balanced-Accuracy on testing set：89.31%\n",
      "Recall on testing set：90.43%\n",
      "F-measure on testing set：73.03%\n",
      "Score of SVM:0.836\n"
     ]
    }
   ],
   "source": [
    "##RUS-SVM\n",
    "import time\n",
    "start=time.time()\n",
    "model_svc_r=SVC(gamma='scale',probability=True)\n",
    "# model_in.fit(pt.fit_transform(train_image_),train_label_)\n",
    "model_svc_r.fit(X_resampled, y_resampled)\n",
    "end=time.time()\n",
    "y_train_proba=model_svc_r.predict_proba(x_tr)\n",
    "y_train_label=model_svc_r.predict(x_tr)\n",
    "y_test_proba=model_svc_r.predict_proba(x_te)\n",
    "# print('AUC:{}'.format(roc_auc_score(y_te,y_test_proba[:,-1])))\n",
    "y_test_label=model_svc_r.predict(x_te)\n",
    "bacc=balanced_accuracy_score(y_te,y_test_label)\n",
    "fbeta=fbeta_score(y_te,y_test_label,beta=2.94)\n",
    "recall=recall_score(y_te,y_test_label)\n",
    "\n",
    "print(\"Running time:%.2f秒\"%(end-start))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_test_label)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_test_label)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_test_label,beta=2.94)))\n",
    "print('Score of SVM:{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end-start)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier :CS-SVM\n",
      "Running time:483.40秒\n",
      "Balanced-Accuracy on testing set：90.13%\n",
      "Recall on testing set：86.17%\n",
      "F-measure on testing set：77.65%\n",
      "Score of SVM:0.838\n"
     ]
    }
   ],
   "source": [
    "##SVM+cost-learning\n",
    "import time\n",
    "start=time.time()\n",
    "model_svc3=SVC(class_weight='balanced',probability=True)\n",
    "# model_in.fit(pt.fit_transform(train_image_),train_label_)\n",
    "# model_svc3.fit(X_resampled_smote,y_resampled_smote)\n",
    "model_svc3.fit(x_tr,y_tr)\n",
    "end=time.time()\n",
    "y_train_proba=model_svc3.predict_proba(x_tr)\n",
    "y_train_label=model_svc3.predict(x_tr)\n",
    "y_test_proba=model_svc3.predict_proba(x_te)\n",
    "\n",
    "y_test_label=model_svc3.predict(x_te)\n",
    "\n",
    "bacc=balanced_accuracy_score(y_te,y_test_label)\n",
    "fbeta=fbeta_score(y_te,y_test_label,beta=2.94)\n",
    "recall=recall_score(y_te,y_test_label)\n",
    "print('Classifier :CS-SVM')\n",
    "print(\"Running time:%.2f秒\"%(end-start))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_test_label)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_test_label)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_test_label,beta=2.94)))\n",
    "print('Score of SVM:{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end-start)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:0.9226044294612757\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.89      0.94      3812\n",
      "           1       0.26      0.78      0.39       188\n",
      "\n",
      "    accuracy                           0.88      4000\n",
      "   macro avg       0.62      0.83      0.66      4000\n",
      "weighted avg       0.95      0.88      0.91      4000\n",
      "\n",
      "Classifier :RUS-Boost\n",
      "Running time:8.57秒\n",
      "Balanced-Accuracy on testing set：83.32%\n",
      "Recall on testing set：77.66%\n",
      "F-measure on testing set：64.26%\n",
      "Score of SVM:0.744\n"
     ]
    }
   ],
   "source": [
    "##RUSBoost\n",
    "from imblearn.ensemble import RUSBoostClassifier\n",
    "rusb = RUSBoostClassifier(random_state=0)\n",
    "start=time.time()\n",
    "rusb.fit(x_tr,y_tr)\n",
    "end=time.time()\n",
    "y_train_proba=rusb.predict_proba(x_tr)\n",
    "y_train_label=rusb.predict(x_tr)\n",
    "y_test_proba=rusb.predict_proba(x_te)\n",
    "print('AUC:{}'.format(roc_auc_score(y_te,y_test_proba[:,-1])))\n",
    "y_test_label=rusb.predict(x_te)\n",
    "# y_test_proba=model_in.predict_proba(image_test)\n",
    "# y_test_label=model_in.predict(image_test)\n",
    "bacc=balanced_accuracy_score(y_te,y_test_label)\n",
    "fbeta=fbeta_score(y_te,y_test_label,beta=2.94)\n",
    "recall=recall_score(y_te,y_test_label)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_te,y_test_label))\n",
    "print('Classifier :RUS-Boost')\n",
    "print(\"Running time:%.2f秒\"%(end-start))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_test_label)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_test_label)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_test_label,beta=2.94)))\n",
    "print('Score of SVM:{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end-start)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:0.9706037485209084\n",
      "Classifier :MLP\n",
      "Running time:23.58秒\n",
      "Balanced-Accuracy on testing set：84.09%\n",
      "Recall on testing set：69.15%\n",
      "F-measure on testing set：69.96%\n",
      "Score of SVM:0.737\n"
     ]
    }
   ],
   "source": [
    "##MLP\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp=MLPClassifier(random_state=0, max_iter=200,hidden_layer_sizes=(50,))\n",
    "start=time.time()\n",
    "\n",
    "\n",
    "mlp.fit(x_tr,y_tr)\n",
    "end=time.time()\n",
    "y_train_proba=mlp.predict_proba(x_tr)\n",
    "y_train_label=mlp.predict(x_tr)\n",
    "y_test_proba=mlp.predict_proba(x_te)\n",
    "print('AUC:{}'.format(roc_auc_score(y_te,y_test_proba[:,-1])))\n",
    "y_test_label=mlp.predict(x_te)\n",
    "# y_test_proba=model_in.predict_proba(image_test)\n",
    "# y_test_label=model_in.predict(image_test)\n",
    "bacc=balanced_accuracy_score(y_te,y_test_label)\n",
    "fbeta=fbeta_score(y_te,y_test_label,beta=2.94)\n",
    "recall=recall_score(y_te,y_test_label)\n",
    "print('Classifier :MLP')\n",
    "print(\"Running time:%.2f秒\"%(end-start))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_test_label)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_test_label)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_test_label,beta=2.94)))\n",
    "print('Score of SVM:{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end-start)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "X_resampled_smote,y_resampled_smote  = SMOTE().fit_resample(x_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      3812\n",
      "           1       0.72      0.73      0.73       188\n",
      "\n",
      "    accuracy                           0.97      4000\n",
      "   macro avg       0.85      0.86      0.86      4000\n",
      "weighted avg       0.97      0.97      0.97      4000\n",
      "\n",
      "Classifier :SMOTE-MLP\n",
      "Running time:17.00秒\n",
      "Balanced-Accuracy on testing set：85.99%\n",
      "Recall on testing set：73.40%\n",
      "F-measure on testing set：73.24%\n",
      "Score of SVM:0.768\n"
     ]
    }
   ],
   "source": [
    "##MLP+SMOTE\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp=MLPClassifier(random_state=0, max_iter=200,hidden_layer_sizes=(50,))\n",
    "start=time.time()\n",
    "\n",
    "\n",
    "mlp.fit(X_resampled_smote,y_resampled_smote)\n",
    "end=time.time()\n",
    "y_train_proba=mlp.predict_proba(x_tr)\n",
    "y_train_label=mlp.predict(x_tr)\n",
    "y_test_proba=mlp.predict_proba(x_te)\n",
    "\n",
    "y_test_label=mlp.predict(x_te)\n",
    "# y_test_proba=model_in.predict_proba(image_test)\n",
    "# y_test_label=model_in.predict(image_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_te,y_test_label))\n",
    "bacc=balanced_accuracy_score(y_te,y_test_label)\n",
    "fbeta=fbeta_score(y_te,y_test_label,beta=2.94)\n",
    "recall=recall_score(y_te,y_test_label)\n",
    "print('Classifier :SMOTE-MLP')\n",
    "print(\"Running time:%.2f秒\"%(end-start))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_test_label)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_test_label)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_test_label,beta=2.94)))\n",
    "print('Score of SVM:{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end-start)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier :RUS-MLP\n",
      "Running time:3.05秒\n",
      "Balanced-Accuracy on testing set：90.13%\n",
      "Recall on testing set：92.02%\n",
      "F-measure on testing set：74.28%\n",
      "Score of SVM:0.85\n"
     ]
    }
   ],
   "source": [
    "##MLP+RUS\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp=MLPClassifier(random_state=10, max_iter=200,hidden_layer_sizes=(50,))\n",
    "start=time.time()\n",
    "\n",
    "\n",
    "mlp.fit(X_resampled,y_resampled)\n",
    "end=time.time()\n",
    "y_train_proba=mlp.predict_proba(x_tr)\n",
    "y_train_label=mlp.predict(x_tr)\n",
    "y_test_proba=mlp.predict_proba(x_te)\n",
    "\n",
    "y_test_label=mlp.predict(x_te)\n",
    "# y_test_proba=model_in.predict_proba(image_test)\n",
    "# y_test_label=model_in.predict(image_test)\n",
    "\n",
    "bacc=balanced_accuracy_score(y_te,y_test_label)\n",
    "fbeta=fbeta_score(y_te,y_test_label,beta=2.94)\n",
    "recall=recall_score(y_te,y_test_label)\n",
    "print('Classifier :RUS-MLP')\n",
    "print(\"Running time:%.2f秒\"%(end-start))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_test_label)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_test_label)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_test_label,beta=2.94)))\n",
    "print('Score :{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end-start)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import NearMiss\n",
    "# nm1 = NearMiss(version=1)\n",
    "nm2= NearMiss(version=2)\n",
    "nm3= NearMiss(version=3)\n",
    "# X_resampled_1, y_resampled_1 = nm1.fit_resample(x_tr, y_tr)\n",
    "X_resampled_2, y_resampled_2= nm2.fit_resample(x_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import ADASYN\n",
    "X_resampled_,y_resampled_=ADASYN(random_state=42).fit_resample(x_tr,y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:0.6801505882878257\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      3812\n",
      "           1       0.37      0.39      0.38       188\n",
      "\n",
      "    accuracy                           0.94      4000\n",
      "   macro avg       0.67      0.68      0.67      4000\n",
      "weighted avg       0.94      0.94      0.94      4000\n",
      "\n",
      "Classifier :CS-Cart\n",
      "Running time:15.70秒\n",
      "Balanced-Accuracy on testing set：68.02%\n",
      "Recall on testing set：39.36%\n",
      "F-measure on testing set：39.08%\n",
      "Score of SVM:0.484\n"
     ]
    }
   ],
   "source": [
    "##CART+cost-learing\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(class_weight='balanced',random_state=0)\n",
    "start=time.time()\n",
    "\n",
    "\n",
    "clf.fit(x_tr,y_tr)\n",
    "end=time.time()\n",
    "y_train_proba=clf.predict_proba(x_tr)\n",
    "y_train_label=clf.predict(x_tr)\n",
    "y_test_proba=clf.predict_proba(x_te)\n",
    "print('AUC:{}'.format(roc_auc_score(y_te,y_test_proba[:,-1])))\n",
    "y_test_label=clf.predict(x_te)\n",
    "# y_test_proba=model_in.predict_proba(image_test)\n",
    "# y_test_label=model_in.predict(image_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_te,y_test_label))\n",
    "bacc=balanced_accuracy_score(y_te,y_test_label)\n",
    "fbeta=fbeta_score(y_te,y_test_label,beta=2.94)\n",
    "recall=recall_score(y_te,y_test_label)\n",
    "print('Classifier :CS-Cart')\n",
    "print(\"Running time:%.2f秒\"%(end-start))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_test_label)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_test_label)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_test_label,beta=2.94)))\n",
    "print('Score of CS-Cart:{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end-start)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44556666666666667"
      ]
     },
     "execution_count": 1010,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.39+0.682+0.391)*0.3+0.1/15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:0.7088589225514055\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96      3812\n",
      "           1       0.31      0.47      0.38       188\n",
      "\n",
      "    accuracy                           0.93      4000\n",
      "   macro avg       0.64      0.71      0.67      4000\n",
      "weighted avg       0.94      0.93      0.93      4000\n",
      "\n",
      "Classifier :SMOTE-Cart\n",
      "Running time:111.43秒\n",
      "Balanced-Accuracy on testing set：70.89%\n",
      "Recall on testing set：46.81%\n",
      "F-measure on testing set：44.55%\n",
      "Score of SVM:0.535\n"
     ]
    }
   ],
   "source": [
    "##CART+SMOTE\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "start=time.time()\n",
    "\n",
    "\n",
    "clf.fit(X_resampled_smote,y_resampled_smote)\n",
    "end=time.time()\n",
    "y_train_proba=clf.predict_proba(x_tr)\n",
    "y_train_label=clf.predict(x_tr)\n",
    "y_test_proba=clf.predict_proba(x_te)\n",
    "print('AUC:{}'.format(roc_auc_score(y_te,y_test_proba[:,-1])))\n",
    "y_test_label=clf.predict(x_te)\n",
    "# y_test_proba=model_in.predict_proba(image_test)\n",
    "# y_test_label=model_in.predict(image_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_te,y_test_label))\n",
    "bacc=balanced_accuracy_score(y_te,y_test_label)\n",
    "fbeta=fbeta_score(y_te,y_test_label,beta=2.94)\n",
    "recall=recall_score(y_te,y_test_label)\n",
    "print('Classifier :SMOTE-Cart')\n",
    "print(\"Running time:%.2f秒\"%(end-start))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_test_label)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_test_label)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_test_label,beta=2.94)))\n",
    "print('Score of SMOTE-Cart:{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end-start)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:0.7916210845928869\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.77      0.87      3812\n",
      "           1       0.15      0.81      0.25       188\n",
      "\n",
      "    accuracy                           0.77      4000\n",
      "   macro avg       0.57      0.79      0.56      4000\n",
      "weighted avg       0.95      0.77      0.84      4000\n",
      "\n",
      "Classifier :RUS-Cart\n",
      "Running time:1.25秒\n",
      "Balanced-Accuracy on testing set：79.16%\n",
      "Recall on testing set：81.38%\n",
      "F-measure on testing set：55.53%\n",
      "Score of RUS-Cart:0.721\n"
     ]
    }
   ],
   "source": [
    "##CART+RUS\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "start=time.time()\n",
    "\n",
    "\n",
    "clf.fit(X_resampled,y_resampled)\n",
    "end=time.time()\n",
    "y_train_proba=clf.predict_proba(x_tr)\n",
    "y_train_label=clf.predict(x_tr)\n",
    "y_test_proba=clf.predict_proba(x_te)\n",
    "print('AUC:{}'.format(roc_auc_score(y_te,y_test_proba[:,-1])))\n",
    "y_test_label=clf.predict(x_te)\n",
    "# y_test_proba=model_in.predict_proba(image_test)\n",
    "# y_test_label=model_in.predict(image_test)\n",
    "bacc=balanced_accuracy_score(y_te,y_test_label)\n",
    "fbeta=fbeta_score(y_te,y_test_label,beta=2.94)\n",
    "recall=recall_score(y_te,y_test_label)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_te,y_test_label))\n",
    "print('Classifier :RUS-Cart')\n",
    "print(\"Running time:%.2f秒\"%(end-start))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_test_label)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_test_label)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_test_label,beta=2.94)))\n",
    "print('Score of RUS-Cart:{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end-start)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1005,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7290672131147541"
      ]
     },
     "execution_count": 1005,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.555+0.792+0.81)*0.3+0.1/1.22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imbalanced_ensemble.ensemble import SMOTEBoostClassifier\n",
    "smb = SMOTEBoostClassifier(random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "smote_tomek = SMOTETomek(random_state=42)\n",
    "X_resampled, y_resampled = smote_tomek.fit_resample(x_tr,y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:0.918534136322029\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.96      0.97      3812\n",
      "           1       0.46      0.63      0.53       188\n",
      "\n",
      "    accuracy                           0.95      4000\n",
      "   macro avg       0.72      0.80      0.75      4000\n",
      "weighted avg       0.96      0.95      0.95      4000\n",
      "\n",
      "Classifier :SMOTE-Boost\n",
      "Running time:234.80秒\n",
      "Balanced-Accuracy on testing set：79.81%\n",
      "Recall on testing set：63.30%\n",
      "F-measure on testing set：60.91%\n",
      "Score :0.673\n"
     ]
    }
   ],
   "source": [
    "##SMOTEBoost\n",
    "start=time.time()\n",
    "smb.fit(x_tr,y_tr)\n",
    "end=time.time()\n",
    "y_train_proba=smb.predict_proba(x_tr)\n",
    "y_train_label=smb.predict(x_tr)\n",
    "y_test_proba=smb.predict_proba(x_te)\n",
    "print('AUC:{}'.format(roc_auc_score(y_te,y_test_proba[:,-1])))\n",
    "y_test_label=smb.predict(x_te)\n",
    "# y_test_proba=model_in.predict_proba(image_test)\n",
    "# y_test_label=model_in.predict(image_test)\n",
    "bacc=balanced_accuracy_score(y_te,y_test_label)\n",
    "fbeta=fbeta_score(y_te,y_test_label,beta=2.94)\n",
    "recall=recall_score(y_te,y_test_label)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_te,y_test_label))\n",
    "print('Classifier :SMOTE-Boost')\n",
    "print(\"Running time:%.2f秒\"%(end-start))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_test_label)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_test_label)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_test_label,beta=2.94)))\n",
    "print('Score :{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end-start)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:0.9448515047665825\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.85      0.92      3812\n",
      "           1       0.23      0.90      0.37       188\n",
      "\n",
      "    accuracy                           0.86      4000\n",
      "   macro avg       0.61      0.88      0.64      4000\n",
      "weighted avg       0.96      0.86      0.89      4000\n",
      "\n",
      "Classifier :RUS-Boost\n",
      "Running time:6.59秒\n",
      "Balanced-Accuracy on testing set：87.67%\n",
      "Recall on testing set：89.89%\n",
      "F-measure on testing set：69.38%\n",
      "Score :0.816\n"
     ]
    }
   ],
   "source": [
    "adb=AdaBoostClassifier(random_state=0)\n",
    "start=time.time()\n",
    "adb.fit(X_resampled, y_resampled )\n",
    "end=time.time()\n",
    "y_train_proba=adb.predict_proba(x_tr)\n",
    "y_train_label=adb.predict(x_tr)\n",
    "y_test_proba=adb.predict_proba(x_te)\n",
    "print('AUC:{}'.format(roc_auc_score(y_te,y_test_proba[:,-1])))\n",
    "y_test_label=adb.predict(x_te)\n",
    "# y_test_proba=model_in.predict_proba(image_test)\n",
    "# y_test_label=model_in.predict(image_test)\n",
    "bacc=balanced_accuracy_score(y_te,y_test_label)\n",
    "fbeta=fbeta_score(y_te,y_test_label,beta=2.94)\n",
    "recall=recall_score(y_te,y_test_label)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_te,y_test_label))\n",
    "print('Classifier :RUS-Boost')\n",
    "print(\"Running time:%.2f秒\"%(end-start))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_test_label)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_test_label)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_test_label,beta=2.94)))\n",
    "print('Score :{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end-start)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98      3812\n",
      "           1       0.73      0.55      0.63       188\n",
      "\n",
      "    accuracy                           0.97      4000\n",
      "   macro avg       0.85      0.77      0.81      4000\n",
      "weighted avg       0.97      0.97      0.97      4000\n",
      "\n",
      "Classifier :Adaboost\n",
      "Running time:81.01秒\n",
      "Balanced-Accuracy on testing set：76.90%\n",
      "Recall on testing set：54.79%\n",
      "F-measure on testing set：56.25%\n",
      "Score:0.62\n"
     ]
    }
   ],
   "source": [
    "##ADABoost\n",
    "adb=AdaBoostClassifier(random_state=0)\n",
    "start=time.time()\n",
    "adb.fit(x_tr,y_tr)\n",
    "end=time.time()\n",
    "y_train_proba=adb.predict_proba(x_tr)\n",
    "y_train_label=adb.predict(x_tr)\n",
    "y_test_proba=adb.predict_proba(x_te)\n",
    "\n",
    "y_test_label=adb.predict(x_te)\n",
    "# y_test_proba=model_in.predict_proba(image_test)\n",
    "# y_test_label=model_in.predict(image_test)\n",
    "bacc=balanced_accuracy_score(y_te,y_test_label)\n",
    "fbeta=fbeta_score(y_te,y_test_label,beta=2.94)\n",
    "recall=recall_score(y_te,y_test_label)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_te,y_test_label))\n",
    "\n",
    "print('Classifier :Adaboost')\n",
    "print(\"Running time:%.2f秒\"%(end-start))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_test_label)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_test_label)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_test_label,beta=2.94)))\n",
    "print('Score:{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end-start)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:0.872678104976446\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      3812\n",
      "           1       0.84      0.44      0.58       188\n",
      "\n",
      "    accuracy                           0.97      4000\n",
      "   macro avg       0.91      0.72      0.78      4000\n",
      "weighted avg       0.97      0.97      0.97      4000\n",
      "\n",
      "Classifier :Bagging\n",
      "Running time:285.68秒\n",
      "Balanced-Accuracy on testing set：71.86%\n",
      "Recall on testing set：44.15%\n",
      "F-measure on testing set：46.43%\n",
      "Score:0.536\n"
     ]
    }
   ],
   "source": [
    "##BAgging\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "bag = BaggingClassifier(\n",
    "                                random_state=0)\n",
    "\n",
    "\n",
    "start=time.time()\n",
    "\n",
    "\n",
    "bag.fit(x_tr,y_tr)\n",
    "end=time.time()\n",
    "y_train_proba=bag.predict_proba(x_tr)\n",
    "y_train_label=bag.predict(x_tr)\n",
    "y_test_proba=bag.predict_proba(x_te)\n",
    "print('AUC:{}'.format(roc_auc_score(y_te,y_test_proba[:,-1])))\n",
    "y_test_label=bag.predict(x_te)\n",
    "# y_test_proba=model_in.predict_proba(image_test)\n",
    "# y_test_label=model_in.predict(image_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_te,y_test_label))\n",
    "bacc=balanced_accuracy_score(y_te,y_test_label)\n",
    "fbeta=fbeta_score(y_te,y_test_label,beta=2.94)\n",
    "recall=recall_score(y_te,y_test_label)\n",
    "print('Classifier :Bagging')\n",
    "print(\"Running time:%.2f秒\"%(end-start))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_test_label)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_test_label)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_test_label,beta=2.94)))\n",
    "print('Score:{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end-start)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.86      0.92      3812\n",
      "           1       0.23      0.85      0.36       188\n",
      "\n",
      "    accuracy                           0.86      4000\n",
      "   macro avg       0.61      0.85      0.64      4000\n",
      "weighted avg       0.96      0.86      0.90      4000\n",
      "\n",
      "Classifier :RUS-Bag\n",
      "Running time:8.17秒\n",
      "Balanced-Accuracy on testing set：85.40%\n",
      "Recall on testing set：84.57%\n",
      "F-measure on testing set：66.41%\n",
      "Score:0.781\n"
     ]
    }
   ],
   "source": [
    "##Bagging+RUS\n",
    "bag = BaggingClassifier(\n",
    "                                random_state=0)\n",
    "\n",
    "\n",
    "start=time.time()\n",
    "\n",
    "\n",
    "bag.fit(X_resampled,y_resampled)\n",
    "end=time.time()\n",
    "y_train_proba=bag.predict_proba(x_tr)\n",
    "y_train_label=bag.predict(x_tr)\n",
    "y_test_proba=bag.predict_proba(x_te)\n",
    "\n",
    "y_test_label=bag.predict(x_te)\n",
    "# y_test_proba=model_in.predict_proba(image_test)\n",
    "# y_test_label=model_in.predict(image_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_te,y_test_label))\n",
    "bacc=balanced_accuracy_score(y_te,y_test_label)\n",
    "fbeta=fbeta_score(y_te,y_test_label,beta=2.94)\n",
    "recall=recall_score(y_te,y_test_label)\n",
    "print('Classifier :RUS-Bag')\n",
    "print(\"Running time:%.2f秒\"%(end-start))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_test_label)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_test_label)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_test_label,beta=2.94)))\n",
    "print('Score:{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end-start)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC:0.8955579803978477\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98      3812\n",
      "           1       0.56      0.44      0.49       188\n",
      "\n",
      "    accuracy                           0.96      4000\n",
      "   macro avg       0.77      0.71      0.74      4000\n",
      "weighted avg       0.95      0.96      0.96      4000\n",
      "\n",
      "运行时间:613.53秒\n",
      "训练集准确率：99.91%\n",
      "测试集准确率：95.75%\n",
      "测试集Bacc：71.22%\n",
      "测试集f2：45.14%\n",
      "测试集recall：44.15%\n"
     ]
    }
   ],
   "source": [
    "##Bagging +SMOte\n",
    "'''bag = BaggingClassifier(\n",
    "                                random_state=0)\n",
    "\n",
    "\n",
    "start=time.time()\n",
    "\n",
    "\n",
    "bag.fit(X_resampled_smote,y_resampled_smote)\n",
    "end=time.time()\n",
    "y_train_proba=bag.predict_proba(x_tr)\n",
    "y_train_label=bag.predict(x_tr)\n",
    "y_test_proba=bag.predict_proba(x_te)\n",
    "print('AUC:{}'.format(roc_auc_score(y_te,y_test_proba[:,-1])))\n",
    "y_test_label=bag.predict(x_te)\n",
    "# y_test_proba=model_in.predict_proba(image_test)\n",
    "# y_test_label=model_in.predict(image_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_te,y_test_label))\n",
    "bacc=balanced_accuracy_score(y_te,y_test_label)\n",
    "fbeta=fbeta_score(y_te,y_test_label,beta=2.94)\n",
    "recall=recall_score(y_te,y_test_label)\n",
    "print('Classifier :SMOTE-Bag')\n",
    "print(\"Running time:%.2f秒\"%(end-start))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_test_label)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_test_label)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_test_label,beta=2.94)))\n",
    "print('Score:{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end-start)))'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5356255871886121"
      ]
     },
     "execution_count": 1111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.44+0.464+0.719)*0.33+0.01/281"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(799,)"
      ]
     },
     "execution_count": 821,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split(err)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "## XUAN[2019]+L2正则项\n",
    "def split(x):\n",
    "    n=y_tr[y_tr==1].shape[0]\n",
    "    m=int((X_train.shape[0]-n)/19)\n",
    "    indice=[k*m for k in range(1,19)]\n",
    "    indice.append(X_train.shape[0]-n)\n",
    "    x_c=np.concatenate((x[y_tr==0],x[y_tr==1]))\n",
    "    return np.array(np.split(x_c,indice))\n",
    "\n",
    "class solution_XX:\n",
    "    \n",
    "    #默认没有正则化，正则项参数默认为1，学习率默认为0.001，迭代次数为10001次\n",
    "    def __init__(self,penalty = None,Lambda = 0.03,a = 0.5,epochs = 200):\n",
    "        self.W = None\n",
    "        self.penalty = penalty\n",
    "        self.Lambda = Lambda\n",
    "        self.a = a\n",
    "        self.epochs =epochs\n",
    "        self.sigmoid = lambda x:1/(1 + np.exp(-x))\n",
    "        \n",
    "#     err=(y_tr-s.sigmoid(np.dot(X_train,w)))**2\n",
    "#     err[index]=2.6*err[index]\n",
    "\n",
    "    def f_XX(self,X,Y):\n",
    "        if self.penalty=='l1':f=np.array([np.mean((self.sigmoid(np.dot(x,self.W))-y)**2 )+self.Lambda*np.sum(np.abs(self.W)) for x,y in zip(X,Y)])\n",
    "        elif self.penalty=='l2':f=np.array([np.mean((self.sigmoid(np.dot(x,self.W))-y)**2 )+self.Lambda*np.sum(self.W**2) for x,y in zip(X,Y)])#pre_Xtrain,pre_Ytrain\n",
    "        else:f=np.array([np.mean((self.sigmoid(np.dot(x,self.W))-y)**2 ) for x,y in zip(X,Y)])\n",
    "\n",
    "        return f         \n",
    "    def Gf_XX(self,X,Y):#求导数矩阵N*2\n",
    "        if self.penalty=='l1':d=np.array([x.T.dot(0.02*(self.sigmoid(np.dot(x,self.W))-y)*self.sigmoid(np.dot(x,self.W))*(1-self.sigmoid(np.dot(x,self.W))))+self.Lambda*np.sign(self.W )for x,y in zip(X,Y)])\n",
    "        elif self.penalty=='l2':\n",
    "            d=np.array([x.T.dot(0.02*(self.sigmoid(np.dot(x,self.W))-y)*self.sigmoid(np.dot(x,self.W))*(1-self.sigmoid(np.dot(x,self.W))))+2*self.Lambda*self.W for x,y in zip(X,Y)])\n",
    "#     return d.reshape(20,12289)\n",
    "        else:d=np.array([x.T.dot(0.02*(self.sigmoid(np.dot(x,self.W))-y)*self.sigmoid(np.dot(x,self.W))*(1-self.sigmoid(np.dot(x,self.W)))) for x,y in zip(X,Y)])\n",
    "        return d\n",
    "    def direction_XX(self,X,Y):\n",
    "        gra=self.Gf_XX(X,Y)#导数矩阵N*30001\n",
    "        p=matrix(gra.dot(gra.T),tc='d')\n",
    "        q=matrix(-self.f_XX(X,Y),tc='d')#N维列向量\n",
    "        G=matrix(np.diag(np.array([-1]*20)),tc='d')#N=20\n",
    "        h=matrix(np.array([[0]]*20),tc='d')\n",
    "        A=matrix([[1.0]]*20)\n",
    "        b=matrix([1.0])\n",
    "        solvers.options['show_progress'] = False\n",
    "        sol = solvers.qp(p,q,G,h,A,b)\n",
    "        t=np.array(sol['x'])\n",
    "        d= -(gra.T.dot(t))\n",
    "        return d.reshape((X_train.shape[-1],))#30001维列向量\n",
    "# def BB_X(v):\n",
    "#     return sigmoid(np.dot(pre_Xtest,v))\n",
    "# def ff_1_X(w,v):\n",
    "#     w=w.reshape(15,1)\n",
    "#     return np.mean((sigmoid(np.dot(BB(v),w[:-1])+w[-1])-test_label)**2 )\n",
    "# def ff_X(w):\n",
    "#     return np.mean((sigmoid(np.dot(pre_Xtest,w))-test_label)**2 )\n",
    "    def fit(self,X,Y):\n",
    "        \n",
    "        call=[]\n",
    "        pre=[]\n",
    "        loss=[]\n",
    "        testloss=[]\n",
    "        np.random.seed(1324)\n",
    "        self.W=np.random.random((X_train.shape[-1],))*2-1\n",
    "#         self.W=w_2\n",
    "        n=y_te[y_te==1.].shape[0]\n",
    "        for k in range(200):\n",
    "    #     while np.linalg.norm(d)//10**(-8) >= 10:\n",
    "            d=self.direction_XX(X,Y)\n",
    "#             print(np.linalg.norm(d))\n",
    "            if np.linalg.norm(d)//10**(-7) < 25:\n",
    "                break\n",
    "            sigma=0.8\n",
    "            f_1=np.max(self.f_XX(X,Y))\n",
    "            w=self.W\n",
    "            self.W=d*sigma+w\n",
    "            while np.max(self.f_XX(X,Y))>np.max(f_1):\n",
    "                sigma=sigma*0.8\n",
    "                self.W=d*sigma+w\n",
    "            self.W=d*sigma+w\n",
    "\n",
    "            \n",
    "    #         output=output.reshape(4000,)\n",
    "    #         pt=max(output[output==1.].shape[0],1)\n",
    "    #         m=0\n",
    "    #         for j in range(4000):\n",
    "    #             if output[j]==test_label[j]==1:\n",
    "    #                 m+=1\n",
    "\n",
    "\n",
    "    # # b1=np.random.random((1,10))*2-1\n",
    "\n",
    "\n",
    "            \n",
    "    #         call.append(m/n)\n",
    "    #         pre.append(m/pt)\n",
    "    #         loss.append(max(f_X(w)))\n",
    "    #         testloss.append(ff_X(w))\n",
    "        return self.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier :L2 max-mean loss\n",
      "Running time:20.09 s\n",
      "Balanced-Accuracy on testing set：90.07%\n",
      "Recall on testing set：86.70%\n",
      "F-measure on testing set：77.13%\n",
      "Score:0.838\n"
     ]
    }
   ],
   "source": [
    "s_2=solution_XX(penalty='l2')\n",
    "start_x=time.time()\n",
    "w_2 = s_2.fit(pre_XX,pre_YY)\n",
    "end_x=time.time()\n",
    "y_pred=s_2.sigmoid(X_test.dot(w_2))\n",
    "\n",
    "y_train_pred=s_2.sigmoid(X_train.dot(w_2))\n",
    "y_train_pred[y_train_pred>=0.5]=1\n",
    "y_train_pred[y_train_pred<0.5]=0\n",
    "y_pred[y_pred>=0.5]=1\n",
    "y_pred[y_pred<0.5]=0\n",
    "# # C_x.append(precision_score(y_pred,train_label_[test_index]))\n",
    "bacc=balanced_accuracy_score(y_te,y_pred)\n",
    "fbeta=fbeta_score(y_te,y_pred,beta=2.94)\n",
    "recall=recall_score(y_te,y_pred)\n",
    "print('Classifier :L2 max-mean loss')\n",
    "print(\"Running time:%.2f s\"%(end_x-start_x))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_pred)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_pred)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_pred,beta=2.94)))\n",
    "print('Score:{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end_x-start_x)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier : max-mean loss\n",
      "Running time:13.82\n",
      "Balanced-Accuracy on testing set：88.20%\n",
      "Recall on testing set：83.51%\n",
      "F-measure on testing set：73.75%\n",
      "Score:0.811\n"
     ]
    }
   ],
   "source": [
    "s=solution_XX()\n",
    "start=time.time()\n",
    "w = s.fit(pre_XX,pre_YY)\n",
    "end=time.time()\n",
    "y_pred=s.sigmoid(X_test.dot(w))\n",
    "\n",
    "y_train_pred=s.sigmoid(X_train.dot(w))\n",
    "y_train_pred[y_train_pred>=0.5]=1\n",
    "y_train_pred[y_train_pred<0.5]=0\n",
    "y_pred[y_pred>=0.5]=1\n",
    "y_pred[y_pred<0.5]=0\n",
    "# # C_x.append(precision_score(y_pred,train_label_[test_index]))\n",
    "bacc=balanced_accuracy_score(y_te,y_pred)\n",
    "fbeta=fbeta_score(y_te,y_pred,beta=2.94)\n",
    "recall=recall_score(y_te,y_pred)\n",
    "print('Classifier : max-mean loss')\n",
    "print(\"Running time:%.2f\"%(end-start))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_pred)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_pred)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_pred,beta=2.94)))\n",
    "print('Score:{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end-start)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean_uncertain method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_(x,u):\n",
    "    return 1.0/(1.0+np.exp(-x-u))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_in=(np.dot(x_tr,model_in.coef_.T)+model_in.intercept_).reshape(x_tr.shape[0],)\n",
    "n_len=pre_in[pre_in>=0].shape[0]\n",
    "index=np.argwhere(pre_in>=0).reshape(n_len,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanuncertainty(x,n):\n",
    "    r=[]\n",
    "    for i in range(0,len(x)+1-n,n//5):\n",
    "        r.append(np.mean(x[i:i+n]))\n",
    "    return min(r),max(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equa_xh(x,n):\n",
    "    pre_in=np.dot(X_train,w_2).reshape(x_tr.shape[0],)\n",
    "    ini_err=y_tr-sigmoid_(pre_in,x)\n",
    "    err_w=shuffle(np.concatenate((np.random.choice(ini_err[y_tr==0],ini_err[y_tr==1].shape[0]),ini_err[y_tr==1])))\n",
    "    for k in range(ini_err.shape[0]):\n",
    "    \n",
    "        if y_tr[k]==1:\n",
    "            ini_err[k]=10*ini_err[k]\n",
    "\n",
    "    \n",
    "    return meanuncertainty(ini_err,n)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equa_m(x,n):\n",
    "    \n",
    "    ini_err=y_tr-sigmoid_(pre_in,x)\n",
    "#     err_w=shuffle(np.concatenate((np.random.choice(ini_err[y_tr==0],ini_err[y_tr==1].shape[0]),ini_err[y_tr==1])))\n",
    "    for k in range(ini_err.shape[0]):\n",
    "    \n",
    "        if y_tr[k]==1:\n",
    "            ini_err[k]=10*ini_err[k]\n",
    "\n",
    "    \n",
    "    return meanuncertainty(ini_err,n)[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08102938])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fsolve(lambda x:equa_xh(x,560),0.5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanxh_1=np.array([fsolve(lambda x:equa_xh(x,n),0.5 ) for n in range(10,1000,10)])\n",
    "# meanxh_2=np.array([fsolve(lambda x:equa_xh(x,n),0.5 ) for n in range(1000,10000,100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(meanxh_1,index=np.array(range(10,1000,10))).to_csv('glass_upper_mean.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 940,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_xh=np.concatenate((meanxh_1,meanxh_2))[:-35].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0004772297745227459, 0.0004772297745227459)"
      ]
     },
     "execution_count": 880,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meanuncertainty(y_tr-sigmoid_(pre_in,0),16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_1=np.array([fsolve(lambda x:equa_m(x,n),0.5 ) for n in range(10,1000,10)])\n",
    "mean_2=np.array([fsolve(lambda x:equa_m(x,n),0.5 ) for n in range(1000,10000,100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mean_lr=np.concatenate((mean_1,mean_2))[:-20].reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 966,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.692307692307693"
      ]
     },
     "execution_count": 966,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "12800/650"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice optimal window size on  Cross-validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "metrics_names=['Balanced_acc','F2_score','Recall','Precision']\n",
    "def evaluate_modelxh(X,y,m,model=solution_XX()):\n",
    "    \n",
    "    bacc,f2,rec,pre=list(),list(),list(),list()\n",
    "   \n",
    "#     cv = RepeatedStratifiedKFold(n_splits=10,n_repeats=1, random_state=1)\n",
    "    for train_index,test_index in cv.split(X,y):\n",
    "        \n",
    "#         N=X[train_index].shape[0]\n",
    "#         n=y[train_index][y[train_index]==1].shape[0]\n",
    "#         c_1=np.concatenate((X[train_index][y[train_index]==0],X[train_index][y[train_index]==1]))\n",
    "#         l_1=np.concatenate((np.array([0]*(N-n)),np.array([1]*n)))\n",
    "#         M=int((N-n)/19)\n",
    "#         indice=[k*M for k in range(1,19)]\n",
    "#         indice.append(N-n)\n",
    "#         pre_cv=np.array(np.split(c_1,indice))\n",
    "#         pre_y=np.array(np.split(l_1,indice))\n",
    "#         start=time.time()\n",
    "#         w= model.fit(pre_cv,pre_y)\n",
    "#         end=time.time()\n",
    "#         print(end-start)\n",
    "#         w_x.append(w)\n",
    "        prob_y=sigmoid_(X_train[test_index].dot(w_2),m)\n",
    "        \n",
    "        prdict_y=np.round(prob_y)\n",
    "        rec.append(recall_score(y[test_index],prdict_y))\n",
    "        pre.append(precision_score(y[test_index],prdict_y))\n",
    "\n",
    "        bacc.append(balanced_accuracy_score(y[test_index],prdict_y))\n",
    "        f2.append(fbeta_score(y[test_index],prdict_y,beta=2.94))\n",
    " \n",
    "    return bacc,f2,rec,pre\n",
    "    \n",
    "#                 X_resampled_smote,y_resampled_smote=SMOTE().fit_resample(X[train_index], y[train_index])\n",
    "           \n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanxh_1=meanxh_1.reshape(99,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.436972672117328"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meanxh_1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.566361665725708\n",
      "10.885604858398438\n",
      "10.936495780944824\n",
      "11.015794277191162\n",
      "11.218694925308228\n",
      "60.134746074676514\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "evaluate_modelxh(X_train,y_tr,meanxh_1[1])\n",
    "end=time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.73898983001709\n",
      "11.121030807495117\n",
      "11.174737930297852\n",
      "12.029088020324707\n",
      "13.282639980316162\n",
      "12.631754875183105\n",
      "12.181988954544067\n",
      "10.262044906616211\n",
      "11.65503215789795\n",
      "11.197637796401978\n",
      "12.983259916305542\n",
      "12.804320096969604\n",
      "11.872678995132446\n",
      "12.662386894226074\n",
      "12.476053953170776\n",
      "12.269008874893188\n",
      "11.643685817718506\n",
      "11.485091924667358\n",
      "11.457480192184448\n",
      "11.54461407661438\n",
      "11.973052024841309\n",
      "10.619282007217407\n",
      "11.33933711051941\n",
      "11.54064393043518\n",
      "11.010878086090088\n",
      "11.210134029388428\n",
      "11.531489133834839\n",
      "12.165132999420166\n",
      "12.23213815689087\n",
      "11.158826112747192\n",
      "11.2482008934021\n",
      "12.193104028701782\n",
      "11.94871997833252\n",
      "11.479596138000488\n",
      "11.426672220230103\n",
      "11.367796897888184\n",
      "10.417709112167358\n",
      "11.163100004196167\n",
      "11.110937118530273\n",
      "11.550012111663818\n",
      "11.526576042175293\n",
      "11.59375786781311\n",
      "11.157629013061523\n",
      "12.361654996871948\n",
      "11.655821084976196\n",
      "11.971081972122192\n",
      "11.351592063903809\n",
      "11.106999158859253\n",
      "11.659721851348877\n",
      "11.253346920013428\n",
      "11.258503913879395\n",
      "11.255056858062744\n",
      "11.136001825332642\n",
      "11.195443868637085\n",
      "11.14368987083435\n",
      "10.999866008758545\n",
      "11.181936979293823\n",
      "13.157772064208984\n",
      "13.126621007919312\n",
      "13.10197401046753\n",
      "11.689800262451172\n",
      "13.574444055557251\n",
      "11.921428203582764\n",
      "12.280020952224731\n",
      "12.326550960540771\n",
      "13.125996828079224\n",
      "12.494706869125366\n",
      "12.312365770339966\n",
      "12.522922277450562\n",
      "13.167587757110596\n",
      "11.809667825698853\n",
      "11.368901252746582\n",
      "11.625837087631226\n",
      "10.519732236862183\n",
      "11.367630958557129\n",
      "11.580526113510132\n",
      "12.371399879455566\n",
      "11.986377954483032\n",
      "12.475345849990845\n",
      "11.66766095161438\n",
      "12.583786010742188\n",
      "11.7381911277771\n",
      "12.174489259719849\n",
      "12.558834075927734\n",
      "11.472303867340088\n",
      "12.000126361846924\n",
      "12.757660865783691\n",
      "12.034326076507568\n",
      "12.41840672492981\n",
      "12.874721765518188\n",
      "13.797470092773438\n",
      "13.749099016189575\n",
      "13.743696928024292\n",
      "12.962029933929443\n",
      "13.06371521949768\n",
      "12.473803043365479\n",
      "12.712616205215454\n",
      "12.212163925170898\n",
      "12.335638046264648\n",
      "12.612874984741211\n",
      "12.341186761856079\n",
      "12.073657035827637\n",
      "12.805129051208496\n",
      "11.332962036132812\n",
      "12.854470252990723\n",
      "12.729711294174194\n",
      "12.470171928405762\n",
      "11.962383031845093\n",
      "12.403496265411377\n",
      "11.849937915802002\n",
      "12.638722896575928\n",
      "11.873875141143799\n",
      "12.434103965759277\n",
      "12.376193046569824\n",
      "11.922215938568115\n",
      "11.061118841171265\n",
      "11.735827922821045\n",
      "12.201172828674316\n",
      "12.243020057678223\n",
      "11.684848070144653\n",
      "11.949538946151733\n",
      "12.106861114501953\n",
      "11.504142761230469\n",
      "10.90866994857788\n",
      "10.71494722366333\n",
      "12.24153208732605\n",
      "12.102817058563232\n",
      "12.505706787109375\n",
      "12.04236888885498\n",
      "12.963188886642456\n",
      "12.646384000778198\n",
      "11.37930703163147\n",
      "11.0470130443573\n",
      "12.659956932067871\n",
      "12.650861263275146\n",
      "12.551062107086182\n",
      "12.670920848846436\n",
      "12.750005006790161\n",
      "12.493176221847534\n",
      "12.466238975524902\n",
      "12.204213857650757\n",
      "11.809813022613525\n",
      "11.882098913192749\n",
      "10.604474067687988\n",
      "11.526143074035645\n",
      "12.624446868896484\n",
      "10.82369089126587\n",
      "12.335373163223267\n",
      "13.039380073547363\n",
      "11.538142919540405\n",
      "11.33372688293457\n",
      "11.651015281677246\n",
      "11.32540512084961\n",
      "11.116686820983887\n",
      "11.582278966903687\n",
      "12.328495979309082\n",
      "12.349268913269043\n",
      "11.686149835586548\n",
      "11.75951099395752\n",
      "11.64284086227417\n",
      "11.301023960113525\n",
      "11.682307004928589\n",
      "10.919969081878662\n",
      "10.933289051055908\n",
      "10.952290058135986\n",
      "11.827877044677734\n",
      "11.962989091873169\n",
      "11.452753782272339\n",
      "11.592219829559326\n",
      "11.47798204421997\n",
      "11.68489384651184\n",
      "11.678343057632446\n",
      "11.901603937149048\n",
      "11.390331029891968\n",
      "10.928263187408447\n",
      "11.732164859771729\n",
      "11.34754204750061\n",
      "11.484716892242432\n",
      "11.828542947769165\n",
      "11.893771648406982\n",
      "11.53245997428894\n",
      "11.920637130737305\n",
      "11.92478609085083\n",
      "11.558840036392212\n",
      "10.905041933059692\n",
      "10.808525085449219\n",
      "10.587066888809204\n",
      "11.748378992080688\n",
      "11.051541805267334\n",
      "12.167886734008789\n",
      "13.038713216781616\n",
      "11.862956762313843\n",
      "11.480139017105103\n",
      "11.668737888336182\n",
      "11.33596420288086\n",
      "11.516333818435669\n",
      "10.31481122970581\n",
      "11.308851957321167\n",
      "11.483659029006958\n",
      "11.603057861328125\n",
      "11.718719959259033\n",
      "11.892893075942993\n",
      "11.814175367355347\n",
      "13.789106369018555\n",
      "12.971369981765747\n",
      "12.786878824234009\n",
      "12.698414087295532\n",
      "13.128325939178467\n",
      "12.166221857070923\n",
      "12.137180089950562\n",
      "11.782023906707764\n",
      "11.510085821151733\n",
      "11.448185920715332\n",
      "11.50915789604187\n",
      "12.301076889038086\n",
      "12.896385192871094\n",
      "12.85873007774353\n",
      "12.945526123046875\n",
      "12.765633821487427\n",
      "14.364444732666016\n",
      "13.009803056716919\n",
      "13.384443283081055\n",
      "13.45093297958374\n",
      "13.334199905395508\n",
      "13.165600061416626\n",
      "12.005242824554443\n",
      "11.588743925094604\n",
      "11.555861949920654\n",
      "13.333214282989502\n",
      "13.059878826141357\n",
      "14.243636131286621\n",
      "13.181634902954102\n",
      "13.4327552318573\n",
      "12.370965003967285\n",
      "12.963320016860962\n",
      "13.524993181228638\n",
      "13.639724016189575\n",
      "13.900380849838257\n",
      "14.059860944747925\n",
      "13.013004064559937\n",
      "12.442667007446289\n",
      "12.742147207260132\n",
      "12.20734190940857\n",
      "13.017629146575928\n",
      "13.86541199684143\n",
      "11.681318044662476\n",
      "12.661700963973999\n",
      "11.605605125427246\n",
      "11.681519269943237\n",
      "12.572929859161377\n",
      "11.948002099990845\n",
      "11.770756006240845\n",
      "10.97523307800293\n",
      "10.846097946166992\n",
      "11.737626075744629\n",
      "12.072408199310303\n",
      "11.866122961044312\n",
      "11.617411851882935\n",
      "11.70351266860962\n",
      "11.574334383010864\n",
      "10.930239200592041\n",
      "11.926247358322144\n",
      "12.205263137817383\n",
      "11.87635087966919\n",
      "10.34660291671753\n",
      "12.084242343902588\n",
      "12.233942985534668\n",
      "12.107783079147339\n",
      "12.370553970336914\n",
      "12.785888910293579\n",
      "11.906691789627075\n",
      "11.894004344940186\n",
      "11.636426210403442\n",
      "11.570681095123291\n",
      "11.888125896453857\n",
      "12.002280235290527\n",
      "11.298541069030762\n",
      "11.296699285507202\n",
      "12.191936016082764\n",
      "11.329438924789429\n",
      "10.820964813232422\n",
      "10.768474817276001\n",
      "10.934813976287842\n",
      "11.15261197090149\n",
      "11.092038869857788\n",
      "12.11258602142334\n",
      "12.008757829666138\n",
      "11.887644290924072\n",
      "12.090179920196533\n",
      "11.575321912765503\n",
      "13.158328771591187\n",
      "11.802286148071289\n",
      "12.038928747177124\n",
      "11.703951120376587\n",
      "11.61372995376587\n",
      "11.371335983276367\n",
      "11.218202829360962\n",
      "12.092068910598755\n",
      "10.694956064224243\n",
      "11.66208004951477\n",
      "11.75192904472351\n",
      "11.398416757583618\n",
      "11.35227108001709\n",
      "11.391793012619019\n",
      "11.762164115905762\n",
      "11.714455842971802\n",
      "11.806247234344482\n",
      "11.752207040786743\n",
      "11.151477098464966\n",
      "11.94660210609436\n",
      "11.94469404220581\n",
      "12.033493995666504\n",
      "10.372888803482056\n",
      "11.102439880371094\n",
      "11.064344882965088\n",
      "11.272733926773071\n",
      "12.165099143981934\n",
      "13.836708784103394\n",
      "12.15152621269226\n",
      "10.796431064605713\n",
      "11.949343919754028\n",
      "12.427345037460327\n",
      "12.300535917282104\n",
      "11.323820114135742\n",
      "11.778141021728516\n",
      "12.713315963745117\n",
      "11.738644123077393\n",
      "12.253610849380493\n",
      "11.775269985198975\n",
      "11.35108208656311\n",
      "12.370503902435303\n",
      "12.572537899017334\n",
      "12.960753917694092\n",
      "12.494240045547485\n",
      "12.615370035171509\n",
      "12.033870220184326\n",
      "12.258085250854492\n",
      "12.63617205619812\n",
      "12.84781265258789\n",
      "12.786906957626343\n",
      "12.140864133834839\n",
      "12.131318092346191\n",
      "12.717395067214966\n",
      "12.572506189346313\n",
      "12.529446840286255\n",
      "12.340907096862793\n",
      "11.928088903427124\n",
      "12.883357048034668\n",
      "12.812610149383545\n",
      "12.800145864486694\n",
      "11.514545202255249\n",
      "11.33937406539917\n",
      "11.971149921417236\n",
      "12.249266862869263\n",
      "11.920780181884766\n",
      "11.740846157073975\n",
      "12.644824981689453\n",
      "13.24658465385437\n",
      "13.142691135406494\n",
      "12.847281217575073\n",
      "12.68023681640625\n",
      "11.54449200630188\n",
      "12.85079288482666\n",
      "12.177148818969727\n",
      "12.090842962265015\n",
      "12.244137287139893\n",
      "11.900899648666382\n",
      "11.86559510231018\n",
      "13.049566268920898\n",
      "12.596570014953613\n",
      "11.976831912994385\n",
      "13.127846002578735\n",
      "11.722821950912476\n",
      "11.609199047088623\n",
      "11.349331378936768\n",
      "13.29634690284729\n",
      "12.404233932495117\n",
      "12.06997799873352\n",
      "12.502231121063232\n",
      "12.339474201202393\n",
      "12.828435897827148\n",
      "12.711536169052124\n",
      "12.315121173858643\n",
      "12.006277322769165\n",
      "12.282166957855225\n",
      "12.388880014419556\n",
      "12.21450686454773\n",
      "12.331034898757935\n",
      "12.250916004180908\n",
      "12.17226505279541\n",
      "13.059284925460815\n",
      "11.81066083908081\n",
      "11.990214824676514\n",
      "12.576688766479492\n",
      "12.070899248123169\n",
      "12.893722772598267\n",
      "11.652988910675049\n",
      "12.273303985595703\n",
      "11.68501091003418\n",
      "12.77693223953247\n",
      "11.693264961242676\n",
      "11.572311162948608\n",
      "12.100075006484985\n",
      "12.52477502822876\n",
      "12.851707935333252\n",
      "11.873593091964722\n",
      "12.487844228744507\n",
      "11.745990991592407\n",
      "12.680686235427856\n",
      "12.635389804840088\n",
      "12.655994892120361\n",
      "12.667892217636108\n",
      "12.901527166366577\n",
      "12.19317078590393\n",
      "12.766025066375732\n",
      "12.342330932617188\n",
      "13.079601049423218\n",
      "11.46900200843811\n",
      "11.60660982131958\n",
      "12.62879204750061\n",
      "13.3432776927948\n",
      "13.126601934432983\n",
      "12.628505945205688\n",
      "12.93427300453186\n",
      "12.011372804641724\n",
      "12.41715407371521\n",
      "11.92650294303894\n",
      "11.317626953125\n",
      "11.963968992233276\n",
      "11.987678050994873\n",
      "12.68703579902649\n",
      "12.643712043762207\n",
      "12.39566683769226\n",
      "12.340311050415039\n",
      "11.403051137924194\n",
      "13.011538982391357\n",
      "12.871344804763794\n",
      "11.732842922210693\n",
      "12.717447996139526\n",
      "12.001231908798218\n",
      "12.62795901298523\n",
      "12.360936880111694\n",
      "11.880399227142334\n",
      "12.653290033340454\n",
      "12.68125319480896\n",
      "12.550901889801025\n",
      "12.411739110946655\n",
      "12.641947984695435\n",
      "12.396529912948608\n",
      "12.939426183700562\n",
      "12.553602695465088\n",
      "12.530821323394775\n",
      "12.698554039001465\n",
      "12.627462148666382\n",
      "12.101289987564087\n",
      "11.383698225021362\n",
      "11.580793857574463\n",
      "12.036747694015503\n",
      "11.894373893737793\n",
      "12.743167877197266\n",
      "12.766620874404907\n",
      "12.233726978302002\n",
      "12.450474977493286\n",
      "10.932761192321777\n",
      "11.769111633300781\n",
      "12.205909013748169\n",
      "11.771279096603394\n",
      "11.71308422088623\n",
      "11.692901134490967\n",
      "11.362272024154663\n",
      "12.002172946929932\n",
      "11.5225989818573\n",
      "11.714028120040894\n",
      "11.304284811019897\n",
      "11.57340693473816\n",
      "12.185099840164185\n",
      "11.965821981430054\n",
      "11.413856029510498\n",
      "11.373434066772461\n",
      "11.468050956726074\n",
      "11.331604957580566\n",
      "11.119681119918823\n",
      "11.728901863098145\n",
      "12.563289880752563\n",
      "12.571185111999512\n",
      "12.685585260391235\n",
      "12.012365102767944\n",
      "12.485505104064941\n",
      "11.33577013015747\n",
      "11.293287992477417\n",
      "11.382009029388428\n",
      "10.875258922576904\n",
      "11.420211791992188\n",
      "12.485893964767456\n",
      "11.524950981140137\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_resultsxh = []\n",
    "\n",
    "for k in meanxh_1:\n",
    "    result=np.mean(evaluate_modelxh(X_train,y_tr,k),axis=1)\n",
    "    metric_res = {'upper_mean': k}\n",
    "    for name, value in zip(metrics_names, result):\n",
    "#             print(name, ': ', value)\n",
    "            metric_res[name] = value\n",
    "       \n",
    "\n",
    "    all_resultsxh.append(metric_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_resultsxh_w = []\n",
    "\n",
    "for k in meanxh_1:\n",
    "    result=np.mean(evaluate_modelxh(X_train,y_tr,k),axis=1)\n",
    "    metric_res = {'upper_mean': k}\n",
    "    for name, value in zip(metrics_names, result):\n",
    "#             print(name, ': ', value)\n",
    "            metric_res[name] = value\n",
    "       \n",
    "\n",
    "    all_resultsxh_w.append(metric_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(all_resultsxh_w).to_csv('CV_select_mu_MM_gla_simple.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "metrics_names=['Balanced_acc','F2_score','AUC','Recall','Precision']\n",
    "def evaluate_model(X,y,m,model=LogisticRegression()):\n",
    "    \n",
    "    bacc,f2,auc,rec,pre=list(),list(),list(),list(),list()\n",
    "   \n",
    "#     cv = RepeatedStratifiedKFold(n_splits=10,n_repeats=1, random_state=1)\n",
    "    for train_index,test_index in cv.split(X,y):\n",
    "#                 X_resampled_smote,y_resampled_smote=SMOTE().fit_resample(X[train_index], y[train_index])\n",
    "                model.fit(X[train_index],y[train_index])\n",
    "                prob_y=sigmoid_((np.dot(X[test_index],model.coef_.T)+model.intercept_),m)\n",
    "                auc.append(roc_auc_score(y[test_index],prob_y))\n",
    "                \n",
    "                prdict_y=np.round(prob_y)\n",
    "                rec.append(recall_score(y[test_index],prdict_y))\n",
    "                pre.append(precision_score(y[test_index],prdict_y))\n",
    "                \n",
    "                bacc.append(balanced_accuracy_score(y[test_index],prdict_y))\n",
    "                f2.append(fbeta_score(y[test_index],prdict_y,beta=2))\n",
    " \n",
    "    return bacc,f2,auc,rec,pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "##交叉验证集选上下均值\n",
    "all_results = []\n",
    "\n",
    "for k in mean_lr:\n",
    "    result=np.mean(evaluate_model(x_tr,y_tr,k),axis=1)\n",
    "    metric_res = {'upper_mean': k}\n",
    "    for name, value in zip(metrics_names, result):\n",
    "#             print(name, ': ', value)\n",
    "            metric_res[name] = value\n",
    "       \n",
    "\n",
    "    all_results.append(metric_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.DataFrame(all_results).to_csv('CV_which_mu_to_set_inLRBIAS.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def huber(true, pred, delta):\\n    loss = np.where(np.abs(true-pred) < delta , 0.5*((true-pred)**2), delta*np.abs(true - pred) - 0.5*(delta**2))\\n    return np.sum(loss)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def huber(true, pred, delta):\n",
    "    loss = np.where(np.abs(true-pred) < delta , 0.5*((true-pred)**2), delta*np.abs(true - pred) - 0.5*(delta**2))\n",
    "    return np.sum(loss)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volatility-uncertain method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def F_u(x,arr):\n",
    "    p=[]\n",
    "    for c in arr:\n",
    "        \n",
    "        if c <0:\n",
    "            p.append(2*x[1]*st.norm.cdf(c/x[1])/(x[0]+x[1]))\n",
    "        else:\n",
    "            p.append(1-2*x[0]*st.norm.cdf(-c/x[0])/(x[0]+x[1]))\n",
    "    return np.array(p)\n",
    "def F_L(x,arr):\n",
    "    p=[]\n",
    "    for c in arr:\n",
    "        \n",
    "        if c <0:\n",
    "            p.append(2*x[0]*st.norm.cdf(c/x[0])/(x[0]+x[1]))\n",
    "        else:\n",
    "            p.append(1-2*x[1]*st.norm.cdf(-c/x[1])/(x[0]+x[1]))\n",
    "    return np.array(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equa_v(x,n):\n",
    " \n",
    "    \n",
    "    err_u=y_tr-F_u(x,pre_in)\n",
    "    err_L=y_tr-F_L(x,pre_in)\n",
    "#     for k in range(y_tr.shape[0]):\n",
    "    \n",
    "#         if y_tr[k]==1:\n",
    "#             err_u[k]=2*err_u[k]\n",
    "#             err_L[k]=19*err_L[k]\n",
    "    \n",
    "    return np.array([meanuncertainty(err_u,n)[1],meanuncertainty(err_L,n)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1084,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equa_vxh(x,n):\n",
    "    pre_in=np.dot(X_train,w).reshape(x_tr.shape[0],)\n",
    "    err_u=y_tr-F_u(x,pre_in)\n",
    "    err_L=y_tr-F_L(x,pre_in)\n",
    "#     for k in range(y_tr.shape[0]):\n",
    "    \n",
    "#         if y_tr[k]==1:\n",
    "#             err_u[k]=2*err_u[k]\n",
    "#             err_L[k]=19*err_L[k]\n",
    "    \n",
    "    return np.array([meanuncertainty(err_u,n)[1],meanuncertainty(err_L,n)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1094,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var_1=np.array([fsolve(lambda x:equa_vxh(x,n),[0.5,1.5] ) for n in range(50,110,10)])\n",
    "var_2=np.array([fsolve(lambda x:equa_vxh(x,n),[0.5,1.5] ) for n in range(500,1000,50)])\n",
    "var_3=np.array([fsolve(lambda x:equa_vxh(x,n),[0.5,1.5] ) for n in range(1000,6000,100)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1096,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_xh=np.concatenate((var_2,var_3))\n",
    "var_xhmean=pd.DataFrame(var_xh,index=np.hstack((\n",
    "                                           np.array(range(500,1000,50)),np.array(range(1000,6000,100)))))\n",
    "var_xhmean.to_csv('var_xhmean_celeb.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice optimal window size on  Cross-validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_names=['Balanced_acc','F2_score','Recall','Precision']\n",
    "def evaluate_varxh(X,y,m,model=solution_XX(penalty='l2')):\n",
    "    \n",
    "    bacc,f2,rec,pre=list(),list(),list(),list()\n",
    "   \n",
    "#     cv = RepeatedStratifiedKFold(n_splits=10,n_repeats=1, random_state=1)\n",
    "    for train_index,test_index in cv.split(X,y):\n",
    "        \n",
    "        N=X[train_index].shape[0]\n",
    "        n=y[train_index][y[train_index]==1].shape[0]\n",
    "        c_1=np.concatenate((X[train_index][y[train_index]==0],X[train_index][y[train_index]==1]))\n",
    "        l_1=np.concatenate((np.array([0]*(N-n)),np.array([1]*n)))\n",
    "        M=int((N-n)/19)\n",
    "        indice=[k*M for k in range(1,19)]\n",
    "        indice.append(N-n)\n",
    "        pre_cv=np.array(np.split(c_1,indice))\n",
    "        pre_y=np.array(np.split(l_1,indice))\n",
    "        w= model.fit(pre_cv,pre_y)\n",
    "#         w_x.append(w)\n",
    "        prob_y=F_u(m,X_train[test_index].dot(w))\n",
    "        \n",
    "        prdict_y=np.round(prob_y)\n",
    "        rec.append(recall_score(y[test_index],prdict_y))\n",
    "        pre.append(precision_score(y[test_index],prdict_y))\n",
    "\n",
    "        bacc.append(balanced_accuracy_score(y[test_index],prdict_y))\n",
    "        f2.append(fbeta_score(y[test_index],prdict_y,beta=2.94))\n",
    " \n",
    "    return bacc,f2,rec,pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index,test_index in cv.split(X_train,y_tr):\n",
    "        \n",
    "        N=X_train[train_index].shape[0]\n",
    "        n=y_tr[train_index][y_tr[train_index]==1].shape[0]\n",
    "        c_1=np.concatenate((X_train[train_index][y_tr[train_index]==0],X_train[train_index][y_tr[train_index]==1]))\n",
    "        l_1=np.concatenate((np.array([0]*(N-n)),np.array([1]*n)))\n",
    "        M=int((N-n)/19)\n",
    "        indice=[k*M for k in range(1,19)]\n",
    "        indice.append(N-n)\n",
    "        pre_cv=np.array(np.split(c_1,indice))\n",
    "        pre_y=np.array(np.split(l_1,indice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_resultsxhvar = []\n",
    "\n",
    "for k in var_xh:\n",
    "    result=np.mean(evaluate_varxh(X_train,y_tr,k),axis=1)\n",
    "    metric_res = {'lower-var': k[0],'upper-var':k[1]}\n",
    "    for name, value in zip(metrics_names, result):\n",
    "#             print(name, ': ', value)\n",
    "            metric_res[name] = value\n",
    "       \n",
    "\n",
    "    all_resultsxhvar.append(metric_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1105,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(all_resultsxhvar).to_csv('CV_which_sigma_to_set_inxhBIAS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_1=np.array([fsolve(lambda x:equa_v(x,n),[0.5,1.5] ) for n in range(50,110,10)])\n",
    "var_2=np.array([fsolve(lambda x:equa_v(x,n),[0.5,1.5] ) for n in range(110,1000,50)])\n",
    "var_3=np.array([fsolve(lambda x:equa_v(x,n),[0.5,1.5] ) for n in range(1000,6000,100)])\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "var=np.concatenate((var_1,var_2,var_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_mean=pd.DataFrame(var,index=np.hstack((np.array(range(50,110,10)),\n",
    "                                           np.array(range(110,1000,50)),np.array(range(1000,6000,100)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_mean.to_csv('var_mean_celeb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_evaluate_model(X,y,m,model=LogisticRegression()):\n",
    "    \n",
    "    bacc,f2,auc,rec,pre=list(),list(),list(),list(),list()\n",
    "   \n",
    "#     cv = RepeatedStratifiedKFold(n_splits=10,n_repeats=1, random_state=1)\n",
    "    for train_index,test_index in cv.split(X,y):\n",
    "#                 X_resampled_smote,y_resampled_smote=SMOTE().fit_resample(X[train_index], y[train_index])\n",
    "                model.fit(X[train_index],y[train_index])\n",
    "                prob_y=F_u(m,(np.dot(X[test_index],model_in.coef_.T)+model_in.intercept_).reshape(X[test_index].shape[0],))\n",
    "        \n",
    "                auc.append(roc_auc_score(y[test_index],prob_y))\n",
    "                \n",
    "                prdict_y=np.round(prob_y)\n",
    "                rec.append(recall_score(y[test_index],prdict_y))\n",
    "                pre.append(precision_score(y[test_index],prdict_y))\n",
    "                \n",
    "                bacc.append(balanced_accuracy_score(y[test_index],prdict_y))\n",
    "                f2.append(fbeta_score(y[test_index],prdict_y,beta=2))\n",
    " \n",
    "    return bacc,f2,auc,rec,pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_results_var = []\n",
    "\n",
    "for k in var:\n",
    "    result=np.mean(var_evaluate_model(x_tr,y_tr,k),axis=1)\n",
    "    metric_res = {'lower_var':k[0],'upper_var': k[1]}\n",
    "    for name, value in zip(metrics_names, result):\n",
    "#             print(name, ': ', value)\n",
    "            metric_res[name] = value\n",
    "       \n",
    "\n",
    "    all_results_var.append(metric_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(all_results_var).to_csv('CV_which_sigma_to_set_inLRBIAS.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.90      0.95      3812\n",
      "           1       0.32      0.92      0.47       188\n",
      "\n",
      "    accuracy                           0.90      4000\n",
      "   macro avg       0.66      0.91      0.71      4000\n",
      "weighted avg       0.96      0.90      0.92      4000\n",
      "\n",
      "Classifier : volatility-unceratin max-mean \n",
      "Running time:20.36\n",
      "Balanced-Accuracy on testing set：91.16%\n",
      "Recall on testing set：92.02%\n",
      "F-measure on testing set：76.95%\n",
      "Score:0.859\n"
     ]
    }
   ],
   "source": [
    "##volatility-bias max-mean\n",
    "\n",
    "y_preX_v=F_u([0.075,0.487],np.dot(X_test,w_2)).reshape(x_te.shape[0],)\n",
    "\n",
    "y_preX_v[y_preX_v<0.5]=0\n",
    "y_preX_v[y_preX_v>=0.5]=1\n",
    "print(classification_report(y_te,y_preX_v))\n",
    "bacc=balanced_accuracy_score(y_te,y_preX_v)\n",
    "fbeta=fbeta_score(y_te,y_preX_v,beta=2.94)\n",
    "recall=recall_score(y_te,y_preX_v)\n",
    "print('Classifier : volatility-unceratin max-mean ')\n",
    "print(\"Running time:%.2f\"%(end_x-start_x))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_preX_v)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_preX_v)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_preX_v,beta=2.94)))\n",
    "print('Score:{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end_x-start_x)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98      3812\n",
      "           1       0.65      0.78      0.71       188\n",
      "\n",
      "    accuracy                           0.97      4000\n",
      "   macro avg       0.82      0.88      0.85      4000\n",
      "weighted avg       0.97      0.97      0.97      4000\n",
      "\n",
      "Classifier : volatility-unceratin LR \n",
      "Running time:3.26\n",
      "Balanced-Accuracy on testing set：88.07%\n",
      "Recall on testing set：78.19%\n",
      "F-measure on testing set：76.63%\n",
      "Score:0.805\n"
     ]
    }
   ],
   "source": [
    "y_preX_v=F_u([0.26,2.27],np.dot(x_te,model_in.coef_.T)+model_in.intercept_).reshape(x_te.shape[0],)\n",
    "\n",
    "y_preX_v[y_preX_v<0.5]=0\n",
    "y_preX_v[y_preX_v>=0.5]=1\n",
    "print(classification_report(y_te,y_preX_v))\n",
    "bacc=balanced_accuracy_score(y_te,y_preX_v)\n",
    "fbeta=fbeta_score(y_te,y_preX_v,beta=2.94)\n",
    "recall=recall_score(y_te,y_preX_v)\n",
    "print('Classifier : volatility-unceratin LR ')\n",
    "print(\"Running time:%.2f\"%(end_i-start_i))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_preX_v)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_preX_v)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_preX_v,beta=2.94)))\n",
    "print('Score:{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end_i-start_i)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.90      0.94      3812\n",
      "           1       0.30      0.92      0.45       188\n",
      "\n",
      "    accuracy                           0.90      4000\n",
      "   macro avg       0.65      0.91      0.70      4000\n",
      "weighted avg       0.96      0.90      0.92      4000\n",
      "\n",
      "Classifier : mean-unceratin LR \n",
      "Running time:3.26\n",
      "Balanced-Accuracy on testing set：90.76%\n",
      "Recall on testing set：92.02%\n",
      "F-measure on testing set：75.90%\n",
      "Score:0.862\n"
     ]
    }
   ],
   "source": [
    "##Mean-bias LR\n",
    "y_preX_m=sigmoid_((np.dot(x_te,model_in.coef_.T)+model_in.intercept_).reshape(x_te.shape[0],),4.13)\n",
    "\n",
    "y_preX_m[y_preX_m<0.5]=0\n",
    "y_preX_m[y_preX_m>=0.5]=1\n",
    "print(classification_report(y_te,y_preX_m))\n",
    "print('Classifier : mean-unceratin LR ')\n",
    "print(\"Running time:%.2f\"%(end_i-start_i))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_preX_m)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_preX_m)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_preX_m,beta=2.94)))\n",
    "print('Score:{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end_i-start_i)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.90      0.95      3812\n",
      "           1       0.32      0.92      0.47       188\n",
      "\n",
      "    accuracy                           0.90      4000\n",
      "   macro avg       0.66      0.91      0.71      4000\n",
      "weighted avg       0.96      0.90      0.92      4000\n",
      "\n",
      "Classifier : mean-unceratin max-mean \n",
      "Running time:20.09 s\n",
      "Balanced-Accuracy on testing set：91.14%\n",
      "Recall on testing set：92.02%\n",
      "F-measure on testing set：76.92%\n",
      "Score:0.859\n"
     ]
    }
   ],
   "source": [
    "##Mean-bias max-mean\n",
    "y_preX_x=sigmoid_(np.dot(X_test,w_2).reshape(x_te.shape[0],),0.28)\n",
    "\n",
    "y_preX_x[y_preX_x<0.5]=0\n",
    "y_preX_x[y_preX_x>=0.5]=1\n",
    "print(classification_report(y_te,y_preX_x))\n",
    "bacc=balanced_accuracy_score(y_te,y_preX_x)\n",
    "fbeta=fbeta_score(y_te,y_preX_x,beta=2.94)\n",
    "recall=recall_score(y_te,y_preX_x)\n",
    "print('Classifier : mean-unceratin max-mean ')\n",
    "print(\"Running time:%.2f s\"%(end_x-start_x))\n",
    "# from sklearn.metrics import accuracy_score\n",
    "print('Balanced-Accuracy on testing set：{:.2%}'.format(balanced_accuracy_score(y_te,y_preX_x)))\n",
    "print('Recall on testing set：{:.2%}'.format(recall_score(y_te,y_preX_x)))\n",
    "print('F-measure on testing set：{:.2%}'.format(fbeta_score(y_te,y_preX_x,beta=2.94)))\n",
    "print('Score:{:.3}'.format(np.mean([recall,bacc,fbeta])*0.99+0.01/(end_x-start_x)))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "编辑元数据",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
